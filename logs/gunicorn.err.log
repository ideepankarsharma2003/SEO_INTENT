/home/ubuntu/intent_generation/gunicorn_run: line 16: exec: gunicorn: not found
/home/ubuntu/intent_generation/gunicorn_run: line 16: exec: gunicorn: not found
/home/ubuntu/intent_generation/gunicorn_run: line 16: exec: gunicorn: not found
/home/ubuntu/intent_generation/gunicorn_run: line 16: exec: gunicorn: not found
[2023-09-12 04:40:16 +0000] [110302] [INFO] Starting gunicorn 21.2.0
[2023-09-12 04:40:16 +0000] [110302] [INFO] Listening at: http://0.0.0.0:9000 (110302)
[2023-09-12 04:40:16 +0000] [110302] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-12 04:40:16 +0000] [110305] [INFO] Booting worker with pid: 110305
[2023-09-12 04:40:17 +0000] [110320] [INFO] Booting worker with pid: 110320
[2023-09-12 04:40:36 +0000] [110320] [INFO] Started server process [110320]
[2023-09-12 04:40:36 +0000] [110320] [INFO] Waiting for application startup.
[2023-09-12 04:40:36 +0000] [110320] [INFO] Application startup complete.
[2023-09-12 04:40:36 +0000] [110305] [INFO] Started server process [110305]
[2023-09-12 04:40:36 +0000] [110305] [INFO] Waiting for application startup.
[2023-09-12 04:40:36 +0000] [110305] [INFO] Application startup complete.
[2023-09-12 04:45:52 +0000] [110302] [CRITICAL] WORKER TIMEOUT (pid:110305)
[2023-09-12 04:45:52 +0000] [110302] [ERROR] Worker (pid:110305) was sent code 134!
[2023-09-12 04:45:52 +0000] [112149] [INFO] Booting worker with pid: 112149
[2023-09-12 04:45:58 +0000] [112149] [INFO] Started server process [112149]
[2023-09-12 04:45:58 +0000] [112149] [INFO] Waiting for application startup.
[2023-09-12 04:45:58 +0000] [112149] [INFO] Application startup complete.
[2023-09-12 05:13:15 +0000] [110302] [INFO] Handling signal: term
[2023-09-12 05:13:15 +0000] [110320] [INFO] Shutting down
[2023-09-12 05:13:15 +0000] [112149] [INFO] Shutting down
[2023-09-12 05:13:15 +0000] [110320] [INFO] Waiting for application shutdown.
[2023-09-12 05:13:15 +0000] [110320] [INFO] Application shutdown complete.
[2023-09-12 05:13:15 +0000] [110320] [INFO] Finished server process [110320]
[2023-09-12 05:13:15 +0000] [110320] [INFO] Worker exiting (pid: 110320)
[2023-09-12 05:13:15 +0000] [112149] [INFO] Waiting for application shutdown.
[2023-09-12 05:13:15 +0000] [112149] [INFO] Application shutdown complete.
[2023-09-12 05:13:15 +0000] [112149] [INFO] Finished server process [112149]
[2023-09-12 05:13:15 +0000] [112149] [INFO] Worker exiting (pid: 112149)
[2023-09-12 05:13:17 +0000] [110302] [INFO] Shutting down: Master
Exception ignored in: <function tail_f_producer.__del__ at 0x7f92e4f6e680>
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 649, in __del__
    self._close()
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 675, in _close
    self.file.close()
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function tail_f_producer.__del__ at 0x7f92e4f6e680>
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 649, in __del__
    self._close()
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 675, in _close
    self.file.close()
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function tail_f_producer.__del__ at 0x7f92e4f6e680>
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 649, in __del__
    self._close()
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 675, in _close
    self.file.close()
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function tail_f_producer.__del__ at 0x7f92e4f6e680>
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 649, in __del__
    self._close()
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 675, in _close
    self.file.close()
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function tail_f_producer.__del__ at 0x7f92e4f6e680>
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 649, in __del__
    self._close()
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 675, in _close
    self.file.close()
OSError: [Errno 9] Bad file descriptor
Exception ignored in: <function tail_f_producer.__del__ at 0x7f92e4f6e680>
Traceback (most recent call last):
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 649, in __del__
    self._close()
  File "/usr/lib/python3/dist-packages/supervisor/http.py", line 675, in _close
    self.file.close()
OSError: [Errno 9] Bad file descriptor
[2023-09-12 05:13:19 +0000] [115187] [INFO] Starting gunicorn 21.2.0
[2023-09-12 05:13:19 +0000] [115187] [INFO] Listening at: http://0.0.0.0:9000 (115187)
[2023-09-12 05:13:19 +0000] [115187] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-12 05:13:19 +0000] [115196] [INFO] Booting worker with pid: 115196
[2023-09-12 05:13:19 +0000] [115199] [INFO] Booting worker with pid: 115199
[2023-09-12 05:13:32 +0000] [115196] [INFO] Started server process [115196]
[2023-09-12 05:13:32 +0000] [115196] [INFO] Waiting for application startup.
[2023-09-12 05:13:32 +0000] [115196] [INFO] Application startup complete.
[2023-09-12 05:13:32 +0000] [115199] [INFO] Started server process [115199]
[2023-09-12 05:13:32 +0000] [115199] [INFO] Waiting for application startup.
[2023-09-12 05:13:32 +0000] [115199] [INFO] Application startup complete.
[2023-09-12 06:54:59 +0000] [115187] [CRITICAL] WORKER TIMEOUT (pid:115199)
[2023-09-12 06:54:59 +0000] [115187] [ERROR] Worker (pid:115199) was sent code 134!
[2023-09-12 06:54:59 +0000] [119760] [INFO] Booting worker with pid: 119760
[2023-09-12 06:55:08 +0000] [119760] [INFO] Started server process [119760]
[2023-09-12 06:55:08 +0000] [119760] [INFO] Waiting for application startup.
[2023-09-12 06:55:08 +0000] [119760] [INFO] Application startup complete.
[2023-09-12 07:47:26 +0000] [115187] [CRITICAL] WORKER TIMEOUT (pid:119760)
[2023-09-12 07:47:26 +0000] [115187] [ERROR] Worker (pid:119760) was sent code 134!
[2023-09-12 07:47:26 +0000] [121643] [INFO] Booting worker with pid: 121643
[2023-09-12 07:47:32 +0000] [121643] [INFO] Started server process [121643]
[2023-09-12 07:47:32 +0000] [121643] [INFO] Waiting for application startup.
[2023-09-12 07:47:32 +0000] [121643] [INFO] Application startup complete.
[2023-09-12 08:25:16 +0000] [115187] [CRITICAL] WORKER TIMEOUT (pid:121643)
[2023-09-12 08:25:16 +0000] [115187] [ERROR] Worker (pid:121643) was sent code 134!
[2023-09-12 08:25:16 +0000] [125013] [INFO] Booting worker with pid: 125013
[2023-09-12 08:25:23 +0000] [125013] [INFO] Started server process [125013]
[2023-09-12 08:25:23 +0000] [125013] [INFO] Waiting for application startup.
[2023-09-12 08:25:23 +0000] [125013] [INFO] Application startup complete.
[2023-09-12 08:41:51 +0000] [115187] [CRITICAL] WORKER TIMEOUT (pid:125013)
[2023-09-12 08:41:51 +0000] [115187] [ERROR] Worker (pid:125013) was sent code 134!
[2023-09-12 08:41:51 +0000] [125820] [INFO] Booting worker with pid: 125820
[2023-09-12 08:41:57 +0000] [125820] [INFO] Started server process [125820]
[2023-09-12 08:41:57 +0000] [125820] [INFO] Waiting for application startup.
[2023-09-12 08:41:57 +0000] [125820] [INFO] Application startup complete.
[2023-09-13 06:38:34 +0000] [115187] [INFO] Handling signal: term
[2023-09-13 06:38:34 +0000] [125820] [INFO] Shutting down
[2023-09-13 06:38:34 +0000] [115196] [INFO] Shutting down
[2023-09-13 06:38:34 +0000] [125820] [INFO] Waiting for application shutdown.
[2023-09-13 06:38:34 +0000] [125820] [INFO] Application shutdown complete.
[2023-09-13 06:38:34 +0000] [125820] [INFO] Finished server process [125820]
[2023-09-13 06:38:34 +0000] [125820] [INFO] Worker exiting (pid: 125820)
[2023-09-13 06:38:34 +0000] [115196] [INFO] Waiting for application shutdown.
[2023-09-13 06:38:34 +0000] [115196] [INFO] Application shutdown complete.
[2023-09-13 06:38:34 +0000] [115196] [INFO] Finished server process [115196]
[2023-09-13 06:38:34 +0000] [115196] [INFO] Worker exiting (pid: 115196)
[2023-09-13 06:38:36 +0000] [115187] [INFO] Shutting down: Master
[2023-09-13 06:46:20 +0000] [137073] [INFO] Starting gunicorn 21.2.0
[2023-09-13 06:46:20 +0000] [137073] [INFO] Listening at: http://0.0.0.0:9000 (137073)
[2023-09-13 06:46:20 +0000] [137073] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 06:46:20 +0000] [137082] [INFO] Booting worker with pid: 137082
[2023-09-13 06:46:21 +0000] [137085] [INFO] Booting worker with pid: 137085
[2023-09-13 06:46:34 +0000] [137085] [INFO] Started server process [137085]
[2023-09-13 06:46:34 +0000] [137085] [INFO] Waiting for application startup.
[2023-09-13 06:46:34 +0000] [137085] [INFO] Application startup complete.
[2023-09-13 06:46:34 +0000] [137082] [INFO] Started server process [137082]
[2023-09-13 06:46:34 +0000] [137082] [INFO] Waiting for application startup.
[2023-09-13 06:46:34 +0000] [137082] [INFO] Application startup complete.
[2023-09-13 06:51:05 +0000] [137073] [INFO] Handling signal: term
[2023-09-13 06:51:05 +0000] [137082] [INFO] Shutting down
[2023-09-13 06:51:05 +0000] [137085] [INFO] Shutting down
[2023-09-13 06:51:05 +0000] [137082] [INFO] Waiting for application shutdown.
[2023-09-13 06:51:05 +0000] [137082] [INFO] Application shutdown complete.
[2023-09-13 06:51:05 +0000] [137082] [INFO] Finished server process [137082]
[2023-09-13 06:51:05 +0000] [137082] [INFO] Worker exiting (pid: 137082)
[2023-09-13 06:51:05 +0000] [137085] [INFO] Waiting for application shutdown.
[2023-09-13 06:51:05 +0000] [137085] [INFO] Application shutdown complete.
[2023-09-13 06:51:05 +0000] [137085] [INFO] Finished server process [137085]
[2023-09-13 06:51:05 +0000] [137085] [INFO] Worker exiting (pid: 137085)
[2023-09-13 06:51:06 +0000] [137073] [INFO] Shutting down: Master
[2023-09-13 06:51:09 +0000] [138914] [INFO] Starting gunicorn 21.2.0
[2023-09-13 06:51:09 +0000] [138914] [INFO] Listening at: http://0.0.0.0:9000 (138914)
[2023-09-13 06:51:09 +0000] [138914] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 06:51:09 +0000] [138928] [INFO] Booting worker with pid: 138928
[2023-09-13 06:51:09 +0000] [138934] [INFO] Booting worker with pid: 138934
[2023-09-13 06:51:27 +0000] [138914] [INFO] Handling signal: term
[2023-09-13 06:51:27 +0000] [138914] [ERROR] Worker (pid:138934) was sent SIGTERM!
[2023-09-13 06:51:27 +0000] [138914] [ERROR] Worker (pid:138928) was sent SIGTERM!
[2023-09-13 06:51:27 +0000] [138914] [INFO] Shutting down: Master
[2023-09-13 06:51:51 +0000] [687] [INFO] Starting gunicorn 21.2.0
[2023-09-13 06:51:51 +0000] [687] [INFO] Listening at: http://0.0.0.0:9000 (687)
[2023-09-13 06:51:51 +0000] [687] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 06:51:51 +0000] [771] [INFO] Booting worker with pid: 771
[2023-09-13 06:51:51 +0000] [776] [INFO] Booting worker with pid: 776
[2023-09-13 06:52:15 +0000] [771] [INFO] Started server process [771]
[2023-09-13 06:52:15 +0000] [776] [INFO] Started server process [776]
[2023-09-13 06:52:15 +0000] [776] [INFO] Waiting for application startup.
[2023-09-13 06:52:15 +0000] [771] [INFO] Waiting for application startup.
[2023-09-13 06:52:15 +0000] [776] [INFO] Application startup complete.
[2023-09-13 06:52:15 +0000] [771] [INFO] Application startup complete.
[2023-09-13 10:47:43 +0000] [687] [INFO] Handling signal: term
[2023-09-13 10:47:43 +0000] [776] [INFO] Shutting down
[2023-09-13 10:47:43 +0000] [771] [INFO] Shutting down
[2023-09-13 10:47:44 +0000] [776] [INFO] Waiting for application shutdown.
[2023-09-13 10:47:44 +0000] [776] [INFO] Application shutdown complete.
[2023-09-13 10:47:44 +0000] [776] [INFO] Finished server process [776]
[2023-09-13 10:47:44 +0000] [776] [INFO] Worker exiting (pid: 776)
[2023-09-13 10:47:44 +0000] [771] [INFO] Waiting for application shutdown.
[2023-09-13 10:47:44 +0000] [771] [INFO] Application shutdown complete.
[2023-09-13 10:47:44 +0000] [771] [INFO] Finished server process [771]
[2023-09-13 10:47:44 +0000] [771] [INFO] Worker exiting (pid: 771)
[2023-09-13 10:47:46 +0000] [687] [INFO] Shutting down: Master
[2023-09-13 10:47:49 +0000] [51937] [INFO] Starting gunicorn 21.2.0
[2023-09-13 10:47:49 +0000] [51937] [INFO] Listening at: http://0.0.0.0:9000 (51937)
[2023-09-13 10:47:49 +0000] [51937] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 10:47:49 +0000] [51946] [INFO] Booting worker with pid: 51946
[2023-09-13 10:47:49 +0000] [51948] [INFO] Booting worker with pid: 51948
[2023-09-13 10:48:15 +0000] [51948] [INFO] Started server process [51948]
[2023-09-13 10:48:15 +0000] [51946] [INFO] Started server process [51946]
[2023-09-13 10:48:15 +0000] [51948] [INFO] Waiting for application startup.
[2023-09-13 10:48:15 +0000] [51946] [INFO] Waiting for application startup.
[2023-09-13 10:48:15 +0000] [51948] [INFO] Application startup complete.
[2023-09-13 10:48:15 +0000] [51946] [INFO] Application startup complete.
[2023-09-13 10:57:42 +0000] [51937] [CRITICAL] WORKER TIMEOUT (pid:51948)
[2023-09-13 11:00:09 +0000] [51937] [CRITICAL] WORKER TIMEOUT (pid:51946)
[2023-09-13 11:02:25 +0000] [51937] [ERROR] Worker (pid:51948) was sent SIGKILL! Perhaps out of memory?
[2023-09-13 11:02:25 +0000] [52552] [INFO] Booting worker with pid: 52552
[2023-09-13 11:02:25 +0000] [51937] [ERROR] Worker (pid:51946) was sent SIGKILL! Perhaps out of memory?
[2023-09-13 11:02:25 +0000] [52558] [INFO] Booting worker with pid: 52558
[2023-09-13 11:12:01 +0000] [682] [INFO] Starting gunicorn 21.2.0
[2023-09-13 11:12:01 +0000] [682] [INFO] Listening at: http://0.0.0.0:9000 (682)
[2023-09-13 11:12:01 +0000] [682] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 11:12:01 +0000] [755] [INFO] Booting worker with pid: 755
[2023-09-13 11:12:02 +0000] [784] [INFO] Booting worker with pid: 784
[2023-09-13 11:12:24 +0000] [784] [INFO] Started server process [784]
[2023-09-13 11:12:24 +0000] [755] [INFO] Started server process [755]
[2023-09-13 11:12:24 +0000] [755] [INFO] Waiting for application startup.
[2023-09-13 11:12:24 +0000] [784] [INFO] Waiting for application startup.
[2023-09-13 11:12:24 +0000] [755] [INFO] Application startup complete.
[2023-09-13 11:12:24 +0000] [784] [INFO] Application startup complete.
[2023-09-13 11:16:40 +0000] [684] [INFO] Starting gunicorn 21.2.0
[2023-09-13 11:16:40 +0000] [684] [INFO] Listening at: http://0.0.0.0:9000 (684)
[2023-09-13 11:16:40 +0000] [684] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 11:16:40 +0000] [780] [INFO] Booting worker with pid: 780
[2023-09-13 11:16:40 +0000] [783] [INFO] Booting worker with pid: 783
[2023-09-13 11:17:03 +0000] [780] [INFO] Started server process [780]
[2023-09-13 11:17:03 +0000] [783] [INFO] Started server process [783]
[2023-09-13 11:17:03 +0000] [783] [INFO] Waiting for application startup.
[2023-09-13 11:17:03 +0000] [780] [INFO] Waiting for application startup.
[2023-09-13 11:17:03 +0000] [783] [INFO] Application startup complete.
[2023-09-13 11:17:03 +0000] [780] [INFO] Application startup complete.
[2023-09-13 11:26:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:780)
[2023-09-13 11:27:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:783)
[2023-09-13 11:27:32 +0000] [684] [ERROR] Worker (pid:780) was sent SIGKILL! Perhaps out of memory?
[2023-09-13 11:27:33 +0000] [2077] [INFO] Booting worker with pid: 2077
[2023-09-13 11:27:33 +0000] [684] [ERROR] Worker (pid:783) was sent SIGKILL! Perhaps out of memory?
[2023-09-13 11:27:33 +0000] [2080] [INFO] Booting worker with pid: 2080
[2023-09-13 11:28:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:2077)
[2023-09-13 11:28:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:2080)
[2023-09-13 11:28:04 +0000] [684] [ERROR] Worker (pid:2080) was sent SIGKILL! Perhaps out of memory?
[2023-09-13 11:28:04 +0000] [684] [ERROR] Worker (pid:2077) was sent SIGKILL! Perhaps out of memory?
[2023-09-13 11:28:04 +0000] [2533] [INFO] Booting worker with pid: 2533
[2023-09-13 11:28:04 +0000] [2535] [INFO] Booting worker with pid: 2535
[2023-09-13 11:28:31 +0000] [2535] [INFO] Started server process [2535]
[2023-09-13 11:28:31 +0000] [2533] [INFO] Started server process [2533]
[2023-09-13 11:28:31 +0000] [2535] [INFO] Waiting for application startup.
[2023-09-13 11:28:31 +0000] [2533] [INFO] Waiting for application startup.
[2023-09-13 11:28:31 +0000] [2535] [INFO] Application startup complete.
[2023-09-13 11:28:31 +0000] [2533] [INFO] Application startup complete.
[2023-09-13 12:02:38 +0000] [684] [INFO] Handling signal: term
[2023-09-13 12:02:38 +0000] [2533] [INFO] Shutting down
[2023-09-13 12:02:38 +0000] [2535] [INFO] Shutting down
[2023-09-13 12:02:38 +0000] [2533] [INFO] Waiting for application shutdown.
[2023-09-13 12:02:38 +0000] [2533] [INFO] Application shutdown complete.
[2023-09-13 12:02:38 +0000] [2533] [INFO] Finished server process [2533]
[2023-09-13 12:02:38 +0000] [2533] [INFO] Worker exiting (pid: 2533)
[2023-09-13 12:02:38 +0000] [2535] [INFO] Waiting for application shutdown.
[2023-09-13 12:02:38 +0000] [2535] [INFO] Application shutdown complete.
[2023-09-13 12:02:38 +0000] [2535] [INFO] Finished server process [2535]
[2023-09-13 12:02:38 +0000] [2535] [INFO] Worker exiting (pid: 2535)
[2023-09-13 12:02:40 +0000] [684] [INFO] Shutting down: Master
[2023-09-13 12:02:59 +0000] [694] [INFO] Starting gunicorn 21.2.0
[2023-09-13 12:02:59 +0000] [694] [INFO] Listening at: http://0.0.0.0:9000 (694)
[2023-09-13 12:02:59 +0000] [694] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 12:02:59 +0000] [765] [INFO] Booting worker with pid: 765
[2023-09-13 12:02:59 +0000] [797] [INFO] Booting worker with pid: 797
[2023-09-13 12:03:23 +0000] [765] [INFO] Started server process [765]
[2023-09-13 12:03:23 +0000] [797] [INFO] Started server process [797]
[2023-09-13 12:03:23 +0000] [765] [INFO] Waiting for application startup.
[2023-09-13 12:03:23 +0000] [797] [INFO] Waiting for application startup.
[2023-09-13 12:03:23 +0000] [765] [INFO] Application startup complete.
[2023-09-13 12:03:23 +0000] [797] [INFO] Application startup complete.
[2023-09-13 12:14:09 +0000] [694] [INFO] Handling signal: term
[2023-09-13 12:14:09 +0000] [765] [INFO] Shutting down
[2023-09-13 12:14:09 +0000] [797] [INFO] Shutting down
[2023-09-13 12:14:09 +0000] [765] [INFO] Waiting for application shutdown.
[2023-09-13 12:14:09 +0000] [765] [INFO] Application shutdown complete.
[2023-09-13 12:14:09 +0000] [765] [INFO] Finished server process [765]
[2023-09-13 12:14:09 +0000] [765] [INFO] Worker exiting (pid: 765)
[2023-09-13 12:14:09 +0000] [797] [INFO] Waiting for application shutdown.
[2023-09-13 12:14:09 +0000] [797] [INFO] Application shutdown complete.
[2023-09-13 12:14:09 +0000] [797] [INFO] Finished server process [797]
[2023-09-13 12:14:09 +0000] [797] [INFO] Worker exiting (pid: 797)
[2023-09-13 12:14:10 +0000] [694] [INFO] Shutting down: Master
[2023-09-13 12:14:30 +0000] [667] [INFO] Starting gunicorn 21.2.0
[2023-09-13 12:14:30 +0000] [667] [INFO] Listening at: http://0.0.0.0:9000 (667)
[2023-09-13 12:14:30 +0000] [667] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 12:14:30 +0000] [735] [INFO] Booting worker with pid: 735
[2023-09-13 12:14:30 +0000] [757] [INFO] Booting worker with pid: 757
[2023-09-13 12:15:00 +0000] [667] [CRITICAL] WORKER TIMEOUT (pid:735)
[2023-09-13 12:15:00 +0000] [667] [CRITICAL] WORKER TIMEOUT (pid:757)
[2023-09-13 12:15:00 +0000] [667] [ERROR] Worker (pid:735) was sent code 134!
[2023-09-13 12:15:00 +0000] [1773] [INFO] Booting worker with pid: 1773
[2023-09-13 12:15:00 +0000] [667] [ERROR] Worker (pid:757) was sent code 134!
[2023-09-13 12:15:00 +0000] [1774] [INFO] Booting worker with pid: 1774
[2023-09-13 12:15:08 +0000] [1773] [INFO] Started server process [1773]
[2023-09-13 12:15:08 +0000] [1773] [INFO] Waiting for application startup.
[2023-09-13 12:15:08 +0000] [1774] [INFO] Started server process [1774]
[2023-09-13 12:15:08 +0000] [1774] [INFO] Waiting for application startup.
[2023-09-13 12:15:08 +0000] [1773] [INFO] Application startup complete.
[2023-09-13 12:15:08 +0000] [1774] [INFO] Application startup complete.
[2023-09-13 12:21:05 +0000] [667] [INFO] Handling signal: term
[2023-09-13 12:21:05 +0000] [1774] [INFO] Shutting down
[2023-09-13 12:21:05 +0000] [1773] [INFO] Shutting down
[2023-09-13 12:21:05 +0000] [1774] [INFO] Waiting for application shutdown.
[2023-09-13 12:21:05 +0000] [1774] [INFO] Application shutdown complete.
[2023-09-13 12:21:05 +0000] [1774] [INFO] Finished server process [1774]
[2023-09-13 12:21:05 +0000] [1774] [INFO] Worker exiting (pid: 1774)
[2023-09-13 12:21:05 +0000] [1773] [INFO] Waiting for application shutdown.
[2023-09-13 12:21:05 +0000] [1773] [INFO] Application shutdown complete.
[2023-09-13 12:21:05 +0000] [1773] [INFO] Finished server process [1773]
[2023-09-13 12:21:05 +0000] [1773] [INFO] Worker exiting (pid: 1773)
[2023-09-13 12:21:06 +0000] [667] [INFO] Shutting down: Master
[2023-09-13 12:21:25 +0000] [651] [INFO] Starting gunicorn 21.2.0
[2023-09-13 12:21:25 +0000] [651] [INFO] Listening at: http://0.0.0.0:9000 (651)
[2023-09-13 12:21:25 +0000] [651] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 12:21:25 +0000] [704] [INFO] Booting worker with pid: 704
[2023-09-13 12:21:26 +0000] [726] [INFO] Booting worker with pid: 726
[2023-09-13 12:21:49 +0000] [704] [INFO] Started server process [704]
[2023-09-13 12:21:49 +0000] [726] [INFO] Started server process [726]
[2023-09-13 12:21:49 +0000] [704] [INFO] Waiting for application startup.
[2023-09-13 12:21:49 +0000] [726] [INFO] Waiting for application startup.
[2023-09-13 12:21:49 +0000] [726] [INFO] Application startup complete.
[2023-09-13 12:21:49 +0000] [704] [INFO] Application startup complete.
[2023-09-13 13:44:13 +0000] [651] [INFO] Handling signal: term
[2023-09-13 13:44:13 +0000] [726] [INFO] Shutting down
[2023-09-13 13:44:13 +0000] [704] [INFO] Shutting down
[2023-09-13 13:44:13 +0000] [726] [INFO] Waiting for application shutdown.
[2023-09-13 13:44:13 +0000] [704] [INFO] Waiting for application shutdown.
[2023-09-13 13:44:13 +0000] [726] [INFO] Application shutdown complete.
[2023-09-13 13:44:13 +0000] [726] [INFO] Finished server process [726]
[2023-09-13 13:44:13 +0000] [704] [INFO] Application shutdown complete.
[2023-09-13 13:44:13 +0000] [704] [INFO] Finished server process [704]
[2023-09-13 13:44:13 +0000] [726] [INFO] Worker exiting (pid: 726)
[2023-09-13 13:44:13 +0000] [704] [INFO] Worker exiting (pid: 704)
[2023-09-13 13:44:16 +0000] [651] [INFO] Shutting down: Master
[2023-09-13 13:44:17 +0000] [36253] [INFO] Starting gunicorn 21.2.0
[2023-09-13 13:44:17 +0000] [36253] [INFO] Listening at: http://0.0.0.0:9000 (36253)
[2023-09-13 13:44:17 +0000] [36253] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 13:44:17 +0000] [36262] [INFO] Booting worker with pid: 36262
[2023-09-13 13:44:17 +0000] [36265] [INFO] Booting worker with pid: 36265
[2023-09-13 13:44:37 +0000] [36262] [INFO] Started server process [36262]
[2023-09-13 13:44:37 +0000] [36265] [INFO] Started server process [36265]
[2023-09-13 13:44:37 +0000] [36265] [INFO] Waiting for application startup.
[2023-09-13 13:44:37 +0000] [36262] [INFO] Waiting for application startup.
[2023-09-13 13:44:37 +0000] [36262] [INFO] Application startup complete.
[2023-09-13 13:44:37 +0000] [36265] [INFO] Application startup complete.
[2023-09-13 13:57:21 +0000] [36253] [INFO] Handling signal: term
[2023-09-13 13:57:21 +0000] [36262] [INFO] Shutting down
[2023-09-13 13:57:21 +0000] [36265] [INFO] Shutting down
[2023-09-13 13:57:21 +0000] [36262] [INFO] Waiting for application shutdown.
[2023-09-13 13:57:21 +0000] [36265] [INFO] Waiting for application shutdown.
[2023-09-13 13:57:21 +0000] [36262] [INFO] Application shutdown complete.
[2023-09-13 13:57:21 +0000] [36265] [INFO] Application shutdown complete.
[2023-09-13 13:57:21 +0000] [36262] [INFO] Finished server process [36262]
[2023-09-13 13:57:21 +0000] [36265] [INFO] Finished server process [36265]
[2023-09-13 13:57:21 +0000] [36262] [INFO] Worker exiting (pid: 36262)
[2023-09-13 13:57:21 +0000] [36265] [INFO] Worker exiting (pid: 36265)
[2023-09-13 13:57:24 +0000] [36253] [INFO] Shutting down: Master
[2023-09-13 13:57:43 +0000] [688] [INFO] Starting gunicorn 21.2.0
[2023-09-13 13:57:43 +0000] [688] [INFO] Listening at: http://0.0.0.0:9000 (688)
[2023-09-13 13:57:43 +0000] [688] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-13 13:57:43 +0000] [763] [INFO] Booting worker with pid: 763
[2023-09-13 13:57:43 +0000] [767] [INFO] Booting worker with pid: 767
[2023-09-13 13:58:07 +0000] [767] [INFO] Started server process [767]
[2023-09-13 13:58:07 +0000] [763] [INFO] Started server process [763]
[2023-09-13 13:58:07 +0000] [763] [INFO] Waiting for application startup.
[2023-09-13 13:58:07 +0000] [767] [INFO] Waiting for application startup.
[2023-09-13 13:58:07 +0000] [767] [INFO] Application startup complete.
[2023-09-13 13:58:07 +0000] [763] [INFO] Application startup complete.
[2023-09-14 05:30:24 +0000] [688] [INFO] Handling signal: term
[2023-09-14 05:30:24 +0000] [767] [INFO] Shutting down
[2023-09-14 05:30:24 +0000] [763] [INFO] Shutting down
[2023-09-14 05:30:24 +0000] [767] [INFO] Waiting for application shutdown.
[2023-09-14 05:30:24 +0000] [767] [INFO] Application shutdown complete.
[2023-09-14 05:30:24 +0000] [767] [INFO] Finished server process [767]
[2023-09-14 05:30:24 +0000] [767] [INFO] Worker exiting (pid: 767)
[2023-09-14 05:30:24 +0000] [763] [INFO] Waiting for application shutdown.
[2023-09-14 05:30:24 +0000] [763] [INFO] Application shutdown complete.
[2023-09-14 05:30:24 +0000] [763] [INFO] Finished server process [763]
[2023-09-14 05:30:24 +0000] [763] [INFO] Worker exiting (pid: 763)
[2023-09-14 05:30:26 +0000] [688] [INFO] Shutting down: Master
[2023-09-14 05:30:45 +0000] [684] [INFO] Starting gunicorn 21.2.0
[2023-09-14 05:30:45 +0000] [684] [INFO] Listening at: http://0.0.0.0:9000 (684)
[2023-09-14 05:30:45 +0000] [684] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-14 05:30:45 +0000] [761] [INFO] Booting worker with pid: 761
[2023-09-14 05:30:45 +0000] [789] [INFO] Booting worker with pid: 789
[2023-09-14 05:31:08 +0000] [761] [INFO] Started server process [761]
[2023-09-14 05:31:08 +0000] [761] [INFO] Waiting for application startup.
[2023-09-14 05:31:08 +0000] [761] [INFO] Application startup complete.
[2023-09-14 05:31:08 +0000] [789] [INFO] Started server process [789]
[2023-09-14 05:31:08 +0000] [789] [INFO] Waiting for application startup.
[2023-09-14 05:31:08 +0000] [789] [INFO] Application startup complete.
[2023-09-14 05:35:53 +0000] [684] [INFO] Handling signal: term
[2023-09-14 05:35:53 +0000] [761] [INFO] Shutting down
[2023-09-14 05:35:53 +0000] [789] [INFO] Shutting down
[2023-09-14 05:35:53 +0000] [761] [INFO] Waiting for application shutdown.
[2023-09-14 05:35:53 +0000] [789] [INFO] Waiting for application shutdown.
[2023-09-14 05:35:53 +0000] [789] [INFO] Application shutdown complete.
[2023-09-14 05:35:53 +0000] [761] [INFO] Application shutdown complete.
[2023-09-14 05:35:53 +0000] [789] [INFO] Finished server process [789]
[2023-09-14 05:35:53 +0000] [761] [INFO] Finished server process [761]
[2023-09-14 05:35:53 +0000] [789] [INFO] Worker exiting (pid: 789)
[2023-09-14 05:35:53 +0000] [761] [INFO] Worker exiting (pid: 761)
[2023-09-14 05:35:55 +0000] [684] [INFO] Shutting down: Master
[2023-09-14 05:36:13 +0000] [657] [INFO] Starting gunicorn 21.2.0
[2023-09-14 05:36:13 +0000] [657] [INFO] Listening at: http://0.0.0.0:9000 (657)
[2023-09-14 05:36:13 +0000] [657] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-14 05:36:13 +0000] [727] [INFO] Booting worker with pid: 727
[2023-09-14 05:36:13 +0000] [730] [INFO] Booting worker with pid: 730
[2023-09-14 05:36:38 +0000] [727] [INFO] Started server process [727]
[2023-09-14 05:36:38 +0000] [730] [INFO] Started server process [730]
[2023-09-14 05:36:38 +0000] [730] [INFO] Waiting for application startup.
[2023-09-14 05:36:38 +0000] [727] [INFO] Waiting for application startup.
[2023-09-14 05:36:38 +0000] [727] [INFO] Application startup complete.
[2023-09-14 05:36:38 +0000] [730] [INFO] Application startup complete.
[2023-09-14 05:55:55 +0000] [657] [INFO] Handling signal: term
[2023-09-14 05:55:55 +0000] [727] [INFO] Shutting down
[2023-09-14 05:55:55 +0000] [730] [INFO] Shutting down
[2023-09-14 05:55:56 +0000] [727] [INFO] Waiting for application shutdown.
[2023-09-14 05:55:56 +0000] [730] [INFO] Waiting for application shutdown.
[2023-09-14 05:55:56 +0000] [727] [INFO] Application shutdown complete.
[2023-09-14 05:55:56 +0000] [727] [INFO] Finished server process [727]
[2023-09-14 05:55:56 +0000] [730] [INFO] Application shutdown complete.
[2023-09-14 05:55:56 +0000] [730] [INFO] Finished server process [730]
[2023-09-14 05:55:56 +0000] [727] [INFO] Worker exiting (pid: 727)
[2023-09-14 05:55:56 +0000] [730] [INFO] Worker exiting (pid: 730)
[2023-09-14 05:55:57 +0000] [657] [INFO] Shutting down: Master
[2023-09-14 05:56:16 +0000] [676] [INFO] Starting gunicorn 21.2.0
[2023-09-14 05:56:16 +0000] [676] [INFO] Listening at: http://0.0.0.0:9000 (676)
[2023-09-14 05:56:16 +0000] [676] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-14 05:56:16 +0000] [747] [INFO] Booting worker with pid: 747
[2023-09-14 05:56:16 +0000] [755] [INFO] Booting worker with pid: 755
[2023-09-14 05:56:39 +0000] [747] [INFO] Started server process [747]
[2023-09-14 05:56:39 +0000] [755] [INFO] Started server process [755]
[2023-09-14 05:56:39 +0000] [747] [INFO] Waiting for application startup.
[2023-09-14 05:56:39 +0000] [755] [INFO] Waiting for application startup.
[2023-09-14 05:56:39 +0000] [755] [INFO] Application startup complete.
[2023-09-14 05:56:39 +0000] [747] [INFO] Application startup complete.
[2023-09-14 06:25:05 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:755)
[2023-09-14 06:25:45 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:747)
[2023-09-14 06:25:45 +0000] [676] [ERROR] Worker (pid:755) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 06:25:45 +0000] [5859] [INFO] Booting worker with pid: 5859
[2023-09-14 06:25:45 +0000] [676] [ERROR] Worker (pid:747) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 06:25:45 +0000] [5862] [INFO] Booting worker with pid: 5862
[2023-09-14 06:26:14 +0000] [5862] [INFO] Started server process [5862]
[2023-09-14 06:26:14 +0000] [5859] [INFO] Started server process [5859]
[2023-09-14 06:26:14 +0000] [5859] [INFO] Waiting for application startup.
[2023-09-14 06:26:14 +0000] [5862] [INFO] Waiting for application startup.
[2023-09-14 06:26:14 +0000] [5859] [INFO] Application startup complete.
[2023-09-14 06:26:14 +0000] [5862] [INFO] Application startup complete.
[2023-09-14 06:30:41 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:5859)
[2023-09-14 06:31:43 +0000] [676] [ERROR] Worker (pid:5859) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 06:31:43 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:5862)
[2023-09-14 06:31:43 +0000] [6399] [INFO] Booting worker with pid: 6399
[2023-09-14 06:31:43 +0000] [676] [ERROR] Worker (pid:5862) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 06:31:43 +0000] [6405] [INFO] Booting worker with pid: 6405
[2023-09-14 06:32:13 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:6399)
[2023-09-14 06:32:13 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:6405)
[2023-09-14 06:32:13 +0000] [676] [ERROR] Worker (pid:6405) was sent code 134!
[2023-09-14 06:32:13 +0000] [7254] [INFO] Booting worker with pid: 7254
[2023-09-14 06:32:13 +0000] [676] [ERROR] Worker (pid:6399) was sent code 134!
[2023-09-14 06:32:13 +0000] [7255] [INFO] Booting worker with pid: 7255
[2023-09-14 06:33:39 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:7254)
[2023-09-14 06:33:39 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:7255)
[2023-09-14 06:33:40 +0000] [676] [ERROR] Worker (pid:7254) was sent code 134!
[2023-09-14 06:33:40 +0000] [7290] [INFO] Booting worker with pid: 7290
[2023-09-14 06:33:40 +0000] [676] [ERROR] Worker (pid:7255) was sent code 134!
[2023-09-14 06:33:41 +0000] [7291] [INFO] Booting worker with pid: 7291
[2023-09-14 06:34:02 +0000] [7290] [INFO] Started server process [7290]
[2023-09-14 06:34:02 +0000] [7291] [INFO] Started server process [7291]
[2023-09-14 06:34:02 +0000] [7290] [INFO] Waiting for application startup.
[2023-09-14 06:34:02 +0000] [7291] [INFO] Waiting for application startup.
[2023-09-14 06:34:02 +0000] [7290] [INFO] Application startup complete.
[2023-09-14 06:34:02 +0000] [7291] [INFO] Application startup complete.
[2023-09-14 06:55:34 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:7290)
[2023-09-14 06:55:42 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:7291)
[2023-09-14 06:55:43 +0000] [676] [ERROR] Worker (pid:7291) was sent code 134!
[2023-09-14 06:55:43 +0000] [9992] [INFO] Booting worker with pid: 9992
[2023-09-14 06:55:43 +0000] [676] [ERROR] Worker (pid:7290) was sent code 134!
[2023-09-14 06:55:43 +0000] [9993] [INFO] Booting worker with pid: 9993
[2023-09-14 06:56:04 +0000] [9992] [INFO] Started server process [9992]
[2023-09-14 06:56:04 +0000] [9992] [INFO] Waiting for application startup.
[2023-09-14 06:56:04 +0000] [9993] [INFO] Started server process [9993]
[2023-09-14 06:56:04 +0000] [9993] [INFO] Waiting for application startup.
[2023-09-14 06:56:04 +0000] [9992] [INFO] Application startup complete.
[2023-09-14 06:56:04 +0000] [9993] [INFO] Application startup complete.
[2023-09-14 06:58:42 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:9992)
[2023-09-14 06:58:42 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:9993)
[2023-09-14 06:58:43 +0000] [676] [ERROR] Worker (pid:9992) was sent code 134!
[2023-09-14 06:58:43 +0000] [10378] [INFO] Booting worker with pid: 10378
[2023-09-14 06:58:43 +0000] [676] [ERROR] Worker (pid:9993) was sent code 134!
[2023-09-14 06:58:43 +0000] [10379] [INFO] Booting worker with pid: 10379
[2023-09-14 06:59:05 +0000] [10378] [INFO] Started server process [10378]
[2023-09-14 06:59:05 +0000] [10379] [INFO] Started server process [10379]
[2023-09-14 06:59:05 +0000] [10378] [INFO] Waiting for application startup.
[2023-09-14 06:59:05 +0000] [10379] [INFO] Waiting for application startup.
[2023-09-14 06:59:05 +0000] [10378] [INFO] Application startup complete.
[2023-09-14 06:59:05 +0000] [10379] [INFO] Application startup complete.
[2023-09-14 07:03:57 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:10378)
[2023-09-14 07:04:59 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:10379)
[2023-09-14 07:05:09 +0000] [676] [ERROR] Worker (pid:10379) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:05:09 +0000] [11631] [INFO] Booting worker with pid: 11631
[2023-09-14 07:05:10 +0000] [676] [ERROR] Worker (pid:10378) was sent code 134!
[2023-09-14 07:05:10 +0000] [11635] [INFO] Booting worker with pid: 11635
[2023-09-14 07:05:34 +0000] [11631] [INFO] Started server process [11631]
[2023-09-14 07:05:34 +0000] [11631] [INFO] Waiting for application startup.
[2023-09-14 07:05:34 +0000] [11635] [INFO] Started server process [11635]
[2023-09-14 07:05:34 +0000] [11631] [INFO] Application startup complete.
[2023-09-14 07:05:34 +0000] [11635] [INFO] Waiting for application startup.
[2023-09-14 07:05:34 +0000] [11635] [INFO] Application startup complete.
[2023-09-14 07:10:17 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:11631)
[2023-09-14 07:16:38 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:11635)
[2023-09-14 07:17:45 +0000] [676] [ERROR] Worker (pid:11635) was sent code 134!
[2023-09-14 07:17:45 +0000] [14063] [INFO] Booting worker with pid: 14063
[2023-09-14 07:17:45 +0000] [676] [ERROR] Worker (pid:11631) was sent code 134!
[2023-09-14 07:17:45 +0000] [14064] [INFO] Booting worker with pid: 14064
[2023-09-14 07:18:05 +0000] [14063] [INFO] Started server process [14063]
[2023-09-14 07:18:05 +0000] [14064] [INFO] Started server process [14064]
[2023-09-14 07:18:05 +0000] [14063] [INFO] Waiting for application startup.
[2023-09-14 07:18:05 +0000] [14064] [INFO] Waiting for application startup.
[2023-09-14 07:18:05 +0000] [14063] [INFO] Application startup complete.
[2023-09-14 07:18:05 +0000] [14064] [INFO] Application startup complete.
[2023-09-14 07:47:29 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:14063)
[2023-09-14 07:48:24 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:14064)
[2023-09-14 07:49:04 +0000] [676] [ERROR] Worker (pid:14063) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:49:04 +0000] [676] [ERROR] Worker (pid:14064) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:49:04 +0000] [24950] [INFO] Booting worker with pid: 24950
[2023-09-14 07:49:04 +0000] [24953] [INFO] Booting worker with pid: 24953
[2023-09-14 07:49:50 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:24950)
[2023-09-14 07:50:34 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:24953)
[2023-09-14 07:51:50 +0000] [676] [ERROR] Worker (pid:24950) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:51:50 +0000] [26045] [INFO] Booting worker with pid: 26045
[2023-09-14 07:51:50 +0000] [676] [ERROR] Worker (pid:24953) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:51:50 +0000] [26046] [INFO] Booting worker with pid: 26046
[2023-09-14 07:52:29 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:26045)
[2023-09-14 07:59:46 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:26046)
[2023-09-14 07:59:47 +0000] [676] [ERROR] Worker (pid:26046) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:59:47 +0000] [676] [ERROR] Worker (pid:26045) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 07:59:47 +0000] [27207] [INFO] Booting worker with pid: 27207
[2023-09-14 07:59:48 +0000] [27210] [INFO] Booting worker with pid: 27210
[2023-09-14 08:00:10 +0000] [27210] [INFO] Started server process [27210]
[2023-09-14 08:00:10 +0000] [27207] [INFO] Started server process [27207]
[2023-09-14 08:00:10 +0000] [27207] [INFO] Waiting for application startup.
[2023-09-14 08:00:10 +0000] [27210] [INFO] Waiting for application startup.
[2023-09-14 08:00:10 +0000] [27207] [INFO] Application startup complete.
[2023-09-14 08:00:10 +0000] [27210] [INFO] Application startup complete.
[2023-09-14 09:32:29 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:27210)
[2023-09-14 09:34:57 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:27207)
[2023-09-14 09:37:11 +0000] [676] [ERROR] Worker (pid:27210) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 09:37:11 +0000] [43999] [INFO] Booting worker with pid: 43999
[2023-09-14 09:37:11 +0000] [676] [ERROR] Worker (pid:27207) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 09:37:11 +0000] [44005] [INFO] Booting worker with pid: 44005
[2023-09-14 09:37:35 +0000] [44005] [INFO] Started server process [44005]
[2023-09-14 09:37:35 +0000] [43999] [INFO] Started server process [43999]
[2023-09-14 09:37:35 +0000] [44005] [INFO] Waiting for application startup.
[2023-09-14 09:37:35 +0000] [43999] [INFO] Waiting for application startup.
[2023-09-14 09:37:35 +0000] [44005] [INFO] Application startup complete.
[2023-09-14 09:37:35 +0000] [43999] [INFO] Application startup complete.
[2023-09-14 09:38:50 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:44005)
[2023-09-14 09:38:51 +0000] [676] [ERROR] Worker (pid:44005) was sent code 134!
[2023-09-14 09:38:51 +0000] [45366] [INFO] Booting worker with pid: 45366
[2023-09-14 09:38:57 +0000] [45366] [INFO] Started server process [45366]
[2023-09-14 09:38:57 +0000] [45366] [INFO] Waiting for application startup.
[2023-09-14 09:38:57 +0000] [45366] [INFO] Application startup complete.
[2023-09-14 09:43:10 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:43999)
[2023-09-14 09:46:52 +0000] [676] [ERROR] Worker (pid:43999) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 09:46:52 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:45366)
[2023-09-14 09:46:52 +0000] [47247] [INFO] Booting worker with pid: 47247
[2023-09-14 09:46:52 +0000] [676] [ERROR] Worker (pid:45366) was sent SIGKILL! Perhaps out of memory?
[2023-09-14 09:46:52 +0000] [47252] [INFO] Booting worker with pid: 47252
[2023-09-14 09:47:18 +0000] [47247] [INFO] Started server process [47247]
[2023-09-14 09:47:18 +0000] [47252] [INFO] Started server process [47252]
[2023-09-14 09:47:18 +0000] [47247] [INFO] Waiting for application startup.
[2023-09-14 09:47:18 +0000] [47252] [INFO] Waiting for application startup.
[2023-09-14 09:47:18 +0000] [47252] [INFO] Application startup complete.
[2023-09-14 09:47:18 +0000] [47247] [INFO] Application startup complete.
[2023-09-15 05:58:26 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:47247)
[2023-09-15 05:58:26 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:47252)
[2023-09-15 05:58:27 +0000] [676] [ERROR] Worker (pid:47252) was sent code 134!
[2023-09-15 05:58:27 +0000] [77158] [INFO] Booting worker with pid: 77158
[2023-09-15 05:58:27 +0000] [676] [ERROR] Worker (pid:47247) was sent code 134!
[2023-09-15 05:58:27 +0000] [77165] [INFO] Booting worker with pid: 77165
[2023-09-15 05:58:51 +0000] [77158] [INFO] Started server process [77158]
[2023-09-15 05:58:51 +0000] [77165] [INFO] Started server process [77165]
[2023-09-15 05:58:51 +0000] [77158] [INFO] Waiting for application startup.
[2023-09-15 05:58:51 +0000] [77165] [INFO] Waiting for application startup.
[2023-09-15 05:58:51 +0000] [77158] [INFO] Application startup complete.
[2023-09-15 05:58:51 +0000] [77165] [INFO] Application startup complete.
[2023-09-15 06:38:19 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:77158)
[2023-09-15 06:39:41 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:77165)
[2023-09-15 06:42:15 +0000] [676] [ERROR] Worker (pid:77158) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 06:42:15 +0000] [85758] [INFO] Booting worker with pid: 85758
[2023-09-15 06:42:15 +0000] [676] [ERROR] Worker (pid:77165) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 06:42:15 +0000] [85759] [INFO] Booting worker with pid: 85759
[2023-09-15 06:42:46 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:85758)
[2023-09-15 06:42:46 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:85759)
[2023-09-15 06:42:46 +0000] [676] [ERROR] Worker (pid:85759) was sent code 134!
[2023-09-15 06:42:46 +0000] [86962] [INFO] Booting worker with pid: 86962
[2023-09-15 06:42:46 +0000] [676] [ERROR] Worker (pid:85758) was sent code 134!
[2023-09-15 06:42:46 +0000] [86972] [INFO] Booting worker with pid: 86972
[2023-09-15 06:43:16 +0000] [86962] [INFO] Started server process [86962]
[2023-09-15 06:43:16 +0000] [86972] [INFO] Started server process [86972]
[2023-09-15 06:43:16 +0000] [86962] [INFO] Waiting for application startup.
[2023-09-15 06:43:16 +0000] [86972] [INFO] Waiting for application startup.
[2023-09-15 06:43:16 +0000] [86962] [INFO] Application startup complete.
[2023-09-15 06:43:16 +0000] [86972] [INFO] Application startup complete.
[2023-09-15 06:45:44 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:86962)
[2023-09-15 06:47:40 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:86972)
[2023-09-15 06:49:53 +0000] [676] [ERROR] Worker (pid:86962) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 06:49:54 +0000] [88312] [INFO] Booting worker with pid: 88312
[2023-09-15 06:49:54 +0000] [676] [ERROR] Worker (pid:86972) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 06:49:54 +0000] [88322] [INFO] Booting worker with pid: 88322
[2023-09-15 06:50:24 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:88312)
[2023-09-15 06:50:24 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:88322)
[2023-09-15 06:50:24 +0000] [676] [ERROR] Worker (pid:88322) was sent code 134!
[2023-09-15 06:50:24 +0000] [89557] [INFO] Booting worker with pid: 89557
[2023-09-15 06:50:24 +0000] [676] [ERROR] Worker (pid:88312) was sent code 134!
[2023-09-15 06:50:24 +0000] [89558] [INFO] Booting worker with pid: 89558
[2023-09-15 06:50:54 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:89557)
[2023-09-15 06:50:54 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:89558)
[2023-09-15 06:50:55 +0000] [676] [ERROR] Worker (pid:89557) was sent code 134!
[2023-09-15 06:50:55 +0000] [90036] [INFO] Booting worker with pid: 90036
[2023-09-15 06:50:55 +0000] [676] [ERROR] Worker (pid:89558) was sent code 134!
[2023-09-15 06:50:55 +0000] [90037] [INFO] Booting worker with pid: 90037
[2023-09-15 06:51:09 +0000] [90036] [INFO] Started server process [90036]
[2023-09-15 06:51:09 +0000] [90037] [INFO] Started server process [90037]
[2023-09-15 06:51:09 +0000] [90036] [INFO] Waiting for application startup.
[2023-09-15 06:51:09 +0000] [90037] [INFO] Waiting for application startup.
[2023-09-15 06:51:09 +0000] [90037] [INFO] Application startup complete.
[2023-09-15 06:51:09 +0000] [90036] [INFO] Application startup complete.
[2023-09-15 07:10:01 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:90036)
[2023-09-15 07:10:01 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:90037)
[2023-09-15 07:10:03 +0000] [676] [ERROR] Worker (pid:90036) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 07:10:03 +0000] [676] [ERROR] Worker (pid:90037) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 07:10:03 +0000] [98027] [INFO] Booting worker with pid: 98027
[2023-09-15 07:10:03 +0000] [98028] [INFO] Booting worker with pid: 98028
[2023-09-15 07:10:33 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:98027)
[2023-09-15 07:10:33 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:98028)
[2023-09-15 07:10:33 +0000] [676] [ERROR] Worker (pid:98028) was sent code 134!
[2023-09-15 07:10:33 +0000] [98660] [INFO] Booting worker with pid: 98660
[2023-09-15 07:10:33 +0000] [676] [ERROR] Worker (pid:98027) was sent code 134!
[2023-09-15 07:10:33 +0000] [98661] [INFO] Booting worker with pid: 98661
[2023-09-15 07:10:42 +0000] [98661] [INFO] Started server process [98661]
[2023-09-15 07:10:42 +0000] [98660] [INFO] Started server process [98660]
[2023-09-15 07:10:42 +0000] [98661] [INFO] Waiting for application startup.
[2023-09-15 07:10:42 +0000] [98660] [INFO] Waiting for application startup.
[2023-09-15 07:10:42 +0000] [98661] [INFO] Application startup complete.
[2023-09-15 07:10:42 +0000] [98660] [INFO] Application startup complete.
[2023-09-15 08:33:51 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:98660)
[2023-09-15 08:34:10 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:98661)
[2023-09-15 08:36:26 +0000] [676] [ERROR] Worker (pid:98660) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 08:36:26 +0000] [127496] [INFO] Booting worker with pid: 127496
[2023-09-15 08:36:26 +0000] [676] [ERROR] Worker (pid:98661) was sent SIGKILL! Perhaps out of memory?
[2023-09-15 08:36:26 +0000] [127503] [INFO] Booting worker with pid: 127503
[2023-09-15 08:36:49 +0000] [127503] [INFO] Started server process [127503]
[2023-09-15 08:36:49 +0000] [127496] [INFO] Started server process [127496]
[2023-09-15 08:36:49 +0000] [127496] [INFO] Waiting for application startup.
[2023-09-15 08:36:49 +0000] [127503] [INFO] Waiting for application startup.
[2023-09-15 08:36:49 +0000] [127496] [INFO] Application startup complete.
[2023-09-15 08:36:49 +0000] [127503] [INFO] Application startup complete.
[2023-09-18 06:00:59 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:127503)
[2023-09-18 06:00:59 +0000] [676] [ERROR] Worker (pid:127503) was sent code 134!
[2023-09-18 06:00:59 +0000] [230676] [INFO] Booting worker with pid: 230676
[2023-09-18 06:01:16 +0000] [230676] [INFO] Started server process [230676]
[2023-09-18 06:01:16 +0000] [230676] [INFO] Waiting for application startup.
[2023-09-18 06:01:16 +0000] [230676] [INFO] Application startup complete.
[2023-09-18 06:04:32 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:230676)
[2023-09-18 06:04:32 +0000] [676] [ERROR] Worker (pid:230676) was sent code 134!
[2023-09-18 06:04:32 +0000] [230705] [INFO] Booting worker with pid: 230705
[2023-09-18 06:04:48 +0000] [230705] [INFO] Started server process [230705]
[2023-09-18 06:04:48 +0000] [230705] [INFO] Waiting for application startup.
[2023-09-18 06:04:48 +0000] [230705] [INFO] Application startup complete.
[2023-09-18 06:33:53 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:127496)
[2023-09-18 06:33:53 +0000] [676] [ERROR] Worker (pid:127496) was sent code 134!
[2023-09-18 06:33:53 +0000] [230957] [INFO] Booting worker with pid: 230957
[2023-09-18 06:34:02 +0000] [230957] [INFO] Started server process [230957]
[2023-09-18 06:34:02 +0000] [230957] [INFO] Waiting for application startup.
[2023-09-18 06:34:02 +0000] [230957] [INFO] Application startup complete.
[2023-09-18 06:35:02 +0000] [676] [CRITICAL] WORKER TIMEOUT (pid:230957)
[2023-09-18 06:35:03 +0000] [676] [ERROR] Worker (pid:230957) was sent code 134!
[2023-09-18 06:35:03 +0000] [231426] [INFO] Booting worker with pid: 231426
[2023-09-18 06:35:09 +0000] [231426] [INFO] Started server process [231426]
[2023-09-18 06:35:09 +0000] [231426] [INFO] Waiting for application startup.
[2023-09-18 06:35:09 +0000] [231426] [INFO] Application startup complete.
[2023-09-18 10:06:06 +0000] [676] [INFO] Handling signal: term
[2023-09-18 10:06:06 +0000] [230705] [INFO] Shutting down
[2023-09-18 10:06:06 +0000] [231426] [INFO] Shutting down
[2023-09-18 10:06:06 +0000] [230705] [INFO] Waiting for application shutdown.
[2023-09-18 10:06:06 +0000] [231426] [INFO] Waiting for application shutdown.
[2023-09-18 10:06:06 +0000] [230705] [INFO] Application shutdown complete.
[2023-09-18 10:06:06 +0000] [230705] [INFO] Finished server process [230705]
[2023-09-18 10:06:06 +0000] [231426] [INFO] Application shutdown complete.
[2023-09-18 10:06:06 +0000] [231426] [INFO] Finished server process [231426]
[2023-09-18 10:06:06 +0000] [230705] [INFO] Worker exiting (pid: 230705)
[2023-09-18 10:06:06 +0000] [231426] [INFO] Worker exiting (pid: 231426)
[2023-09-18 10:06:07 +0000] [676] [INFO] Shutting down: Master
[2023-09-18 10:06:10 +0000] [240155] [INFO] Starting gunicorn 21.2.0
[2023-09-18 10:06:10 +0000] [240155] [INFO] Listening at: http://0.0.0.0:9000 (240155)
[2023-09-18 10:06:10 +0000] [240155] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-18 10:06:10 +0000] [240163] [INFO] Booting worker with pid: 240163
[2023-09-18 10:06:10 +0000] [240165] [INFO] Booting worker with pid: 240165
[2023-09-18 10:06:19 +0000] [240165] [INFO] Started server process [240165]
[2023-09-18 10:06:19 +0000] [240165] [INFO] Waiting for application startup.
[2023-09-18 10:06:19 +0000] [240165] [INFO] Application startup complete.
[2023-09-18 10:06:19 +0000] [240163] [INFO] Started server process [240163]
[2023-09-18 10:06:19 +0000] [240163] [INFO] Waiting for application startup.
[2023-09-18 10:06:19 +0000] [240163] [INFO] Application startup complete.
[2023-09-18 10:07:05 +0000] [240155] [CRITICAL] WORKER TIMEOUT (pid:240165)
[2023-09-18 10:07:05 +0000] [240155] [ERROR] Worker (pid:240165) was sent code 134!
[2023-09-18 10:07:05 +0000] [240328] [INFO] Booting worker with pid: 240328
[2023-09-18 10:07:11 +0000] [240328] [INFO] Started server process [240328]
[2023-09-18 10:07:11 +0000] [240328] [INFO] Waiting for application startup.
[2023-09-18 10:07:11 +0000] [240328] [INFO] Application startup complete.
[2023-09-19 06:01:37 +0000] [240155] [CRITICAL] WORKER TIMEOUT (pid:240328)
[2023-09-19 06:01:37 +0000] [240155] [ERROR] Worker (pid:240328) was sent code 134!
[2023-09-19 06:01:37 +0000] [249410] [INFO] Booting worker with pid: 249410
[2023-09-19 06:01:45 +0000] [249410] [INFO] Started server process [249410]
[2023-09-19 06:01:45 +0000] [249410] [INFO] Waiting for application startup.
[2023-09-19 06:01:45 +0000] [249410] [INFO] Application startup complete.
[2023-09-19 13:40:52 +0000] [240155] [CRITICAL] WORKER TIMEOUT (pid:249410)
[2023-09-19 13:40:52 +0000] [240155] [ERROR] Worker (pid:249410) was sent code 134!
[2023-09-19 13:40:52 +0000] [266313] [INFO] Booting worker with pid: 266313
[2023-09-19 13:41:04 +0000] [266313] [INFO] Started server process [266313]
[2023-09-19 13:41:04 +0000] [266313] [INFO] Waiting for application startup.
[2023-09-19 13:41:04 +0000] [266313] [INFO] Application startup complete.
[2023-09-21 06:32:39 +0000] [240155] [CRITICAL] WORKER TIMEOUT (pid:266313)
[2023-09-21 06:32:40 +0000] [240155] [ERROR] Worker (pid:266313) was sent code 134!
[2023-09-21 06:32:40 +0000] [296198] [INFO] Booting worker with pid: 296198
[2023-09-21 06:32:55 +0000] [296198] [INFO] Started server process [296198]
[2023-09-21 06:32:55 +0000] [296198] [INFO] Waiting for application startup.
[2023-09-21 06:32:55 +0000] [296198] [INFO] Application startup complete.
[2023-09-21 07:07:46 +0000] [240155] [INFO] Handling signal: term
[2023-09-21 07:07:46 +0000] [296198] [INFO] Shutting down
[2023-09-21 07:07:46 +0000] [240163] [INFO] Shutting down
[2023-09-21 07:07:46 +0000] [296198] [INFO] Waiting for application shutdown.
[2023-09-21 07:07:46 +0000] [296198] [INFO] Application shutdown complete.
[2023-09-21 07:07:46 +0000] [296198] [INFO] Finished server process [296198]
[2023-09-21 07:07:46 +0000] [296198] [INFO] Worker exiting (pid: 296198)
[2023-09-21 07:07:46 +0000] [240163] [INFO] Waiting for application shutdown.
[2023-09-21 07:07:46 +0000] [240163] [INFO] Application shutdown complete.
[2023-09-21 07:07:46 +0000] [240163] [INFO] Finished server process [240163]
[2023-09-21 07:07:46 +0000] [240163] [INFO] Worker exiting (pid: 240163)
[2023-09-21 07:07:48 +0000] [240155] [INFO] Shutting down: Master
[2023-09-21 07:08:13 +0000] [691] [INFO] Starting gunicorn 21.2.0
[2023-09-21 07:08:13 +0000] [691] [INFO] Listening at: http://0.0.0.0:9000 (691)
[2023-09-21 07:08:13 +0000] [691] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-21 07:08:13 +0000] [786] [INFO] Booting worker with pid: 786
[2023-09-21 07:08:13 +0000] [787] [INFO] Booting worker with pid: 787
[2023-09-21 07:08:40 +0000] [786] [INFO] Started server process [786]
[2023-09-21 07:08:40 +0000] [787] [INFO] Started server process [787]
[2023-09-21 07:08:40 +0000] [786] [INFO] Waiting for application startup.
[2023-09-21 07:08:40 +0000] [787] [INFO] Waiting for application startup.
[2023-09-21 07:08:40 +0000] [786] [INFO] Application startup complete.
[2023-09-21 07:08:40 +0000] [787] [INFO] Application startup complete.
[2023-09-21 09:13:53 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:787)
[2023-09-21 09:15:11 +0000] [691] [ERROR] Worker (pid:787) was sent SIGKILL! Perhaps out of memory?
[2023-09-21 09:15:11 +0000] [5866] [INFO] Booting worker with pid: 5866
[2023-09-21 09:15:37 +0000] [5866] [INFO] Started server process [5866]
[2023-09-21 09:15:37 +0000] [5866] [INFO] Waiting for application startup.
[2023-09-21 09:15:37 +0000] [5866] [INFO] Application startup complete.
[2023-09-21 09:22:04 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:786)
[2023-09-21 09:23:39 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:5866)
[2023-09-21 09:26:17 +0000] [691] [ERROR] Worker (pid:786) was sent SIGKILL! Perhaps out of memory?
[2023-09-21 09:26:17 +0000] [6940] [INFO] Booting worker with pid: 6940
[2023-09-21 09:26:18 +0000] [691] [ERROR] Worker (pid:5866) was sent SIGKILL! Perhaps out of memory?
[2023-09-21 09:26:18 +0000] [6946] [INFO] Booting worker with pid: 6946
[2023-09-21 09:26:40 +0000] [6940] [INFO] Started server process [6940]
[2023-09-21 09:26:40 +0000] [6946] [INFO] Started server process [6946]
[2023-09-21 09:26:40 +0000] [6940] [INFO] Waiting for application startup.
[2023-09-21 09:26:40 +0000] [6946] [INFO] Waiting for application startup.
[2023-09-21 09:26:40 +0000] [6940] [INFO] Application startup complete.
[2023-09-21 09:26:40 +0000] [6946] [INFO] Application startup complete.
[2023-09-22 08:18:09 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:6940)
[2023-09-22 08:18:09 +0000] [691] [ERROR] Worker (pid:6940) was sent code 134!
[2023-09-22 08:18:09 +0000] [18731] [INFO] Booting worker with pid: 18731
[2023-09-22 08:18:26 +0000] [18731] [INFO] Started server process [18731]
[2023-09-22 08:18:26 +0000] [18731] [INFO] Waiting for application startup.
[2023-09-22 08:18:26 +0000] [18731] [INFO] Application startup complete.
[2023-09-22 09:39:34 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:6946)
[2023-09-22 09:40:41 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:18731)
[2023-09-22 09:41:08 +0000] [691] [ERROR] Worker (pid:6946) was sent SIGKILL! Perhaps out of memory?
[2023-09-22 09:41:08 +0000] [19268] [INFO] Booting worker with pid: 19268
[2023-09-22 09:41:09 +0000] [691] [ERROR] Worker (pid:18731) was sent code 134!
[2023-09-22 09:41:09 +0000] [19277] [INFO] Booting worker with pid: 19277
[2023-09-22 09:41:30 +0000] [19277] [INFO] Started server process [19277]
[2023-09-22 09:41:30 +0000] [19277] [INFO] Waiting for application startup.
[2023-09-22 09:41:30 +0000] [19268] [INFO] Started server process [19268]
[2023-09-22 09:41:30 +0000] [19268] [INFO] Waiting for application startup.
[2023-09-22 09:41:30 +0000] [19277] [INFO] Application startup complete.
[2023-09-22 09:41:30 +0000] [19268] [INFO] Application startup complete.
[2023-09-22 12:04:04 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:19277)
[2023-09-22 12:07:09 +0000] [691] [CRITICAL] WORKER TIMEOUT (pid:19268)
[2023-09-22 12:12:11 +0000] [691] [ERROR] Worker (pid:19277) was sent SIGKILL! Perhaps out of memory?
[2023-09-22 12:12:11 +0000] [21553] [INFO] Booting worker with pid: 21553
[2023-09-22 12:12:11 +0000] [691] [ERROR] Worker (pid:19268) was sent SIGKILL! Perhaps out of memory?
[2023-09-22 12:12:11 +0000] [21554] [INFO] Booting worker with pid: 21554
[2023-09-22 12:12:33 +0000] [21554] [INFO] Started server process [21554]
[2023-09-22 12:12:33 +0000] [21553] [INFO] Started server process [21553]
[2023-09-22 12:12:33 +0000] [21554] [INFO] Waiting for application startup.
[2023-09-22 12:12:33 +0000] [21553] [INFO] Waiting for application startup.
[2023-09-22 12:12:33 +0000] [21554] [INFO] Application startup complete.
[2023-09-22 12:12:33 +0000] [21553] [INFO] Application startup complete.
[2023-09-25 08:48:06 +0000] [691] [INFO] Handling signal: term
[2023-09-25 08:48:06 +0000] [21554] [INFO] Shutting down
[2023-09-25 08:48:06 +0000] [21553] [INFO] Shutting down
[2023-09-25 08:48:06 +0000] [21554] [INFO] Waiting for application shutdown.
[2023-09-25 08:48:06 +0000] [21554] [INFO] Application shutdown complete.
[2023-09-25 08:48:06 +0000] [21554] [INFO] Finished server process [21554]
[2023-09-25 08:48:06 +0000] [21553] [INFO] Waiting for application shutdown.
[2023-09-25 08:48:06 +0000] [21554] [INFO] Worker exiting (pid: 21554)
[2023-09-25 08:48:06 +0000] [21553] [INFO] Application shutdown complete.
[2023-09-25 08:48:06 +0000] [21553] [INFO] Finished server process [21553]
[2023-09-25 08:48:06 +0000] [21553] [INFO] Worker exiting (pid: 21553)
[2023-09-25 08:48:08 +0000] [691] [INFO] Shutting down: Master
[2023-09-25 08:48:31 +0000] [747] [INFO] Starting gunicorn 21.2.0
[2023-09-25 08:48:31 +0000] [747] [INFO] Listening at: http://0.0.0.0:9000 (747)
[2023-09-25 08:48:31 +0000] [747] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 08:48:31 +0000] [836] [INFO] Booting worker with pid: 836
[2023-09-25 08:48:31 +0000] [845] [INFO] Booting worker with pid: 845
[2023-09-25 08:48:54 +0000] [747] [INFO] Handling signal: term
[2023-09-25 08:48:54 +0000] [747] [ERROR] Worker (pid:836) was sent SIGTERM!
[2023-09-25 08:48:54 +0000] [747] [ERROR] Worker (pid:845) was sent SIGTERM!
[2023-09-25 08:48:54 +0000] [747] [INFO] Shutting down: Master
[2023-09-25 08:49:21 +0000] [757] [INFO] Starting gunicorn 21.2.0
[2023-09-25 08:49:21 +0000] [757] [INFO] Listening at: http://0.0.0.0:9000 (757)
[2023-09-25 08:49:21 +0000] [757] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 08:49:21 +0000] [830] [INFO] Booting worker with pid: 830
[2023-09-25 08:49:21 +0000] [845] [INFO] Booting worker with pid: 845
[2023-09-25 08:49:46 +0000] [830] [INFO] Started server process [830]
[2023-09-25 08:49:46 +0000] [845] [INFO] Started server process [845]
[2023-09-25 08:49:46 +0000] [845] [INFO] Waiting for application startup.
[2023-09-25 08:49:46 +0000] [830] [INFO] Waiting for application startup.
[2023-09-25 08:49:46 +0000] [845] [INFO] Application startup complete.
[2023-09-25 08:49:46 +0000] [830] [INFO] Application startup complete.
[2023-09-25 08:49:53 +0000] [757] [INFO] Handling signal: term
[2023-09-25 08:49:53 +0000] [830] [INFO] Shutting down
[2023-09-25 08:49:53 +0000] [845] [INFO] Shutting down
[2023-09-25 08:49:53 +0000] [830] [INFO] Waiting for application shutdown.
[2023-09-25 08:49:53 +0000] [830] [INFO] Application shutdown complete.
[2023-09-25 08:49:53 +0000] [845] [INFO] Waiting for application shutdown.
[2023-09-25 08:49:53 +0000] [830] [INFO] Finished server process [830]
[2023-09-25 08:49:53 +0000] [845] [INFO] Application shutdown complete.
[2023-09-25 08:49:53 +0000] [845] [INFO] Finished server process [845]
[2023-09-25 08:49:53 +0000] [830] [INFO] Worker exiting (pid: 830)
[2023-09-25 08:49:53 +0000] [845] [INFO] Worker exiting (pid: 845)
[2023-09-25 08:49:54 +0000] [757] [INFO] Shutting down: Master
[2023-09-25 08:51:28 +0000] [738] [INFO] Starting gunicorn 21.2.0
[2023-09-25 08:51:28 +0000] [738] [INFO] Listening at: http://0.0.0.0:9000 (738)
[2023-09-25 08:51:28 +0000] [738] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 08:51:28 +0000] [817] [INFO] Booting worker with pid: 817
[2023-09-25 08:51:28 +0000] [816] [INFO] Booting worker with pid: 816
[2023-09-25 08:51:54 +0000] [817] [INFO] Started server process [817]
[2023-09-25 08:51:54 +0000] [816] [INFO] Started server process [816]
[2023-09-25 08:51:54 +0000] [816] [INFO] Waiting for application startup.
[2023-09-25 08:51:54 +0000] [817] [INFO] Waiting for application startup.
[2023-09-25 08:51:54 +0000] [816] [INFO] Application startup complete.
[2023-09-25 08:51:54 +0000] [817] [INFO] Application startup complete.
[2023-09-25 09:03:29 +0000] [738] [INFO] Handling signal: term
[2023-09-25 09:03:29 +0000] [817] [INFO] Shutting down
[2023-09-25 09:03:29 +0000] [816] [INFO] Shutting down
[2023-09-25 09:03:29 +0000] [817] [INFO] Waiting for application shutdown.
[2023-09-25 09:03:29 +0000] [817] [INFO] Application shutdown complete.
[2023-09-25 09:03:29 +0000] [817] [INFO] Finished server process [817]
[2023-09-25 09:03:29 +0000] [817] [INFO] Worker exiting (pid: 817)
[2023-09-25 09:03:29 +0000] [816] [INFO] Waiting for application shutdown.
[2023-09-25 09:03:29 +0000] [816] [INFO] Application shutdown complete.
[2023-09-25 09:03:29 +0000] [816] [INFO] Finished server process [816]
[2023-09-25 09:03:29 +0000] [816] [INFO] Worker exiting (pid: 816)
[2023-09-25 09:03:30 +0000] [738] [INFO] Shutting down: Master
[2023-09-25 09:03:54 +0000] [742] [INFO] Starting gunicorn 21.2.0
[2023-09-25 09:03:54 +0000] [742] [INFO] Listening at: http://0.0.0.0:9000 (742)
[2023-09-25 09:03:54 +0000] [742] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 09:03:54 +0000] [839] [INFO] Booting worker with pid: 839
[2023-09-25 09:03:54 +0000] [843] [INFO] Booting worker with pid: 843
[2023-09-25 09:04:18 +0000] [843] [INFO] Started server process [843]
[2023-09-25 09:04:18 +0000] [843] [INFO] Waiting for application startup.
[2023-09-25 09:04:18 +0000] [839] [INFO] Started server process [839]
[2023-09-25 09:04:18 +0000] [839] [INFO] Waiting for application startup.
[2023-09-25 09:04:18 +0000] [843] [INFO] Application startup complete.
[2023-09-25 09:04:18 +0000] [839] [INFO] Application startup complete.
[2023-09-25 09:26:01 +0000] [742] [INFO] Handling signal: term
[2023-09-25 09:26:01 +0000] [839] [INFO] Shutting down
[2023-09-25 09:26:01 +0000] [843] [INFO] Shutting down
[2023-09-25 09:26:01 +0000] [839] [INFO] Waiting for application shutdown.
[2023-09-25 09:26:01 +0000] [839] [INFO] Application shutdown complete.
[2023-09-25 09:26:01 +0000] [839] [INFO] Finished server process [839]
[2023-09-25 09:26:01 +0000] [839] [INFO] Worker exiting (pid: 839)
[2023-09-25 09:26:01 +0000] [843] [INFO] Waiting for application shutdown.
[2023-09-25 09:26:01 +0000] [843] [INFO] Application shutdown complete.
[2023-09-25 09:26:01 +0000] [843] [INFO] Finished server process [843]
[2023-09-25 09:26:01 +0000] [843] [INFO] Worker exiting (pid: 843)
[2023-09-25 09:26:03 +0000] [742] [INFO] Shutting down: Master
[2023-09-25 09:27:35 +0000] [803] [INFO] Starting gunicorn 21.2.0
[2023-09-25 09:27:35 +0000] [803] [INFO] Listening at: http://0.0.0.0:9000 (803)
[2023-09-25 09:27:35 +0000] [803] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 09:27:35 +0000] [854] [INFO] Booting worker with pid: 854
[2023-09-25 09:27:35 +0000] [869] [INFO] Booting worker with pid: 869
[2023-09-25 09:27:59 +0000] [854] [INFO] Started server process [854]
[2023-09-25 09:27:59 +0000] [869] [INFO] Started server process [869]
[2023-09-25 09:27:59 +0000] [869] [INFO] Waiting for application startup.
[2023-09-25 09:27:59 +0000] [854] [INFO] Waiting for application startup.
[2023-09-25 09:27:59 +0000] [854] [INFO] Application startup complete.
[2023-09-25 09:27:59 +0000] [869] [INFO] Application startup complete.
[2023-09-25 09:35:41 +0000] [803] [INFO] Handling signal: term
[2023-09-25 09:35:41 +0000] [854] [INFO] Shutting down
[2023-09-25 09:35:41 +0000] [869] [INFO] Shutting down
[2023-09-25 09:35:41 +0000] [854] [INFO] Waiting for application shutdown.
[2023-09-25 09:35:41 +0000] [854] [INFO] Application shutdown complete.
[2023-09-25 09:35:41 +0000] [854] [INFO] Finished server process [854]
[2023-09-25 09:35:41 +0000] [854] [INFO] Worker exiting (pid: 854)
[2023-09-25 09:35:41 +0000] [869] [INFO] Waiting for application shutdown.
[2023-09-25 09:35:41 +0000] [869] [INFO] Application shutdown complete.
[2023-09-25 09:35:41 +0000] [869] [INFO] Finished server process [869]
[2023-09-25 09:35:41 +0000] [869] [INFO] Worker exiting (pid: 869)
[2023-09-25 09:35:43 +0000] [803] [INFO] Shutting down: Master
[2023-09-25 09:38:26 +0000] [754] [INFO] Starting gunicorn 21.2.0
[2023-09-25 09:38:26 +0000] [754] [INFO] Listening at: http://0.0.0.0:9000 (754)
[2023-09-25 09:38:26 +0000] [754] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 09:38:26 +0000] [841] [INFO] Booting worker with pid: 841
[2023-09-25 09:38:26 +0000] [846] [INFO] Booting worker with pid: 846
[2023-09-25 09:38:52 +0000] [841] [INFO] Started server process [841]
[2023-09-25 09:38:52 +0000] [846] [INFO] Started server process [846]
[2023-09-25 09:38:52 +0000] [841] [INFO] Waiting for application startup.
[2023-09-25 09:38:52 +0000] [846] [INFO] Waiting for application startup.
[2023-09-25 09:38:52 +0000] [841] [INFO] Application startup complete.
[2023-09-25 09:38:52 +0000] [846] [INFO] Application startup complete.
[2023-09-25 10:01:23 +0000] [754] [INFO] Handling signal: term
[2023-09-25 10:01:23 +0000] [841] [INFO] Shutting down
[2023-09-25 10:01:23 +0000] [846] [INFO] Shutting down
[2023-09-25 10:01:23 +0000] [841] [INFO] Waiting for application shutdown.
[2023-09-25 10:01:23 +0000] [841] [INFO] Application shutdown complete.
[2023-09-25 10:01:23 +0000] [841] [INFO] Finished server process [841]
[2023-09-25 10:01:23 +0000] [841] [INFO] Worker exiting (pid: 841)
[2023-09-25 10:01:23 +0000] [846] [INFO] Waiting for application shutdown.
[2023-09-25 10:01:23 +0000] [846] [INFO] Application shutdown complete.
[2023-09-25 10:01:23 +0000] [846] [INFO] Finished server process [846]
[2023-09-25 10:01:23 +0000] [846] [INFO] Worker exiting (pid: 846)
[2023-09-25 10:01:24 +0000] [754] [INFO] Shutting down: Master
[2023-09-25 10:19:29 +0000] [757] [INFO] Starting gunicorn 21.2.0
[2023-09-25 10:19:29 +0000] [757] [INFO] Listening at: http://0.0.0.0:9000 (757)
[2023-09-25 10:19:29 +0000] [757] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 10:19:29 +0000] [827] [INFO] Booting worker with pid: 827
[2023-09-25 10:19:29 +0000] [835] [INFO] Booting worker with pid: 835
[2023-09-25 10:19:57 +0000] [835] [INFO] Started server process [835]
[2023-09-25 10:19:57 +0000] [827] [INFO] Started server process [827]
[2023-09-25 10:19:57 +0000] [827] [INFO] Waiting for application startup.
[2023-09-25 10:19:57 +0000] [835] [INFO] Waiting for application startup.
[2023-09-25 10:19:57 +0000] [827] [INFO] Application startup complete.
[2023-09-25 10:19:57 +0000] [835] [INFO] Application startup complete.
[2023-09-25 10:56:37 +0000] [757] [INFO] Handling signal: term
[2023-09-25 10:56:37 +0000] [827] [INFO] Shutting down
[2023-09-25 10:56:37 +0000] [835] [INFO] Shutting down
[2023-09-25 10:56:37 +0000] [835] [INFO] Waiting for application shutdown.
[2023-09-25 10:56:37 +0000] [827] [INFO] Waiting for application shutdown.
[2023-09-25 10:56:37 +0000] [835] [INFO] Application shutdown complete.
[2023-09-25 10:56:37 +0000] [835] [INFO] Finished server process [835]
[2023-09-25 10:56:37 +0000] [827] [INFO] Application shutdown complete.
[2023-09-25 10:56:37 +0000] [827] [INFO] Finished server process [827]
[2023-09-25 10:56:37 +0000] [835] [INFO] Worker exiting (pid: 835)
[2023-09-25 10:56:37 +0000] [827] [INFO] Worker exiting (pid: 827)
[2023-09-25 10:56:38 +0000] [757] [INFO] Shutting down: Master
[2023-09-25 11:09:41 +0000] [742] [INFO] Starting gunicorn 21.2.0
[2023-09-25 11:09:41 +0000] [742] [INFO] Listening at: http://0.0.0.0:9000 (742)
[2023-09-25 11:09:41 +0000] [742] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 11:09:41 +0000] [835] [INFO] Booting worker with pid: 835
[2023-09-25 11:09:41 +0000] [837] [INFO] Booting worker with pid: 837
[2023-09-25 11:10:04 +0000] [837] [INFO] Started server process [837]
[2023-09-25 11:10:04 +0000] [835] [INFO] Started server process [835]
[2023-09-25 11:10:04 +0000] [835] [INFO] Waiting for application startup.
[2023-09-25 11:10:04 +0000] [837] [INFO] Waiting for application startup.
[2023-09-25 11:10:04 +0000] [835] [INFO] Application startup complete.
[2023-09-25 11:10:04 +0000] [837] [INFO] Application startup complete.
[2023-09-25 11:38:40 +0000] [742] [INFO] Handling signal: term
[2023-09-25 11:38:40 +0000] [835] [INFO] Shutting down
[2023-09-25 11:38:40 +0000] [837] [INFO] Shutting down
[2023-09-25 11:38:40 +0000] [835] [INFO] Waiting for application shutdown.
[2023-09-25 11:38:40 +0000] [835] [INFO] Application shutdown complete.
[2023-09-25 11:38:40 +0000] [835] [INFO] Finished server process [835]
[2023-09-25 11:38:40 +0000] [835] [INFO] Worker exiting (pid: 835)
[2023-09-25 11:38:40 +0000] [837] [INFO] Waiting for application shutdown.
[2023-09-25 11:38:40 +0000] [837] [INFO] Application shutdown complete.
[2023-09-25 11:38:40 +0000] [837] [INFO] Finished server process [837]
[2023-09-25 11:38:40 +0000] [837] [INFO] Worker exiting (pid: 837)
[2023-09-25 11:38:42 +0000] [742] [INFO] Shutting down: Master
[2023-09-25 11:39:04 +0000] [769] [INFO] Starting gunicorn 21.2.0
[2023-09-25 11:39:04 +0000] [769] [INFO] Listening at: http://0.0.0.0:9000 (769)
[2023-09-25 11:39:04 +0000] [769] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 11:39:04 +0000] [800] [INFO] Booting worker with pid: 800
[2023-09-25 11:39:04 +0000] [801] [INFO] Booting worker with pid: 801
[2023-09-25 11:39:34 +0000] [769] [CRITICAL] WORKER TIMEOUT (pid:800)
[2023-09-25 11:39:34 +0000] [769] [CRITICAL] WORKER TIMEOUT (pid:801)
[2023-09-25 11:39:35 +0000] [769] [ERROR] Worker (pid:800) was sent code 134!
[2023-09-25 11:39:35 +0000] [1011] [INFO] Booting worker with pid: 1011
[2023-09-25 11:39:35 +0000] [769] [ERROR] Worker (pid:801) was sent code 134!
[2023-09-25 11:39:35 +0000] [1012] [INFO] Booting worker with pid: 1012
[2023-09-25 11:39:42 +0000] [1011] [INFO] Started server process [1011]
[2023-09-25 11:39:42 +0000] [1011] [INFO] Waiting for application startup.
[2023-09-25 11:39:42 +0000] [1012] [INFO] Started server process [1012]
[2023-09-25 11:39:42 +0000] [1012] [INFO] Waiting for application startup.
[2023-09-25 11:39:42 +0000] [1011] [INFO] Application startup complete.
[2023-09-25 11:39:42 +0000] [1012] [INFO] Application startup complete.
[2023-09-25 12:13:36 +0000] [769] [INFO] Handling signal: term
[2023-09-25 12:13:36 +0000] [1012] [INFO] Shutting down
[2023-09-25 12:13:37 +0000] [1011] [INFO] Shutting down
[2023-09-25 12:13:37 +0000] [1012] [INFO] Waiting for application shutdown.
[2023-09-25 12:13:37 +0000] [1012] [INFO] Application shutdown complete.
[2023-09-25 12:13:37 +0000] [1012] [INFO] Finished server process [1012]
[2023-09-25 12:13:37 +0000] [1012] [INFO] Worker exiting (pid: 1012)
[2023-09-25 12:13:37 +0000] [1011] [INFO] Waiting for application shutdown.
[2023-09-25 12:13:37 +0000] [1011] [INFO] Application shutdown complete.
[2023-09-25 12:13:37 +0000] [1011] [INFO] Finished server process [1011]
[2023-09-25 12:13:37 +0000] [1011] [INFO] Worker exiting (pid: 1011)
[2023-09-25 12:13:38 +0000] [769] [INFO] Shutting down: Master
[2023-09-25 12:28:24 +0000] [737] [INFO] Starting gunicorn 21.2.0
[2023-09-25 12:28:24 +0000] [737] [INFO] Listening at: http://0.0.0.0:9000 (737)
[2023-09-25 12:28:24 +0000] [737] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 12:28:24 +0000] [828] [INFO] Booting worker with pid: 828
[2023-09-25 12:28:24 +0000] [849] [INFO] Booting worker with pid: 849
[2023-09-25 12:28:48 +0000] [849] [INFO] Started server process [849]
[2023-09-25 12:28:48 +0000] [849] [INFO] Waiting for application startup.
[2023-09-25 12:28:48 +0000] [828] [INFO] Started server process [828]
[2023-09-25 12:28:48 +0000] [828] [INFO] Waiting for application startup.
[2023-09-25 12:28:48 +0000] [849] [INFO] Application startup complete.
[2023-09-25 12:28:48 +0000] [828] [INFO] Application startup complete.
[2023-09-25 12:37:55 +0000] [737] [INFO] Handling signal: term
[2023-09-25 12:37:55 +0000] [849] [INFO] Shutting down
[2023-09-25 12:37:55 +0000] [828] [INFO] Shutting down
[2023-09-25 12:37:55 +0000] [849] [INFO] Waiting for application shutdown.
[2023-09-25 12:37:55 +0000] [849] [INFO] Application shutdown complete.
[2023-09-25 12:37:55 +0000] [849] [INFO] Finished server process [849]
[2023-09-25 12:37:55 +0000] [849] [INFO] Worker exiting (pid: 849)
[2023-09-25 12:37:55 +0000] [828] [INFO] Waiting for application shutdown.
[2023-09-25 12:37:55 +0000] [828] [INFO] Application shutdown complete.
[2023-09-25 12:37:55 +0000] [828] [INFO] Finished server process [828]
[2023-09-25 12:37:55 +0000] [828] [INFO] Worker exiting (pid: 828)
[2023-09-25 12:37:56 +0000] [737] [INFO] Shutting down: Master
[2023-09-25 12:39:53 +0000] [746] [INFO] Starting gunicorn 21.2.0
[2023-09-25 12:39:53 +0000] [746] [INFO] Listening at: http://0.0.0.0:9000 (746)
[2023-09-25 12:39:53 +0000] [746] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-25 12:39:53 +0000] [845] [INFO] Booting worker with pid: 845
[2023-09-25 12:39:53 +0000] [857] [INFO] Booting worker with pid: 857
[2023-09-25 12:40:23 +0000] [746] [CRITICAL] WORKER TIMEOUT (pid:845)
[2023-09-25 12:40:23 +0000] [746] [CRITICAL] WORKER TIMEOUT (pid:857)
[2023-09-25 12:40:23 +0000] [746] [ERROR] Worker (pid:857) was sent code 134!
[2023-09-25 12:40:23 +0000] [1657] [INFO] Booting worker with pid: 1657
[2023-09-25 12:40:23 +0000] [746] [ERROR] Worker (pid:845) was sent code 134!
[2023-09-25 12:40:23 +0000] [1658] [INFO] Booting worker with pid: 1658
[2023-09-25 12:40:32 +0000] [1658] [INFO] Started server process [1658]
[2023-09-25 12:40:32 +0000] [1657] [INFO] Started server process [1657]
[2023-09-25 12:40:32 +0000] [1658] [INFO] Waiting for application startup.
[2023-09-25 12:40:32 +0000] [1657] [INFO] Waiting for application startup.
[2023-09-25 12:40:32 +0000] [1658] [INFO] Application startup complete.
[2023-09-25 12:40:32 +0000] [1657] [INFO] Application startup complete.
[2023-09-26 05:01:04 +0000] [746] [CRITICAL] WORKER TIMEOUT (pid:1658)
[2023-09-26 05:01:04 +0000] [746] [ERROR] Worker (pid:1658) was sent code 134!
[2023-09-26 05:01:04 +0000] [14398] [INFO] Booting worker with pid: 14398
[2023-09-26 05:01:19 +0000] [14398] [INFO] Started server process [14398]
[2023-09-26 05:01:19 +0000] [14398] [INFO] Waiting for application startup.
[2023-09-26 05:01:19 +0000] [14398] [INFO] Application startup complete.
[2023-09-30 05:43:59 +0000] [746] [CRITICAL] WORKER TIMEOUT (pid:14398)
[2023-09-30 05:43:59 +0000] [746] [ERROR] Worker (pid:14398) was sent code 134!
[2023-09-30 05:43:59 +0000] [98878] [INFO] Booting worker with pid: 98878
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-09-30 05:44:15 +0000] [98878] [INFO] Started server process [98878]
[2023-09-30 05:44:15 +0000] [98878] [INFO] Waiting for application startup.
[2023-09-30 05:44:15 +0000] [98878] [INFO] Application startup complete.
[2023-09-30 12:20:59 +0000] [746] [INFO] Handling signal: term
[2023-09-30 12:20:59 +0000] [1657] [INFO] Shutting down
[2023-09-30 12:20:59 +0000] [98878] [INFO] Shutting down
[2023-09-30 12:20:59 +0000] [1657] [INFO] Waiting for application shutdown.
[2023-09-30 12:20:59 +0000] [98878] [INFO] Waiting for application shutdown.
[2023-09-30 12:20:59 +0000] [98878] [INFO] Application shutdown complete.
[2023-09-30 12:20:59 +0000] [98878] [INFO] Finished server process [98878]
[2023-09-30 12:20:59 +0000] [1657] [INFO] Application shutdown complete.
[2023-09-30 12:20:59 +0000] [1657] [INFO] Finished server process [1657]
[2023-09-30 12:20:59 +0000] [98878] [INFO] Worker exiting (pid: 98878)
[2023-09-30 12:20:59 +0000] [1657] [INFO] Worker exiting (pid: 1657)
[2023-09-30 12:21:01 +0000] [746] [INFO] Shutting down: Master
[2023-09-30 12:21:10 +0000] [100144] [INFO] Starting gunicorn 21.2.0
[2023-09-30 12:21:10 +0000] [100144] [INFO] Listening at: http://0.0.0.0:9000 (100144)
[2023-09-30 12:21:10 +0000] [100144] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-09-30 12:21:10 +0000] [100156] [INFO] Booting worker with pid: 100156
[2023-09-30 12:21:10 +0000] [100160] [INFO] Booting worker with pid: 100160
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-09-30 12:21:21 +0000] [100160] [INFO] Started server process [100160]
[2023-09-30 12:21:21 +0000] [100160] [INFO] Waiting for application startup.
[2023-09-30 12:21:21 +0000] [100160] [INFO] Application startup complete.
[2023-09-30 12:21:21 +0000] [100156] [INFO] Started server process [100156]
[2023-09-30 12:21:21 +0000] [100156] [INFO] Waiting for application startup.
[2023-09-30 12:21:21 +0000] [100156] [INFO] Application startup complete.
[2023-10-04 07:32:48 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:100156)
[2023-10-04 07:34:25 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:100160)
[2023-10-04 07:35:47 +0000] [100144] [ERROR] Worker (pid:100156) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 07:35:47 +0000] [112714] [INFO] Booting worker with pid: 112714
[2023-10-04 07:35:47 +0000] [100144] [ERROR] Worker (pid:100160) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 07:35:47 +0000] [112717] [INFO] Booting worker with pid: 112717
[2023-10-04 07:36:35 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:112714)
[2023-10-04 07:38:19 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:112717)
[2023-10-04 07:40:31 +0000] [100144] [ERROR] Worker (pid:112714) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 07:40:32 +0000] [113190] [INFO] Booting worker with pid: 113190
[2023-10-04 07:40:32 +0000] [100144] [ERROR] Worker (pid:112717) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 07:40:32 +0000] [113192] [INFO] Booting worker with pid: 113192
[2023-10-04 07:41:19 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:113192)
[2023-10-04 07:42:54 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:113190)
[2023-10-04 07:46:54 +0000] [100144] [ERROR] Worker (pid:113192) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 07:46:54 +0000] [113864] [INFO] Booting worker with pid: 113864
[2023-10-04 07:46:54 +0000] [100144] [ERROR] Worker (pid:113190) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 07:46:54 +0000] [113867] [INFO] Booting worker with pid: 113867
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 07:47:14.462197: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 07:47:14.462198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 07:47:16.085513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 07:47:16.085512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 07:47:18.709108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 07:47:18.709108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 07:47:18.727128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 07:47:18.727148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 07:47:18.735576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 07:47:18.735963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 07:47:20 +0000] [113867] [INFO] Started server process [113867]
[2023-10-04 07:47:20 +0000] [113864] [INFO] Started server process [113864]
[2023-10-04 07:47:20 +0000] [113864] [INFO] Waiting for application startup.
[2023-10-04 07:47:20 +0000] [113867] [INFO] Waiting for application startup.
[2023-10-04 07:47:20 +0000] [113864] [INFO] Application startup complete.
[2023-10-04 07:47:20 +0000] [113867] [INFO] Application startup complete.
[2023-10-04 08:41:41 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:113864)
[2023-10-04 08:42:54 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:113867)
[2023-10-04 08:43:08 +0000] [100144] [ERROR] Worker (pid:113867) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:43:08 +0000] [100144] [ERROR] Worker (pid:113864) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:43:08 +0000] [119683] [INFO] Booting worker with pid: 119683
[2023-10-04 08:43:08 +0000] [119684] [INFO] Booting worker with pid: 119684
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 08:43:47 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:119683)
[2023-10-04 08:44:08 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:119684)
[2023-10-04 08:45:05 +0000] [100144] [ERROR] Worker (pid:119683) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:45:05 +0000] [100144] [ERROR] Worker (pid:119684) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:45:05 +0000] [120375] [INFO] Booting worker with pid: 120375
[2023-10-04 08:45:05 +0000] [120376] [INFO] Booting worker with pid: 120376
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 08:45:47 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:120376)
[2023-10-04 08:47:29 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:120375)
[2023-10-04 08:49:45 +0000] [100144] [ERROR] Worker (pid:120376) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:49:45 +0000] [120618] [INFO] Booting worker with pid: 120618
[2023-10-04 08:49:45 +0000] [100144] [ERROR] Worker (pid:120375) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:49:45 +0000] [120622] [INFO] Booting worker with pid: 120622
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 08:50:32 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:120618)
[2023-10-04 08:51:56 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:120622)
[2023-10-04 08:53:11 +0000] [100144] [ERROR] Worker (pid:120618) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:53:11 +0000] [121337] [INFO] Booting worker with pid: 121337
[2023-10-04 08:53:11 +0000] [100144] [ERROR] Worker (pid:120622) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:53:11 +0000] [121346] [INFO] Booting worker with pid: 121346
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 08:54:01 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:121337)
[2023-10-04 08:55:13 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:121346)
[2023-10-04 08:56:23 +0000] [100144] [ERROR] Worker (pid:121337) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:56:24 +0000] [100144] [ERROR] Worker (pid:121346) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 08:56:24 +0000] [121672] [INFO] Booting worker with pid: 121672
[2023-10-04 08:56:24 +0000] [121674] [INFO] Booting worker with pid: 121674
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 08:58:12 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:121672)
[2023-10-04 09:05:45 +0000] [100144] [CRITICAL] WORKER TIMEOUT (pid:121674)
[2023-10-04 09:05:46 +0000] [100144] [ERROR] Worker (pid:121674) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 09:05:46 +0000] [121872] [INFO] Booting worker with pid: 121872
[2023-10-04 09:05:46 +0000] [100144] [ERROR] Worker (pid:121672) was sent code 134!
[2023-10-04 09:05:46 +0000] [121873] [INFO] Booting worker with pid: 121873
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 09:06:06.003320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 09:06:06.003320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 09:06:07.791127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 09:06:07.791127: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 09:06:10.617336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 09:06:10.617336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 09:06:10.633910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 09:06:10.633932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 09:06:10.640538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 09:06:10.640927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 09:06:12 +0000] [121873] [INFO] Started server process [121873]
[2023-10-04 09:06:12 +0000] [121872] [INFO] Started server process [121872]
[2023-10-04 09:06:12 +0000] [121872] [INFO] Waiting for application startup.
[2023-10-04 09:06:12 +0000] [121873] [INFO] Waiting for application startup.
[2023-10-04 09:06:12 +0000] [121872] [INFO] Application startup complete.
[2023-10-04 09:06:12 +0000] [121873] [INFO] Application startup complete.
[2023-10-04 10:46:12 +0000] [100144] [INFO] Handling signal: term
[2023-10-04 10:46:12 +0000] [121873] [INFO] Shutting down
[2023-10-04 10:46:12 +0000] [121872] [INFO] Shutting down
[2023-10-04 10:46:12 +0000] [121873] [INFO] Waiting for application shutdown.
[2023-10-04 10:46:12 +0000] [121873] [INFO] Application shutdown complete.
[2023-10-04 10:46:12 +0000] [121873] [INFO] Finished server process [121873]
[2023-10-04 10:46:12 +0000] [121873] [INFO] Worker exiting (pid: 121873)
[2023-10-04 10:46:12 +0000] [121872] [INFO] Waiting for application shutdown.
[2023-10-04 10:46:12 +0000] [121872] [INFO] Application shutdown complete.
[2023-10-04 10:46:12 +0000] [121872] [INFO] Finished server process [121872]
[2023-10-04 10:46:12 +0000] [121872] [INFO] Worker exiting (pid: 121872)
[2023-10-04 10:46:15 +0000] [100144] [INFO] Shutting down: Master
[2023-10-04 10:46:32 +0000] [657] [INFO] Starting gunicorn 21.2.0
[2023-10-04 10:46:32 +0000] [657] [INFO] Listening at: http://0.0.0.0:9000 (657)
[2023-10-04 10:46:32 +0000] [657] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 10:46:32 +0000] [721] [INFO] Booting worker with pid: 721
[2023-10-04 10:46:32 +0000] [730] [INFO] Booting worker with pid: 730
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 10:47:02 +0000] [657] [CRITICAL] WORKER TIMEOUT (pid:721)
[2023-10-04 10:47:02 +0000] [657] [CRITICAL] WORKER TIMEOUT (pid:730)
[2023-10-04 10:47:02 +0000] [657] [ERROR] Worker (pid:721) was sent code 134!
[2023-10-04 10:47:02 +0000] [1960] [INFO] Booting worker with pid: 1960
[2023-10-04 10:47:02 +0000] [657] [ERROR] Worker (pid:730) was sent code 134!
[2023-10-04 10:47:02 +0000] [1961] [INFO] Booting worker with pid: 1961
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 10:47:11.983084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:47:11.983085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:47:13.702904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:47:13.702950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:47:16.703394: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-10-04 10:47:16.779513: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2023-10-04 10:47:19 +0000] [1961] [INFO] Started server process [1961]
[2023-10-04 10:47:19 +0000] [1961] [INFO] Waiting for application startup.
[2023-10-04 10:47:19 +0000] [1960] [INFO] Started server process [1960]
[2023-10-04 10:47:19 +0000] [1960] [INFO] Waiting for application startup.
[2023-10-04 10:47:19 +0000] [1961] [INFO] Application startup complete.
[2023-10-04 10:47:19 +0000] [1960] [INFO] Application startup complete.
[2023-10-04 10:51:53 +0000] [657] [INFO] Handling signal: term
[2023-10-04 10:51:53 +0000] [1961] [INFO] Shutting down
[2023-10-04 10:51:53 +0000] [1960] [INFO] Shutting down
[2023-10-04 10:51:53 +0000] [1961] [INFO] Waiting for application shutdown.
[2023-10-04 10:51:53 +0000] [1961] [INFO] Application shutdown complete.
[2023-10-04 10:51:53 +0000] [1961] [INFO] Finished server process [1961]
[2023-10-04 10:51:53 +0000] [1961] [INFO] Worker exiting (pid: 1961)
[2023-10-04 10:51:53 +0000] [1960] [INFO] Waiting for application shutdown.
[2023-10-04 10:51:53 +0000] [1960] [INFO] Application shutdown complete.
[2023-10-04 10:51:53 +0000] [1960] [INFO] Finished server process [1960]
[2023-10-04 10:51:53 +0000] [1960] [INFO] Worker exiting (pid: 1960)
[2023-10-04 10:51:55 +0000] [657] [INFO] Shutting down: Master
[2023-10-04 10:51:57 +0000] [18240] [INFO] Starting gunicorn 21.2.0
[2023-10-04 10:51:57 +0000] [18240] [INFO] Listening at: http://0.0.0.0:9000 (18240)
[2023-10-04 10:51:57 +0000] [18240] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 10:51:57 +0000] [18257] [INFO] Booting worker with pid: 18257
[2023-10-04 10:51:57 +0000] [18261] [INFO] Booting worker with pid: 18261
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 10:52:08.125427: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:52:08.543999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:52:09.014910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:52:09.577506: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:52:10.295378: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-10-04 10:52:11.077648: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2023-10-04 10:52:11 +0000] [18257] [INFO] Started server process [18257]
[2023-10-04 10:52:11 +0000] [18257] [INFO] Waiting for application startup.
[2023-10-04 10:52:11 +0000] [18257] [INFO] Application startup complete.
[2023-10-04 10:52:12 +0000] [18261] [INFO] Started server process [18261]
[2023-10-04 10:52:12 +0000] [18261] [INFO] Waiting for application startup.
[2023-10-04 10:52:12 +0000] [18261] [INFO] Application startup complete.
[2023-10-04 10:55:14 +0000] [18240] [INFO] Handling signal: term
[2023-10-04 10:55:14 +0000] [18261] [INFO] Shutting down
[2023-10-04 10:55:14 +0000] [18257] [INFO] Shutting down
[2023-10-04 10:55:14 +0000] [18261] [INFO] Waiting for application shutdown.
[2023-10-04 10:55:14 +0000] [18261] [INFO] Application shutdown complete.
[2023-10-04 10:55:14 +0000] [18261] [INFO] Finished server process [18261]
[2023-10-04 10:55:14 +0000] [18261] [INFO] Worker exiting (pid: 18261)
[2023-10-04 10:55:14 +0000] [18257] [INFO] Waiting for application shutdown.
[2023-10-04 10:55:14 +0000] [18257] [INFO] Application shutdown complete.
[2023-10-04 10:55:14 +0000] [18257] [INFO] Finished server process [18257]
[2023-10-04 10:55:14 +0000] [18257] [INFO] Worker exiting (pid: 18257)
[2023-10-04 10:55:16 +0000] [18240] [INFO] Shutting down: Master
[2023-10-04 10:55:36 +0000] [657] [INFO] Starting gunicorn 21.2.0
[2023-10-04 10:55:36 +0000] [657] [INFO] Listening at: http://0.0.0.0:9000 (657)
[2023-10-04 10:55:36 +0000] [657] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 10:55:36 +0000] [729] [INFO] Booting worker with pid: 729
[2023-10-04 10:55:36 +0000] [775] [INFO] Booting worker with pid: 775
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 10:56:04.912696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:56:04.912697: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
[2023-10-04 10:56:06 +0000] [657] [CRITICAL] WORKER TIMEOUT (pid:729)
[2023-10-04 10:56:06 +0000] [657] [CRITICAL] WORKER TIMEOUT (pid:775)
[2023-10-04 10:56:07 +0000] [657] [ERROR] Worker (pid:729) was sent code 134!
[2023-10-04 10:56:07 +0000] [2229] [INFO] Booting worker with pid: 2229
[2023-10-04 10:56:07 +0000] [657] [ERROR] Worker (pid:775) was sent code 134!
[2023-10-04 10:56:07 +0000] [2230] [INFO] Booting worker with pid: 2230
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 10:56:15.330112: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:56:15.682836: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:56:18.172595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:56:18.174520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:56:24.501314: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-10-04 10:56:24.517853: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2023-10-04 10:56:27 +0000] [2230] [INFO] Started server process [2230]
[2023-10-04 10:56:27 +0000] [2230] [INFO] Waiting for application startup.
[2023-10-04 10:56:27 +0000] [2230] [INFO] Application startup complete.
[2023-10-04 10:56:27 +0000] [2229] [INFO] Started server process [2229]
[2023-10-04 10:56:27 +0000] [2229] [INFO] Waiting for application startup.
[2023-10-04 10:56:27 +0000] [2229] [INFO] Application startup complete.
[2023-10-04 10:57:52 +0000] [657] [INFO] Handling signal: term
[2023-10-04 10:57:52 +0000] [2229] [INFO] Shutting down
[2023-10-04 10:57:52 +0000] [2230] [INFO] Shutting down
[2023-10-04 10:57:53 +0000] [2229] [INFO] Waiting for application shutdown.
[2023-10-04 10:57:53 +0000] [2229] [INFO] Application shutdown complete.
[2023-10-04 10:57:53 +0000] [2229] [INFO] Finished server process [2229]
[2023-10-04 10:57:53 +0000] [2229] [INFO] Worker exiting (pid: 2229)
[2023-10-04 10:57:53 +0000] [2230] [INFO] Waiting for application shutdown.
[2023-10-04 10:57:53 +0000] [2230] [INFO] Application shutdown complete.
[2023-10-04 10:57:53 +0000] [2230] [INFO] Finished server process [2230]
[2023-10-04 10:57:53 +0000] [2230] [INFO] Worker exiting (pid: 2230)
[2023-10-04 10:57:54 +0000] [657] [INFO] Shutting down: Master
[2023-10-04 10:57:57 +0000] [20386] [INFO] Starting gunicorn 21.2.0
[2023-10-04 10:57:57 +0000] [20386] [INFO] Listening at: http://0.0.0.0:9000 (20386)
[2023-10-04 10:57:57 +0000] [20386] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 10:57:57 +0000] [20398] [INFO] Booting worker with pid: 20398
[2023-10-04 10:57:57 +0000] [20402] [INFO] Booting worker with pid: 20402
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 10:58:09.284562: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:58:09.416464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 10:58:10.377635: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:58:10.508954: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 10:58:11.915012: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-10-04 10:58:12.009385: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2023-10-04 10:58:13 +0000] [20398] [INFO] Started server process [20398]
[2023-10-04 10:58:13 +0000] [20398] [INFO] Waiting for application startup.
[2023-10-04 10:58:13 +0000] [20398] [INFO] Application startup complete.
[2023-10-04 10:58:13 +0000] [20402] [INFO] Started server process [20402]
[2023-10-04 10:58:13 +0000] [20402] [INFO] Waiting for application startup.
[2023-10-04 10:58:13 +0000] [20402] [INFO] Application startup complete.
[2023-10-04 11:01:04 +0000] [20386] [INFO] Handling signal: term
[2023-10-04 11:01:04 +0000] [20402] [INFO] Shutting down
[2023-10-04 11:01:04 +0000] [20398] [INFO] Shutting down
[2023-10-04 11:01:04 +0000] [20402] [INFO] Waiting for application shutdown.
[2023-10-04 11:01:04 +0000] [20402] [INFO] Application shutdown complete.
[2023-10-04 11:01:04 +0000] [20402] [INFO] Finished server process [20402]
[2023-10-04 11:01:04 +0000] [20402] [INFO] Worker exiting (pid: 20402)
[2023-10-04 11:01:04 +0000] [20398] [INFO] Waiting for application shutdown.
[2023-10-04 11:01:04 +0000] [20398] [INFO] Application shutdown complete.
[2023-10-04 11:01:04 +0000] [20398] [INFO] Finished server process [20398]
[2023-10-04 11:01:04 +0000] [20398] [INFO] Worker exiting (pid: 20398)
[2023-10-04 11:01:05 +0000] [20386] [INFO] Shutting down: Master
[2023-10-04 11:01:08 +0000] [37092] [INFO] Starting gunicorn 21.2.0
[2023-10-04 11:01:08 +0000] [37092] [INFO] Listening at: http://0.0.0.0:9000 (37092)
[2023-10-04 11:01:08 +0000] [37092] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 11:01:08 +0000] [37105] [INFO] Booting worker with pid: 37105
[2023-10-04 11:01:08 +0000] [37108] [INFO] Booting worker with pid: 37108
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
2023-10-04 11:01:18.422522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:01:19.126580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:01:19.600807: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:01:19.914785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:01:21.367349: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2023-10-04 11:01:21.403440: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
[2023-10-04 11:01:22 +0000] [37105] [INFO] Started server process [37105]
[2023-10-04 11:01:22 +0000] [37105] [INFO] Waiting for application startup.
[2023-10-04 11:01:22 +0000] [37105] [INFO] Application startup complete.
[2023-10-04 11:01:22 +0000] [37108] [INFO] Started server process [37108]
[2023-10-04 11:01:22 +0000] [37108] [INFO] Waiting for application startup.
[2023-10-04 11:01:22 +0000] [37108] [INFO] Application startup complete.
[2023-10-04 11:01:24 +0000] [37092] [INFO] Handling signal: term
[2023-10-04 11:01:24 +0000] [37108] [INFO] Shutting down
[2023-10-04 11:01:24 +0000] [37105] [INFO] Shutting down
[2023-10-04 11:01:24 +0000] [37108] [INFO] Waiting for application shutdown.
[2023-10-04 11:01:24 +0000] [37108] [INFO] Application shutdown complete.
[2023-10-04 11:01:24 +0000] [37108] [INFO] Finished server process [37108]
[2023-10-04 11:01:24 +0000] [37108] [INFO] Worker exiting (pid: 37108)
[2023-10-04 11:01:24 +0000] [37105] [INFO] Waiting for application shutdown.
[2023-10-04 11:01:24 +0000] [37105] [INFO] Application shutdown complete.
[2023-10-04 11:01:24 +0000] [37105] [INFO] Finished server process [37105]
[2023-10-04 11:01:24 +0000] [37105] [INFO] Worker exiting (pid: 37105)
[2023-10-04 11:01:25 +0000] [37092] [INFO] Shutting down: Master
[2023-10-04 11:01:53 +0000] [666] [INFO] Starting gunicorn 21.2.0
[2023-10-04 11:01:53 +0000] [666] [INFO] Listening at: http://0.0.0.0:9000 (666)
[2023-10-04 11:01:53 +0000] [666] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 11:01:53 +0000] [688] [INFO] Booting worker with pid: 688
[2023-10-04 11:01:53 +0000] [708] [INFO] Booting worker with pid: 708
[2023-10-04 11:02:23 +0000] [666] [CRITICAL] WORKER TIMEOUT (pid:688)
[2023-10-04 11:02:23 +0000] [666] [CRITICAL] WORKER TIMEOUT (pid:708)
[2023-10-04 11:02:23 +0000] [666] [ERROR] Worker (pid:708) was sent code 134!
[2023-10-04 11:02:23 +0000] [2227] [INFO] Booting worker with pid: 2227
[2023-10-04 11:02:23 +0000] [666] [ERROR] Worker (pid:688) was sent code 134!
[2023-10-04 11:02:23 +0000] [2228] [INFO] Booting worker with pid: 2228
2023-10-04 11:02:35.161825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:02:35.161832: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:02:36.943568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:02:36.943578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:02:41.282151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:02:41.282151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:02:41.294030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:02:41.294043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:02:41.295507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:02:41.296675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 11:02:43 +0000] [2228] [INFO] Started server process [2228]
[2023-10-04 11:02:43 +0000] [2228] [INFO] Waiting for application startup.
[2023-10-04 11:02:43 +0000] [2227] [INFO] Started server process [2227]
[2023-10-04 11:02:43 +0000] [2228] [INFO] Application startup complete.
[2023-10-04 11:02:43 +0000] [2227] [INFO] Waiting for application startup.
[2023-10-04 11:02:43 +0000] [2227] [INFO] Application startup complete.
[2023-10-04 11:08:41 +0000] [666] [INFO] Handling signal: term
[2023-10-04 11:08:41 +0000] [2227] [INFO] Shutting down
[2023-10-04 11:08:41 +0000] [2228] [INFO] Shutting down
[2023-10-04 11:08:41 +0000] [2228] [INFO] Waiting for application shutdown.
[2023-10-04 11:08:41 +0000] [2227] [INFO] Waiting for application shutdown.
[2023-10-04 11:08:41 +0000] [2228] [INFO] Application shutdown complete.
[2023-10-04 11:08:41 +0000] [2227] [INFO] Application shutdown complete.
[2023-10-04 11:08:41 +0000] [2228] [INFO] Finished server process [2228]
[2023-10-04 11:08:41 +0000] [2227] [INFO] Finished server process [2227]
[2023-10-04 11:08:41 +0000] [2228] [INFO] Worker exiting (pid: 2228)
[2023-10-04 11:08:41 +0000] [2227] [INFO] Worker exiting (pid: 2227)
[2023-10-04 11:08:43 +0000] [666] [INFO] Shutting down: Master
[2023-10-04 11:08:46 +0000] [18084] [INFO] Starting gunicorn 21.2.0
[2023-10-04 11:08:46 +0000] [18084] [INFO] Listening at: http://0.0.0.0:9000 (18084)
[2023-10-04 11:08:46 +0000] [18084] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 11:08:46 +0000] [18096] [INFO] Booting worker with pid: 18096
[2023-10-04 11:08:46 +0000] [18101] [INFO] Booting worker with pid: 18101
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
/home/ubuntu/intent_generation/venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
[2023-10-04 11:08:54 +0000] [18084] [INFO] Handling signal: term
[2023-10-04 11:08:54 +0000] [18084] [ERROR] Worker (pid:18096) was sent SIGTERM!
[2023-10-04 11:08:54 +0000] [18084] [ERROR] Worker (pid:18101) was sent SIGTERM!
[2023-10-04 11:08:54 +0000] [18084] [INFO] Shutting down: Master
[2023-10-04 11:09:17 +0000] [684] [INFO] Starting gunicorn 21.2.0
[2023-10-04 11:09:17 +0000] [684] [INFO] Listening at: http://0.0.0.0:9000 (684)
[2023-10-04 11:09:17 +0000] [684] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-04 11:09:17 +0000] [734] [INFO] Booting worker with pid: 734
[2023-10-04 11:09:17 +0000] [762] [INFO] Booting worker with pid: 762
2023-10-04 11:09:43.779201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:09:43.779201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:09:45.607563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:09:45.607568: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2023-10-04 11:09:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:734)
[2023-10-04 11:09:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:762)
[2023-10-04 11:09:48 +0000] [684] [ERROR] Worker (pid:734) was sent code 134!
[2023-10-04 11:09:48 +0000] [1943] [INFO] Booting worker with pid: 1943
[2023-10-04 11:09:48 +0000] [684] [ERROR] Worker (pid:762) was sent code 134!
[2023-10-04 11:09:48 +0000] [1944] [INFO] Booting worker with pid: 1944
2023-10-04 11:09:54.915810: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:09:55.019464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 11:09:55.736857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:09:55.861418: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 11:09:57.847100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:09:57.847100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:09:57.869624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:09:57.869680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:09:57.881344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 11:09:57.881750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 11:10:00 +0000] [1943] [INFO] Started server process [1943]
[2023-10-04 11:10:00 +0000] [1944] [INFO] Started server process [1944]
[2023-10-04 11:10:00 +0000] [1944] [INFO] Waiting for application startup.
[2023-10-04 11:10:00 +0000] [1943] [INFO] Waiting for application startup.
[2023-10-04 11:10:00 +0000] [1944] [INFO] Application startup complete.
[2023-10-04 11:10:00 +0000] [1943] [INFO] Application startup complete.
[2023-10-04 12:01:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:1943)
[2023-10-04 12:03:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:1944)
[2023-10-04 12:03:04 +0000] [684] [ERROR] Worker (pid:1943) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:03:04 +0000] [8267] [INFO] Booting worker with pid: 8267
[2023-10-04 12:03:04 +0000] [684] [ERROR] Worker (pid:1944) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:03:04 +0000] [8269] [INFO] Booting worker with pid: 8269
[2023-10-04 12:03:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:8267)
[2023-10-04 12:05:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:8269)
[2023-10-04 12:07:08 +0000] [684] [ERROR] Worker (pid:8267) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:07:08 +0000] [8761] [INFO] Booting worker with pid: 8761
[2023-10-04 12:07:08 +0000] [684] [ERROR] Worker (pid:8269) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:07:08 +0000] [8762] [INFO] Booting worker with pid: 8762
[2023-10-04 12:08:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:8761)
[2023-10-04 12:10:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:8762)
[2023-10-04 12:12:11 +0000] [684] [ERROR] Worker (pid:8761) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:12:11 +0000] [8858] [INFO] Booting worker with pid: 8858
[2023-10-04 12:12:11 +0000] [684] [ERROR] Worker (pid:8762) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:12:11 +0000] [8859] [INFO] Booting worker with pid: 8859
[2023-10-04 12:12:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:8859)
[2023-10-04 12:14:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:8858)
[2023-10-04 12:16:03 +0000] [684] [ERROR] Worker (pid:8859) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:16:03 +0000] [9013] [INFO] Booting worker with pid: 9013
[2023-10-04 12:16:04 +0000] [684] [ERROR] Worker (pid:8858) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:16:04 +0000] [9015] [INFO] Booting worker with pid: 9015
[2023-10-04 12:16:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9015)
[2023-10-04 12:18:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9013)
[2023-10-04 12:20:52 +0000] [684] [ERROR] Worker (pid:9015) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:20:52 +0000] [9092] [INFO] Booting worker with pid: 9092
[2023-10-04 12:20:52 +0000] [684] [ERROR] Worker (pid:9013) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:20:52 +0000] [9095] [INFO] Booting worker with pid: 9095
2023-10-04 12:21:12.120378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:21:12.120378: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:21:13.908497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:21:13.908498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:21:16.682820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:16.682820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:16.699777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:16.699824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:16.704076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:16.704330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:19.616681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:19.616727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:19.621165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:19.621411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:19.625582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:19.625831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 12:21:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9092)
[2023-10-04 12:21:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9095)
[2023-10-04 12:21:23 +0000] [684] [ERROR] Worker (pid:9092) was sent code 134!
[2023-10-04 12:21:23 +0000] [9211] [INFO] Booting worker with pid: 9211
[2023-10-04 12:21:23 +0000] [684] [ERROR] Worker (pid:9095) was sent code 134!
[2023-10-04 12:21:23 +0000] [9212] [INFO] Booting worker with pid: 9212
2023-10-04 12:21:34.975871: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:21:34.977327: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:21:36.324400: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:21:36.324399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:21:37.634900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:37.634900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:37.645378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:37.645378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:37.649836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:37.650102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:39.702243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:39.703592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:39.705567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:39.708040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:39.710025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:39.712498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.131914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.134444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.136329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.138226: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:21:41.293418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.295329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.297077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:21:41.298842: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:21:42.370950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.372077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.373124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.374152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.375229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.376250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.377267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.378297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.379359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.380381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.381413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.382445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.383529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.384561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.385598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.386690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.387735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.388764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.389792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.390855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.391873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.392888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.393899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:21:42.394955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:22:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9211)
[2023-10-04 12:23:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9212)
[2023-10-04 12:24:09 +0000] [684] [ERROR] Worker (pid:9211) was sent code 134!
[2023-10-04 12:24:09 +0000] [9296] [INFO] Booting worker with pid: 9296
[2023-10-04 12:24:09 +0000] [684] [ERROR] Worker (pid:9212) was sent code 134!
[2023-10-04 12:24:09 +0000] [9297] [INFO] Booting worker with pid: 9297
2023-10-04 12:24:27.603778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:24:27.603778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:24:29.343329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:24:29.343329: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:24:31.957086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:31.957086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:31.971460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:31.971980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:31.976645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:31.977614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:34.809868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:34.810090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:34.814295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:34.814541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:34.818738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:34.818994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.542618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.545089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.547650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.550152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:24:37.551658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.553669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.555641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:24:37.557471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:24:38.632585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.634070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.635120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.636141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.637139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.638151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.639195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.640197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.641192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.642200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.643249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.644243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.645257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.646264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.647311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.648316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.649326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.650344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.651394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.652542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.653563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.654567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.655587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:24:38.656602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:24:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9296)
[2023-10-04 12:24:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9297)
[2023-10-04 12:24:40 +0000] [684] [ERROR] Worker (pid:9296) was sent code 134!
[2023-10-04 12:24:40 +0000] [9375] [INFO] Booting worker with pid: 9375
[2023-10-04 12:24:40 +0000] [684] [ERROR] Worker (pid:9297) was sent code 134!
[2023-10-04 12:24:40 +0000] [9376] [INFO] Booting worker with pid: 9376
2023-10-04 12:24:55.916489: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:24:55.916489: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:24:57.616118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:24:57.616118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:25:00.365672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:00.365672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:00.379832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:00.379874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:00.384245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:00.384499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:03.174447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:03.174447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:03.178806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:03.179056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:03.183205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:03.183451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.878909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.880014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.883256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.884982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.888190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.889740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:25:04.892920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:25:04.893685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:25:06.001629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.002789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.003756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.004724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.005688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.006707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.007673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.008976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.010228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.011808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.013414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.015066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.016704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.018259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.019915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.021521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.023169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.024719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.026331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.027952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.029537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.031172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.032762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:25:06.034352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:25:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9376)
[2023-10-04 12:26:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9375)
[2023-10-04 12:28:03 +0000] [684] [ERROR] Worker (pid:9376) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:28:03 +0000] [9436] [INFO] Booting worker with pid: 9436
[2023-10-04 12:28:03 +0000] [684] [ERROR] Worker (pid:9375) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:28:03 +0000] [9454] [INFO] Booting worker with pid: 9454
2023-10-04 12:28:22.497967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:28:22.497967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:28:24.208642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:28:24.208644: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:28:26.913333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:26.913333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:26.928518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:26.928546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:26.932868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:26.933121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:29.794057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:29.794165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:29.798472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:29.798728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:29.802923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:29.803179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.413606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.417070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.419121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.420989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:28:32.489767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.491671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.493272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:32.494675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:28:33.564439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.565654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.566715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.567732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.568742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.569760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.570832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.571847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.572856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.573866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.574924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.575935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.576967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.577984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.579051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.580070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.581079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.582091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.583150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.584157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.585174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.586186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.587250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:33.588261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:28:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9436)
[2023-10-04 12:28:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9454)
[2023-10-04 12:28:34 +0000] [684] [ERROR] Worker (pid:9436) was sent code 134!
[2023-10-04 12:28:34 +0000] [9564] [INFO] Booting worker with pid: 9564
[2023-10-04 12:28:34 +0000] [684] [ERROR] Worker (pid:9454) was sent code 134!
[2023-10-04 12:28:34 +0000] [9565] [INFO] Booting worker with pid: 9565
2023-10-04 12:28:50.506759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:28:50.506759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:28:52.216038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:28:52.216038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:28:54.769351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:54.769351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:54.779470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:54.779470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:54.783896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:54.784154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:57.375721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:57.375836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:57.380202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:57.380453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:57.384662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:57.384905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.803414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.806030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.807682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.809222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:28:58.840240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.842831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.845270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:28:58.847341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:28:59.932882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.933978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.935001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.935965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.936928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.937897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.938965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.939964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.940964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.941958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.943008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.944004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.945016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.946014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.947061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.948054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.949050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.950046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.951074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.952071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.953064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.954054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.955103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:28:59.956102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:29:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9565)
[2023-10-04 12:30:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9564)
[2023-10-04 12:31:21 +0000] [684] [ERROR] Worker (pid:9565) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:31:21 +0000] [9627] [INFO] Booting worker with pid: 9627
[2023-10-04 12:31:22 +0000] [684] [ERROR] Worker (pid:9564) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:31:22 +0000] [9639] [INFO] Booting worker with pid: 9639
2023-10-04 12:31:40.514911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:31:40.514911: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:31:42.221551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:31:42.221555: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:31:44.909049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:44.909049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:44.924562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:44.924602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:44.929044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:44.929305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:47.710231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:47.710231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:47.714659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:47.714912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:47.719156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:47.719405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.302045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.305372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.307239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.309476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:31:50.354314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.356352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.358142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:31:50.360000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:31:51.457541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.458624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.459622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.460601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.461604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.462647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.463661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.464652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.465655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.466710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.467726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.468739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.469763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.470819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.471836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.472852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.473853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.474916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.475927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.476918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.477915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.478987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.479999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:31:51.480994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:31:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9627)
[2023-10-04 12:31:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9639)
[2023-10-04 12:31:52 +0000] [684] [ERROR] Worker (pid:9639) was sent code 134!
[2023-10-04 12:31:52 +0000] [9728] [INFO] Booting worker with pid: 9728
[2023-10-04 12:31:52 +0000] [684] [ERROR] Worker (pid:9627) was sent code 134!
[2023-10-04 12:31:52 +0000] [9729] [INFO] Booting worker with pid: 9729
2023-10-04 12:32:08.381938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:32:08.381938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:32:10.056883: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:32:10.056881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:32:12.781226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:12.781226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:12.795561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:12.795602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:12.799859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:12.800115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:15.702287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:15.702287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:15.706687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:15.706936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:15.711124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:15.711373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.132785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.135558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.137416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.139179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:32:17.240822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.242757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.244603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:32:17.246316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:32:18.320385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.322008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.323651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.325230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.326865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.328460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.330027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.331660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.333253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.334865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.336439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.338015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.339647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.341044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.342617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.343804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.345378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.347041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.348660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.350296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.351951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.353579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.355251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:32:18.356880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:32:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9729)
[2023-10-04 12:32:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9728)
[2023-10-04 12:33:48 +0000] [684] [ERROR] Worker (pid:9729) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:33:48 +0000] [9812] [INFO] Booting worker with pid: 9812
[2023-10-04 12:33:49 +0000] [684] [ERROR] Worker (pid:9728) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:33:49 +0000] [9815] [INFO] Booting worker with pid: 9815
2023-10-04 12:34:08.447255: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:34:08.447255: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:34:10.249652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:34:10.249652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:34:13.024898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:13.024898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:13.042676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:13.042676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:13.047142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:13.047403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:15.915226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:15.915288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:15.919611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:15.919861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:15.924051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:15.924293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.679738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.682191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.683946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.686249: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:34:18.690443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.692987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.695581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:18.698089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 12:34:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9812)
[2023-10-04 12:34:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9815)
[2023-10-04 12:34:20 +0000] [684] [ERROR] Worker (pid:9815) was sent code 134!
[2023-10-04 12:34:20 +0000] [9944] [INFO] Booting worker with pid: 9944
[2023-10-04 12:34:20 +0000] [684] [ERROR] Worker (pid:9812) was sent code 134!
[2023-10-04 12:34:20 +0000] [9945] [INFO] Booting worker with pid: 9945
2023-10-04 12:34:32.374546: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:34:32.374549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:34:33.483874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:34:33.483874: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:34:34.706290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:34.706290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:34.715741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:34.715742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:34.720020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:34.720274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:36.706519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:36.706542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:36.708908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:36.713111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:36.713362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:36.717703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.169053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.171800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.173499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.175275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:34:38.236961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.239450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.241819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:34:38.244134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:34:39.377221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.378521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.379565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.380963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.382647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.384140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.385653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.387307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.388754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.390126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.391829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.393471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.395170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.396551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.398219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.399911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.401560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.403253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.404913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.406560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.408223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.409853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.411314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:34:39.412949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:35:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9944)
[2023-10-04 12:36:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9945)
[2023-10-04 12:37:35 +0000] [684] [ERROR] Worker (pid:9944) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:37:35 +0000] [9999] [INFO] Booting worker with pid: 9999
[2023-10-04 12:37:35 +0000] [684] [ERROR] Worker (pid:9945) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:37:35 +0000] [10010] [INFO] Booting worker with pid: 10010
2023-10-04 12:37:55.190123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:37:55.190123: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:37:56.901587: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:37:56.901585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:37:59.683995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:37:59.683995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:37:59.699037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:37:59.699079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:37:59.703360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:37:59.703610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:02.574264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:02.574285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:02.578686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:02.578954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:02.583139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:02.583389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.104669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.107405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.109888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.112350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:38:05.274657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.276645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.278283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:05.280019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 12:38:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:9999)
[2023-10-04 12:38:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10010)
[2023-10-04 12:38:06 +0000] [684] [ERROR] Worker (pid:10010) was sent code 134!
[2023-10-04 12:38:06 +0000] [10188] [INFO] Booting worker with pid: 10188
[2023-10-04 12:38:06 +0000] [684] [ERROR] Worker (pid:9999) was sent code 134!
[2023-10-04 12:38:06 +0000] [10189] [INFO] Booting worker with pid: 10189
2023-10-04 12:38:18.536757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:38:18.555531: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:38:19.822156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:38:19.822156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:38:21.165726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:21.165726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:21.173777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:21.173781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:21.178062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:21.178318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:22.918530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:22.919027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:22.922514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:22.923684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:22.927164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:22.928314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.424341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.426640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.428422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.430352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:38:24.630096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.632332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.634295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:38:24.636243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:38:25.699583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.700629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.701627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.702674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.703667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.704654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.705662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.706719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.707720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.708727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.709731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.710813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.711826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.712824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.713832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.714880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.715882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.716892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.717897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.718954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.719957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.720953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.721962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:38:25.723013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:38:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10188)
[2023-10-04 12:39:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10189)
[2023-10-04 12:40:59 +0000] [684] [ERROR] Worker (pid:10188) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:40:59 +0000] [10255] [INFO] Booting worker with pid: 10255
[2023-10-04 12:41:00 +0000] [684] [ERROR] Worker (pid:10189) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:41:00 +0000] [10256] [INFO] Booting worker with pid: 10256
2023-10-04 12:41:19.046638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:41:19.046638: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:41:20.794670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:41:20.794674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:41:23.378098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:23.378098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:23.394647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:23.394653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:23.398956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:23.399211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:26.144202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:26.144201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:26.148652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:26.148904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:26.153134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:26.153386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.699223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.701315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.703246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.704859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:41:28.720480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.722720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.724375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:28.726193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:41:29.798473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.799766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.800751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.801740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.802760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.804052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.805673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.807319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.808928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.810561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.812030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.813128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.814805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.816421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.818045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.819683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.821249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.822441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.823913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.825503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.827124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.828705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.830250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:29.831857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:41:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10255)
[2023-10-04 12:41:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10256)
[2023-10-04 12:41:31 +0000] [684] [ERROR] Worker (pid:10256) was sent code 134!
[2023-10-04 12:41:31 +0000] [10388] [INFO] Booting worker with pid: 10388
[2023-10-04 12:41:31 +0000] [684] [ERROR] Worker (pid:10255) was sent code 134!
[2023-10-04 12:41:31 +0000] [10389] [INFO] Booting worker with pid: 10389
2023-10-04 12:41:46.564463: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:41:46.564460: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:41:48.260286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:41:48.260285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:41:50.858166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:50.858167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:50.872871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:50.872918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:50.877170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:50.877426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:53.698059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:53.698479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:53.702317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:53.703014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:53.706798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:53.707489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.096267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.098575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.100915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.103443: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:41:55.294493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.297207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.299774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:41:55.302287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:41:56.393090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.394185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.395247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.396254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.397255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.398278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.399344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.400347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.401346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.402352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.403418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.404415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.405426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.406433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.407469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.408467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.409460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.410464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.411490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.412480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.413473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.414480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.415560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:41:56.416560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:42:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10389)
[2023-10-04 12:43:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10388)
[2023-10-04 12:44:30 +0000] [684] [ERROR] Worker (pid:10389) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:44:30 +0000] [10446] [INFO] Booting worker with pid: 10446
[2023-10-04 12:44:31 +0000] [684] [ERROR] Worker (pid:10388) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:44:31 +0000] [10449] [INFO] Booting worker with pid: 10449
2023-10-04 12:44:49.882453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:44:49.882453: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:44:51.584831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:44:51.584831: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:44:54.209996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:54.209996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:54.224891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:54.224894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:54.229335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:54.229597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:57.068758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:57.068903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:57.073169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:57.073422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:57.077612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:57.077856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.814997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.818296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.820572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.822461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:44:59.870064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.872281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.874117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:44:59.876038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:45:00.968023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.969676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.971259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.972578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.974061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.975306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.976756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.978394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.979942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.981309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.982496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.983697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.984860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.986238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.987419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.988542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.990230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.991523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.992715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.994158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.995900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.997617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:00.999354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:01.001072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:45:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10446)
[2023-10-04 12:45:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10449)
[2023-10-04 12:45:02 +0000] [684] [ERROR] Worker (pid:10446) was sent code 134!
[2023-10-04 12:45:02 +0000] [10566] [INFO] Booting worker with pid: 10566
[2023-10-04 12:45:02 +0000] [684] [ERROR] Worker (pid:10449) was sent code 134!
[2023-10-04 12:45:02 +0000] [10567] [INFO] Booting worker with pid: 10567
2023-10-04 12:45:18.010979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:45:18.010979: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:45:19.688657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:45:19.688661: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:45:22.367749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:22.367749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:22.379853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:22.379854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:22.384104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:22.384359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:25.009617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:25.009673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:25.013999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:25.014255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:25.018422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:25.018690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.505262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.507559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.509297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.511115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:45:26.512978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.515048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.516798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:45:26.518513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:45:27.636712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.637708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.638722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.639693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.640663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.641630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.642635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.643601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.644564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.645529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.646500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.647498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.648480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.649439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.650455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.651963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.653582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.655235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.656853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.658483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.660111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.661721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.663368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:45:27.664995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:45:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10567)
[2023-10-04 12:46:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10566)
[2023-10-04 12:46:59 +0000] [684] [ERROR] Worker (pid:10567) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:46:59 +0000] [10629] [INFO] Booting worker with pid: 10629
[2023-10-04 12:46:59 +0000] [684] [ERROR] Worker (pid:10566) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:46:59 +0000] [10632] [INFO] Booting worker with pid: 10632
2023-10-04 12:47:18.012190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:47:18.012190: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:47:19.695681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:47:19.695685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:47:22.283647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:22.283647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:22.297578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:22.297578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:22.302011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:22.302274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:25.183523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:25.183643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:25.188095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:25.188341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:25.192711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:25.192965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.696949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.696959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.701810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.702611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.706903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.707721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:27.712795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:47:27.712797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:47:28.799463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.800524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.801509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.802501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.803515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.804501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.805478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.806804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.808405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.809975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.811601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.813195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.814831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.816430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.817987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.819618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.821027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.822656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.824059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.825678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.827308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.828926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.830542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:28.832106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:47:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10629)
[2023-10-04 12:47:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10632)
[2023-10-04 12:47:30 +0000] [684] [ERROR] Worker (pid:10632) was sent code 134!
[2023-10-04 12:47:30 +0000] [10759] [INFO] Booting worker with pid: 10759
[2023-10-04 12:47:30 +0000] [684] [ERROR] Worker (pid:10629) was sent code 134!
[2023-10-04 12:47:30 +0000] [10760] [INFO] Booting worker with pid: 10760
2023-10-04 12:47:46.340913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:47:46.340913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:47:48.151031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:47:48.151031: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:47:51.163882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:51.163883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:51.181421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:51.181449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:51.185653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:51.185907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:54.066023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:54.066306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:54.070420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:54.070686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:54.074835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:54.075088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.790842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.793687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.795651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.797549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:47:55.835655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.838318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.840922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:47:55.843407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:47:56.926479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.927605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.928593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.929579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.930626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.931639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.932650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.933664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.934728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.935727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.936735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.937731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.938796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.939790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.940798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.941827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.942924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.943919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.944966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.945968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.947003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.947998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.948989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:47:56.949982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:48:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10759)
[2023-10-04 12:49:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10760)
[2023-10-04 12:50:22 +0000] [684] [ERROR] Worker (pid:10759) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:50:22 +0000] [10814] [INFO] Booting worker with pid: 10814
[2023-10-04 12:50:23 +0000] [684] [ERROR] Worker (pid:10760) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:50:23 +0000] [10815] [INFO] Booting worker with pid: 10815
2023-10-04 12:50:41.972572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:50:41.972572: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:50:43.655750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:50:43.655751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:50:46.318881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:46.318881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:46.334949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:46.334987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:46.339240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:46.339499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:49.118786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:49.118813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:49.123195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:49.123444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:49.127635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:49.127886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.792149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.794925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.797374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.799440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:50:51.928833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.930808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.932525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:50:51.934213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:50:53.068519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.070188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.071641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.072907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.074007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.075371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.076870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.078553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.080538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.081685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.083192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.085001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.086651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.088125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.089591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.091435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.092900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.094324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.096184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.097638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.099168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.100705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.101774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:50:53.102889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:50:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10814)
[2023-10-04 12:50:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10815)
[2023-10-04 12:50:53 +0000] [684] [ERROR] Worker (pid:10815) was sent code 134!
[2023-10-04 12:50:53 +0000] [10920] [INFO] Booting worker with pid: 10920
[2023-10-04 12:50:53 +0000] [684] [ERROR] Worker (pid:10814) was sent code 134!
[2023-10-04 12:50:53 +0000] [10921] [INFO] Booting worker with pid: 10921
2023-10-04 12:51:07.871051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:51:07.889693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:51:09.453450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:51:09.453453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:51:11.776585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:11.776585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:11.787216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:11.787218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:11.791497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:11.791752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:14.341646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:14.341646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:14.346041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:14.346298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:14.350493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:14.350741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.805109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.807957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.810455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.812894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:51:15.839359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.841284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.843509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:51:15.845973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:51:16.953507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.954657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.955623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.956583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.957545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.958611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.960328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.961934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.963587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.965204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.966854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.968378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.970000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.971650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.973267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.974900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.976493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.978085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.979685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.980703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.981667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.982675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.983644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:51:16.984604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:51:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10920)
[2023-10-04 12:52:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:10921)
[2023-10-04 12:54:09 +0000] [684] [ERROR] Worker (pid:10920) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:54:09 +0000] [11005] [INFO] Booting worker with pid: 11005
[2023-10-04 12:54:09 +0000] [684] [ERROR] Worker (pid:10921) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:54:09 +0000] [11006] [INFO] Booting worker with pid: 11006
2023-10-04 12:54:28.541798: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:54:28.541798: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:54:30.312564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:54:30.312564: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:54:33.077398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:33.077398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:33.093169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:33.093228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:33.097471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:33.097728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:35.900595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:35.900595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:35.905024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:35.905273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:35.909341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:35.909593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.816960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.820312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.822923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.825295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:54:38.891568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.893448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.895349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:38.897097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:54:39.977163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.978787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.980018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.981709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.983529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.985129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.986745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.988462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.989729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.991382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.993137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.994937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.996570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.998113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:39.999481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.000702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.001967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.003179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.004564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.005811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.007068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.008323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.009535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:54:40.010846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:54:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11005)
[2023-10-04 12:54:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11006)
[2023-10-04 12:54:40 +0000] [684] [ERROR] Worker (pid:11006) was sent code 134!
[2023-10-04 12:54:40 +0000] [11105] [INFO] Booting worker with pid: 11105
[2023-10-04 12:54:40 +0000] [684] [ERROR] Worker (pid:11005) was sent code 134!
[2023-10-04 12:54:40 +0000] [11106] [INFO] Booting worker with pid: 11106
2023-10-04 12:54:53.752404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:54:53.770418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:54:55.406681: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:54:55.406682: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:54:57.724334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:57.724334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:57.734329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:57.734333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:57.738677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:54:57.738935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:00.379701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:00.379767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:00.384123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:00.384371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:00.388591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:00.388840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:01.859855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:01.861690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:01.863574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:01.865115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:55:01.997567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:01.999619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:02.001364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:55:02.003091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:55:03.086198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.087314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.088300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.089820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.091429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.093046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.094692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.096175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.097781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.099422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.101003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.102649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.104306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.105932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.107091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.108062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.109025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.109989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.110993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.111960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.112923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.113882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.115357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:55:03.116959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:55:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11106)
[2023-10-04 12:55:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11105)
[2023-10-04 12:55:53 +0000] [684] [ERROR] Worker (pid:11106) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:55:53 +0000] [11156] [INFO] Booting worker with pid: 11156
[2023-10-04 12:55:54 +0000] [684] [ERROR] Worker (pid:11105) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:55:54 +0000] [11158] [INFO] Booting worker with pid: 11158
2023-10-04 12:56:12.164404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:56:12.164404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:56:13.848877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:56:13.848877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:56:16.556598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:16.556598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:16.569865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:16.569901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:16.574338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:16.574623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:19.355739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:19.355743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:19.360119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:19.360376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:19.364525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:19.364788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:21.990076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:21.992847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:21.994908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:21.997191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:56:22.101742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:22.104349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:22.106855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:22.109311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:56:23.213104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.214838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.216431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.218025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.219674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.221259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.222753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.224342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.225877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.227504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.228856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.229986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.231144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.232268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.233670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.235038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.236489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.237659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.239117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.240713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.242223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.243588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.245069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:23.246398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:56:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11156)
[2023-10-04 12:56:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11158)
[2023-10-04 12:56:25 +0000] [684] [ERROR] Worker (pid:11158) was sent code 134!
[2023-10-04 12:56:25 +0000] [11276] [INFO] Booting worker with pid: 11276
[2023-10-04 12:56:25 +0000] [684] [ERROR] Worker (pid:11156) was sent code 134!
[2023-10-04 12:56:25 +0000] [11277] [INFO] Booting worker with pid: 11277
2023-10-04 12:56:40.625435: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:56:40.625431: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:56:42.365584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:56:42.365583: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:56:45.100276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:45.100276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:45.115633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:45.115636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:45.120052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:45.120314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:47.928113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:47.928157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:47.932507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:47.932757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:47.936915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:47.937166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.637339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.639636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.641495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.643352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:56:49.758289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.760122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.761850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:56:49.763524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:56:50.864822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.866468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.868113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.869554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.870972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.872357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.873781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.875260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.876573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.877991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.879293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.880504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.881819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.883447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.885056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.886660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.888251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.889844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.891489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.893099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.894731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.896335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.897942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 12:56:50.899576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 12:57:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11276)
[2023-10-04 12:57:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11277)
[2023-10-04 12:59:15 +0000] [684] [ERROR] Worker (pid:11276) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:59:15 +0000] [11330] [INFO] Booting worker with pid: 11330
[2023-10-04 12:59:15 +0000] [684] [ERROR] Worker (pid:11277) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 12:59:15 +0000] [11331] [INFO] Booting worker with pid: 11331
2023-10-04 12:59:35.003085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:59:35.003085: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:59:36.774808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:59:36.774808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 12:59:39.625050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:39.625050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:39.639380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:39.639415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:39.643656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:39.643914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:42.498486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:42.498509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:42.502922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:42.503175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:42.507369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:42.507619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.098680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.101514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.104045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.106656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19028 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 12:59:45.196526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.199717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.202505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 12:59:45.205236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 12:59:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11330)
[2023-10-04 12:59:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11331)
[2023-10-04 12:59:46 +0000] [684] [ERROR] Worker (pid:11330) was sent code 134!
[2023-10-04 12:59:46 +0000] [11428] [INFO] Booting worker with pid: 11428
[2023-10-04 12:59:46 +0000] [684] [ERROR] Worker (pid:11331) was sent code 134!
[2023-10-04 12:59:46 +0000] [11429] [INFO] Booting worker with pid: 11429
2023-10-04 12:59:59.273877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 12:59:59.290886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:00:00.448063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:00:00.448063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:00:01.722767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:01.722767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:01.732468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:01.732473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:01.736928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:01.737191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:03.593057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:03.593349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:03.597551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:03.597788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:03.602003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:03.602242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.077019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.079624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.081413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.083182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:00:05.241793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.243802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.245652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:00:05.247578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:00:06.305736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.307099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.308176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.309197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.310209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.311289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.312303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.313339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.314369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.315421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.316447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.317464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.318500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.319531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.320564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.321563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.322584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.323629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.324646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.325663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.326743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.327764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.328779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:00:06.329799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:00:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11428)
[2023-10-04 13:01:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11429)
[2023-10-04 13:02:04 +0000] [684] [ERROR] Worker (pid:11429) was sent code 134!
[2023-10-04 13:02:04 +0000] [11532] [INFO] Booting worker with pid: 11532
[2023-10-04 13:02:04 +0000] [684] [ERROR] Worker (pid:11428) was sent code 134!
[2023-10-04 13:02:04 +0000] [11533] [INFO] Booting worker with pid: 11533
2023-10-04 13:02:23.036294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:02:23.036294: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:02:24.946318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:02:24.946316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:02:27.813265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:27.813265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:27.827001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:27.827010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:27.831280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:27.831534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:30.723676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:30.723683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:30.728014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:30.728270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:30.732434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:30.732692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.528049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.530863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.533344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.535726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:02:33.574174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.576175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.577881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:33.579660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:02:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11532)
[2023-10-04 13:02:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11533)
[2023-10-04 13:02:35 +0000] [684] [ERROR] Worker (pid:11532) was sent code 134!
[2023-10-04 13:02:35 +0000] [11611] [INFO] Booting worker with pid: 11611
[2023-10-04 13:02:35 +0000] [684] [ERROR] Worker (pid:11533) was sent code 134!
[2023-10-04 13:02:35 +0000] [11612] [INFO] Booting worker with pid: 11612
2023-10-04 13:02:47.838117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:02:47.838251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:02:49.114931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:02:49.114929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:02:50.461940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:50.461940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:50.472785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:50.472785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:50.477229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:50.477492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:52.700892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:52.700892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:52.705285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:52.705536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:52.709724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:52.709976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.178445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.181206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.183027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.185241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:02:54.304029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.306068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.307897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:02:54.309440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:02:55.365508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.366648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.367648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.368646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.369636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.370776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.371785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.372781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.373812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.374868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.375862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.376854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.377870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.378900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.379904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.380902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.381895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.382948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.383942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.384935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.385944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.386992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.387996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:02:55.388990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:03:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11611)
[2023-10-04 13:04:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11612)
[2023-10-04 13:05:47 +0000] [684] [ERROR] Worker (pid:11611) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:05:47 +0000] [11667] [INFO] Booting worker with pid: 11667
[2023-10-04 13:05:47 +0000] [684] [ERROR] Worker (pid:11612) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:05:47 +0000] [11668] [INFO] Booting worker with pid: 11668
2023-10-04 13:06:07.012307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:06:07.012307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:06:08.801732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:06:08.801732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:06:11.632309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:11.632309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:11.648347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:11.648399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:11.652793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:11.653057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:14.512254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:14.512261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:14.516827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:14.517090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:14.521435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:14.521693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.224415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.226571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.228686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.230479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:06:17.273174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.275113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.276886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:17.278657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:06:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11667)
[2023-10-04 13:06:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11668)
[2023-10-04 13:06:18 +0000] [684] [ERROR] Worker (pid:11668) was sent code 134!
[2023-10-04 13:06:18 +0000] [11797] [INFO] Booting worker with pid: 11797
[2023-10-04 13:06:18 +0000] [684] [ERROR] Worker (pid:11667) was sent code 134!
[2023-10-04 13:06:18 +0000] [11798] [INFO] Booting worker with pid: 11798
2023-10-04 13:06:30.724696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:06:30.724696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:06:31.757239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:06:31.757239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:06:33.260868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:33.260868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:33.270268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:33.270275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:33.274612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:33.274854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:35.070664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:35.071641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:35.074231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:35.076245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:35.078751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:35.080760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.458834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.461482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.463461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.465354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:06:36.623486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.625463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.627359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:06:36.628962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:06:37.707104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.708121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.709107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.710082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.711093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.712081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.713050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.714022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.715275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.716923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.718624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.720289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.721631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.722626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.723601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.724557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.725508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.726472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.727470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.728779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.730403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.732070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.733733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:06:37.735414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:07:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11797)
[2023-10-04 13:08:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11798)
[2023-10-04 13:09:35 +0000] [684] [ERROR] Worker (pid:11797) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:09:35 +0000] [11849] [INFO] Booting worker with pid: 11849
[2023-10-04 13:09:35 +0000] [684] [ERROR] Worker (pid:11798) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:09:35 +0000] [11850] [INFO] Booting worker with pid: 11850
2023-10-04 13:09:54.709040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:09:54.709040: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:09:56.478016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:09:56.478015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:09:59.108076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:09:59.108076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:09:59.123852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:09:59.123880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:09:59.128123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:09:59.128371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:02.327261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:02.327272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:02.331650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:02.331903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:02.336064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:02.336314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.081702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.081710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.086703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.086991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.091861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.092147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:05.096844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:10:05.097110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:10:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11849)
[2023-10-04 13:10:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11850)
[2023-10-04 13:10:06 +0000] [684] [ERROR] Worker (pid:11850) was sent code 134!
[2023-10-04 13:10:06 +0000] [11967] [INFO] Booting worker with pid: 11967
[2023-10-04 13:10:06 +0000] [684] [ERROR] Worker (pid:11849) was sent code 134!
[2023-10-04 13:10:06 +0000] [11968] [INFO] Booting worker with pid: 11968
2023-10-04 13:10:18.318266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:10:18.318266: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:10:19.389931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:10:19.389931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:10:20.456953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:20.456954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:20.465817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:20.465820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:20.470292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:20.470561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:22.444967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:22.445027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:22.449401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:22.449652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:22.453880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:22.454129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:23.983121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:23.985207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:23.987093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:23.988823: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:10:24.222178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:24.224216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:24.226120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:10:24.228014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:10:25.386544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.388547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.390337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.392139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.393754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.395279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.397057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.398884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.400678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.402473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.404282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.405886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.407149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.408351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.409574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.410855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.412104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.413299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.414509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.415739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.416825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.417890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.419007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:10:25.420062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:10:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11967)
[2023-10-04 13:12:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:11968)
[2023-10-04 13:13:52 +0000] [684] [ERROR] Worker (pid:11967) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:13:52 +0000] [12037] [INFO] Booting worker with pid: 12037
[2023-10-04 13:13:52 +0000] [684] [ERROR] Worker (pid:11968) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:13:52 +0000] [12039] [INFO] Booting worker with pid: 12039
2023-10-04 13:14:12.122997: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:14:12.122997: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:14:13.966277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:14:13.966275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:14:17.034124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:17.034124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:17.050849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:17.050892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:17.055143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:17.055398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:19.926931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:19.926964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:19.931376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:19.931619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:19.935822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:19.936070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.780435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.784215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.786063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.788242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:14:22.805022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.807082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.808836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:22.810643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:14:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12037)
[2023-10-04 13:14:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12039)
[2023-10-04 13:14:23 +0000] [684] [ERROR] Worker (pid:12039) was sent code 134!
[2023-10-04 13:14:23 +0000] [12167] [INFO] Booting worker with pid: 12167
[2023-10-04 13:14:23 +0000] [684] [ERROR] Worker (pid:12037) was sent code 134!
[2023-10-04 13:14:23 +0000] [12168] [INFO] Booting worker with pid: 12168
2023-10-04 13:14:35.877840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:14:35.877838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:14:37.010171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:14:37.010171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:14:38.235739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:38.235739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:38.244275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:38.244275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:38.248727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:38.248989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:40.437852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:40.437878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:40.442238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:40.442489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:40.446734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:40.446988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:41.838095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:41.841364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:41.843321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:41.845294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:14:42.036492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:42.038546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:42.040159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:14:42.041697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:14:43.103848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.104933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.105933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.106981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.107983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.108979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.109975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.111040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.112041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.113032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.114035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.115065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.116079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.117072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.118069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.119113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.120110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.121098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.122087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.123138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.124203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.125197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.126196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:14:43.127241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:15:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12167)
[2023-10-04 13:16:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12168)
[2023-10-04 13:16:22 +0000] [684] [ERROR] Worker (pid:12167) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:16:22 +0000] [12221] [INFO] Booting worker with pid: 12221
[2023-10-04 13:16:22 +0000] [684] [ERROR] Worker (pid:12168) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:16:22 +0000] [12222] [INFO] Booting worker with pid: 12222
2023-10-04 13:16:41.130064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:16:41.130064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:16:43.080327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:16:43.080327: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:16:45.716487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:45.716487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:45.730747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:45.730774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:45.735037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:45.735295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:48.562571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:48.562622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:48.566991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:48.567241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:48.571432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:48.571685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.084965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.087273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.089777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.092374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19022 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:16:51.202433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.204309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.205976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:16:51.207630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:16:52.255910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.257041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.258076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.259160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.260188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.261261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.262301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.263384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.264409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.265443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.266491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.267545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.268587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.269614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.270691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.271722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.272745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.273768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.274878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.275916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.276947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.277974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.279050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.280079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:16:52.281112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:16:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12221)
[2023-10-04 13:16:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12222)
[2023-10-04 13:16:53 +0000] [684] [ERROR] Worker (pid:12222) was sent code 134!
[2023-10-04 13:16:53 +0000] [12310] [INFO] Booting worker with pid: 12310
[2023-10-04 13:16:53 +0000] [684] [ERROR] Worker (pid:12221) was sent code 134!
[2023-10-04 13:16:53 +0000] [12311] [INFO] Booting worker with pid: 12311
2023-10-04 13:17:07.130382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:17:07.130382: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:17:08.738756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:17:08.738755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:17:11.086950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:11.086950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:11.096153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:11.096156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:11.100442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:11.100708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:13.652664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:13.652811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:13.657252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:13.657511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:13.661844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:13.662101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.134840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.136890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.138664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.140252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:17:15.243875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.245884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.247631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:17:15.249418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:17:16.322537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.323699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.324787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.325813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.326830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.327834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.328836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.329833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.330876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.331869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.332872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.333869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.334920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.335915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.336915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.337906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.338947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.339940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.340928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.341922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.342960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.343953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.344948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:17:16.345942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:17:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12310)
[2023-10-04 13:18:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12311)
[2023-10-04 13:19:13 +0000] [684] [ERROR] Worker (pid:12311) was sent code 134!
[2023-10-04 13:19:13 +0000] [12418] [INFO] Booting worker with pid: 12418
[2023-10-04 13:19:13 +0000] [684] [ERROR] Worker (pid:12310) was sent code 134!
[2023-10-04 13:19:13 +0000] [12419] [INFO] Booting worker with pid: 12419
2023-10-04 13:19:31.531936: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:19:31.531936: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:19:33.322773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:19:33.322773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:19:36.041981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:36.041981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:36.054301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:36.054344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:36.058612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:36.058842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:38.946876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:38.946995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:38.951277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:38.951526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:38.955708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:38.955957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.500557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.503357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.505908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.507541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.509273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:19:41.510677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.513145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:19:41.515387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:19:42.627944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.629660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.631368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.633050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.634739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.636399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.638063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.639760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.641425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.643121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.644736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.646417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.648142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.649796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.651487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.653156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.654839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.656492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.658156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.659314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.660431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.662030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.663727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:19:42.665391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:19:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12418)
[2023-10-04 13:19:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12419)
[2023-10-04 13:19:44 +0000] [684] [ERROR] Worker (pid:12419) was sent code 134!
[2023-10-04 13:19:44 +0000] [12487] [INFO] Booting worker with pid: 12487
[2023-10-04 13:19:44 +0000] [684] [ERROR] Worker (pid:12418) was sent code 134!
[2023-10-04 13:19:44 +0000] [12488] [INFO] Booting worker with pid: 12488
2023-10-04 13:20:00.218660: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:20:00.218660: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:20:01.959971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:20:01.959971: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:20:04.637559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:04.637559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:04.653520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:04.653564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:04.657819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:04.658072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:07.452263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:07.452296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:07.456668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:07.456909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:07.461221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:07.461479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.056231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.059636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.061516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.063617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:20:09.065954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.068015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.069799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:20:09.071500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:20:10.168851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.169946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.170966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.171935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.172902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.173870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.174873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.175842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.176865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.177871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.178924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.179931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.181011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.182011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.183060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.184064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.185059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.186056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.187140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.188138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.189134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.190160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.191191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:20:10.192207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:20:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12488)
[2023-10-04 13:21:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12487)
[2023-10-04 13:21:04 +0000] [684] [ERROR] Worker (pid:12488) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:21:04 +0000] [12561] [INFO] Booting worker with pid: 12561
[2023-10-04 13:21:05 +0000] [684] [ERROR] Worker (pid:12487) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:21:05 +0000] [12562] [INFO] Booting worker with pid: 12562
2023-10-04 13:21:24.097352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:21:24.097352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:21:25.849894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:21:25.849894: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:21:28.617703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:28.617708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:28.630232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:28.630242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:28.634517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:28.634769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:31.520071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:31.520101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:31.524517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:31.524767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:31.529006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:31.529257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.394746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.398087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.400655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.403170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:21:34.420853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.423607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.425898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:34.427810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:21:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12561)
[2023-10-04 13:21:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12562)
[2023-10-04 13:21:36 +0000] [684] [ERROR] Worker (pid:12561) was sent code 134!
[2023-10-04 13:21:36 +0000] [12668] [INFO] Booting worker with pid: 12668
[2023-10-04 13:21:36 +0000] [684] [ERROR] Worker (pid:12562) was sent code 134!
[2023-10-04 13:21:36 +0000] [12669] [INFO] Booting worker with pid: 12669
2023-10-04 13:21:48.887868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:21:48.936930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:21:50.016407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:21:50.016407: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:21:51.252679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:51.252679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:51.261872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:51.261875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:51.266184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:51.266433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:53.195384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:53.195869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:53.199617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:53.200306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:53.204080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:53.204776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.762356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.764926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.766780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.768589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:21:54.845175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.847238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.849096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:21:54.850880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:21:55.942453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.944136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.945784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.947388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.948398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.949401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.950416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.951447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.952448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.953447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.954513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.955840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.957416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.959136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.960822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.962467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.963511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.964507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.965504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.966504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.967523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.968519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.969508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:21:55.971078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:22:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12668)
[2023-10-04 13:23:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12669)
[2023-10-04 13:24:42 +0000] [684] [ERROR] Worker (pid:12668) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:24:42 +0000] [12733] [INFO] Booting worker with pid: 12733
[2023-10-04 13:24:42 +0000] [684] [ERROR] Worker (pid:12669) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:24:42 +0000] [12734] [INFO] Booting worker with pid: 12734
2023-10-04 13:25:02.483306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:25:02.483306: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:25:04.365958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:25:04.365958: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:25:07.299919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:07.299919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:07.314668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:07.314682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:07.319014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:07.319265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:10.296323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:10.296323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:10.300740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:10.300993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:10.305193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:10.305433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 13:25:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12733)
[2023-10-04 13:25:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12734)
[2023-10-04 13:25:12 +0000] [684] [ERROR] Worker (pid:12734) was sent code 134!
[2023-10-04 13:25:12 +0000] [12841] [INFO] Booting worker with pid: 12841
[2023-10-04 13:25:12 +0000] [684] [ERROR] Worker (pid:12733) was sent code 134!
[2023-10-04 13:25:12 +0000] [12842] [INFO] Booting worker with pid: 12842
2023-10-04 13:25:20.142399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:25:20.160243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:25:21.381772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:25:21.381772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:25:22.671452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:22.671452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:22.677831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:22.677838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:22.682111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:22.682367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:24.639707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:24.640005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:24.644216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:24.644451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:24.648739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:24.648994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.157486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.160465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.163295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.165967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19032 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:25:26.339017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.341864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.344512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:25:26.347064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:25:27.498873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.499982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.501005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.502029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.503088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.504606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.505954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.507480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.509134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.510720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.512362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.514009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.515726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.517217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.518874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.520522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.522164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.523851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.525489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.527146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.528632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.530201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.531870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.533482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:25:27.535128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:26:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12842)
[2023-10-04 13:28:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12841)
[2023-10-04 13:28:53 +0000] [684] [ERROR] Worker (pid:12842) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:28:53 +0000] [12907] [INFO] Booting worker with pid: 12907
[2023-10-04 13:28:53 +0000] [684] [ERROR] Worker (pid:12841) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:28:53 +0000] [12930] [INFO] Booting worker with pid: 12930
2023-10-04 13:29:12.430307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:29:12.430307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:29:14.113416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:29:14.113416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:29:16.856391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:16.856391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:16.869546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:16.869637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:16.874038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:16.874305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:19.792341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:19.792394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:19.796764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:19.797015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:19.801216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:19.801461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.381860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.385877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.388043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.390504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18992 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:29:22.467433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.469328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.471238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:22.472869: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:29:23.546002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.547804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.549393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.550923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.552444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.554141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.555880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.557168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.558611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.559859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.561150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.562282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.563936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.565124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.566368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.568100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.569844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.571555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.573078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.574697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.576119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.577257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.578488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:23.580141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:29:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12907)
[2023-10-04 13:29:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:12930)
[2023-10-04 13:29:24 +0000] [684] [ERROR] Worker (pid:12930) was sent code 134!
[2023-10-04 13:29:24 +0000] [13037] [INFO] Booting worker with pid: 13037
[2023-10-04 13:29:24 +0000] [684] [ERROR] Worker (pid:12907) was sent code 134!
[2023-10-04 13:29:24 +0000] [13038] [INFO] Booting worker with pid: 13038
2023-10-04 13:29:40.564571: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:29:40.564571: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:29:42.336462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:29:42.336466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:29:44.874772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:44.874771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:44.884703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:44.884711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:44.889111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:44.889367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:47.570326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:47.570326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:47.574766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:47.575023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:47.579218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:47.579478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:48.989347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:48.992362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:48.994315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:48.995990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:29:49.097865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:49.100455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:49.102904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:29:49.105131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:29:50.183048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.184693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.186220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.187684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.189234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.190853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.192442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.194065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.195693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.197307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.199060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.200646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.202275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.203885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.205489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.207119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.208709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.209844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.211439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.213004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.214626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.216213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.217804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:29:50.219432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:30:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13038)
[2023-10-04 13:31:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13037)
[2023-10-04 13:32:58 +0000] [684] [ERROR] Worker (pid:13038) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:32:58 +0000] [13094] [INFO] Booting worker with pid: 13094
[2023-10-04 13:32:59 +0000] [684] [ERROR] Worker (pid:13037) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:32:59 +0000] [13097] [INFO] Booting worker with pid: 13097
2023-10-04 13:33:17.995967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:33:17.995967: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:33:19.860000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:33:19.860004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:33:22.553445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:22.553445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:22.566826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:22.566826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:22.571120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:22.571373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:25.379558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:25.379558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:25.384074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:25.384335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:25.388643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:25.388903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.912236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.914411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.916619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.918730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:33:27.925064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.927043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.928769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:27.930521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:33:29.009376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.010698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.011683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.012675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.013666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.015063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.016609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.018028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.019540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.020877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.022506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.024142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.025473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.027111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.028732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.030354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.031984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.033332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.034930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.036533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.038130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.039769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.041387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:29.043018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:33:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13094)
[2023-10-04 13:33:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13097)
[2023-10-04 13:33:30 +0000] [684] [ERROR] Worker (pid:13097) was sent code 134!
[2023-10-04 13:33:30 +0000] [13224] [INFO] Booting worker with pid: 13224
[2023-10-04 13:33:30 +0000] [684] [ERROR] Worker (pid:13094) was sent code 134!
[2023-10-04 13:33:30 +0000] [13225] [INFO] Booting worker with pid: 13225
2023-10-04 13:33:46.117053: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:33:46.117053: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:33:47.826171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:33:47.826173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:33:50.468475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:50.468475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:50.482079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:50.482104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:50.486333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:50.486585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:53.332043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:53.332113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:53.336432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:53.337154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:53.340813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:53.341742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.853267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.853267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.858743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.858992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.863461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.863732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:33:54.868195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:33:54.868444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:33:55.999479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.000547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.001543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.002534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.003568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.004569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.005575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.006606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.007614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.008611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.009604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.010648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.011658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.012662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.013667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.014704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.015699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.016693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.017689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.018792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.019857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.020857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.021851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:33:56.022893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:34:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13225)
[2023-10-04 13:35:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13224)
[2023-10-04 13:36:11 +0000] [684] [ERROR] Worker (pid:13225) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:36:11 +0000] [13277] [INFO] Booting worker with pid: 13277
[2023-10-04 13:36:12 +0000] [684] [ERROR] Worker (pid:13224) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:36:12 +0000] [13280] [INFO] Booting worker with pid: 13280
2023-10-04 13:36:30.962749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:36:30.962749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:36:32.649731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:36:32.649731: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:36:35.357308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:35.357307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:35.371925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:35.371976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:35.376228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:35.376479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:38.246445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:38.246838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:38.250664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:38.251352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:38.255147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:38.255831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.003971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.006281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.008980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.011530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19034 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:36:41.143834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.145780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.147640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:36:41.149277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:36:42.266955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.268400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.270174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.271468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.273032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.274108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.275653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.277077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.278788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.280388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.281793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.282966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.284286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.285646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.286955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.288013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.289892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.291381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.292832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.294686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.296508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.297962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.299077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.300169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:36:42.301191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:36:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13277)
[2023-10-04 13:36:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13280)
[2023-10-04 13:36:43 +0000] [684] [ERROR] Worker (pid:13280) was sent code 134!
[2023-10-04 13:36:43 +0000] [13377] [INFO] Booting worker with pid: 13377
[2023-10-04 13:36:43 +0000] [684] [ERROR] Worker (pid:13277) was sent code 134!
[2023-10-04 13:36:43 +0000] [13378] [INFO] Booting worker with pid: 13378
2023-10-04 13:36:58.033898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:36:58.033898: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:36:59.628618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:36:59.628617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:37:01.871881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:01.871881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:01.881827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:01.881827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:01.886286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:01.886551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:04.553756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:04.554163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:04.558091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:04.558796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:04.562697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:04.563409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.010790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.010793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.016378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.016633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.021131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.021402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:37:06.025896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:37:06.026148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:37:07.137523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.139086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.140568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.142043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.143560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.145033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.146516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.148012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.149532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.151055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.152549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.154030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.155553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.157010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.158519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.159999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.161457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.162942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.164410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.165878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.167396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.168864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.170347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:37:07.171854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:37:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13377)
[2023-10-04 13:38:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13378)
[2023-10-04 13:39:55 +0000] [684] [ERROR] Worker (pid:13377) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:39:55 +0000] [13432] [INFO] Booting worker with pid: 13432
[2023-10-04 13:39:55 +0000] [684] [ERROR] Worker (pid:13378) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:39:55 +0000] [13433] [INFO] Booting worker with pid: 13433
2023-10-04 13:40:15.415513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:40:15.415513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:40:17.063283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:40:17.063284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:40:19.690761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:19.690761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:19.706111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:19.706139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:19.710416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:19.710679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:22.492683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:22.492747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:22.497264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:22.497523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:22.501856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:22.502120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.191908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.191906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.196978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.197262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.202048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.202333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:25.207097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:40:25.207377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:40:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13432)
[2023-10-04 13:40:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13433)
[2023-10-04 13:40:26 +0000] [684] [ERROR] Worker (pid:13432) was sent code 134!
[2023-10-04 13:40:26 +0000] [13560] [INFO] Booting worker with pid: 13560
[2023-10-04 13:40:26 +0000] [684] [ERROR] Worker (pid:13433) was sent code 134!
[2023-10-04 13:40:26 +0000] [13561] [INFO] Booting worker with pid: 13561
2023-10-04 13:40:39.081748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:40:39.101033: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:40:40.261925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:40:40.261925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:40:41.388995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:41.388995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:41.397595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:41.397601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:41.401881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:41.402150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:43.199707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:43.200723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:43.203238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:43.205209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:43.207688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:43.209658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.612411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.615282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.617801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.620090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:40:44.754429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.756472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.758282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:40:44.760240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:40:45.842641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.843760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.844758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.845789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.846863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.847898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.848929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.849951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.851036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.852050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.853063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.854082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.855179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.856196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.857219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.858234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.859263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.860272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.861291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.862295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.863315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.864308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.865305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:40:45.866305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:41:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13560)
[2023-10-04 13:42:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13561)
[2023-10-04 13:43:47 +0000] [684] [ERROR] Worker (pid:13560) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:43:47 +0000] [13616] [INFO] Booting worker with pid: 13616
[2023-10-04 13:43:47 +0000] [684] [ERROR] Worker (pid:13561) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:43:47 +0000] [13617] [INFO] Booting worker with pid: 13617
2023-10-04 13:44:06.328737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:44:06.328737: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:44:08.166125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:44:08.166125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:44:10.875630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:10.875630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:10.890958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:10.890971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:10.895450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:10.895714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:13.759347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:13.759425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:13.763804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:13.764054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:13.768285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:13.768540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.444354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.448226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.450211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.452419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:44:16.551806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.554553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.557142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:16.559624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:44:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13616)
[2023-10-04 13:44:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13617)
[2023-10-04 13:44:18 +0000] [684] [ERROR] Worker (pid:13617) was sent code 134!
[2023-10-04 13:44:18 +0000] [13744] [INFO] Booting worker with pid: 13744
[2023-10-04 13:44:18 +0000] [684] [ERROR] Worker (pid:13616) was sent code 134!
[2023-10-04 13:44:18 +0000] [13745] [INFO] Booting worker with pid: 13745
2023-10-04 13:44:31.621985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:44:31.622102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:44:33.283087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:44:33.283091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:44:35.717355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:35.717355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:35.727085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:35.727088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:35.731355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:35.731622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:38.353672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:38.353672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:38.358050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:38.358304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:38.362447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:38.362693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.836897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.839755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.841512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.843366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:44:39.904067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.905920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.907779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:44:39.909466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:44:40.974148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.975471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.976459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.977442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.978440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.979490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.980498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.981489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.982509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.983544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.984557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.985602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.986657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.987659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.988659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.989664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.990707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.991727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.992719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.993716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.994796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.995810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.996815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:44:40.997809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:45:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13745)
[2023-10-04 13:46:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13744)
[2023-10-04 13:47:14 +0000] [684] [ERROR] Worker (pid:13745) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:47:14 +0000] [13796] [INFO] Booting worker with pid: 13796
[2023-10-04 13:47:15 +0000] [684] [ERROR] Worker (pid:13744) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:47:15 +0000] [13799] [INFO] Booting worker with pid: 13799
2023-10-04 13:47:34.123552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:47:34.123552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:47:35.897513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:47:35.897513: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:47:38.469093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:38.469093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:38.484770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:38.485113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:38.490415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:38.491074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:41.324533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:41.324536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:41.328931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:41.329188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:41.333360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:41.333617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:43.907224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:43.909730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:43.911852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:43.913772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:47:44.015274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:44.017226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:44.019092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:47:44.020836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:47:45.127624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.128848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.129880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.130947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.131961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.132984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.133996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.135048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.136066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.137074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.138429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.139791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.140824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.141834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.142902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.144203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.145520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.147145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.148172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.149485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.150833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.151842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.152858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.153870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:47:45.154926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:47:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13796)
[2023-10-04 13:47:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13799)
[2023-10-04 13:47:46 +0000] [684] [ERROR] Worker (pid:13799) was sent code 134!
[2023-10-04 13:47:46 +0000] [13896] [INFO] Booting worker with pid: 13896
[2023-10-04 13:47:46 +0000] [684] [ERROR] Worker (pid:13796) was sent code 134!
[2023-10-04 13:47:46 +0000] [13897] [INFO] Booting worker with pid: 13897
2023-10-04 13:48:00.451199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:48:00.451199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:48:02.055566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:48:02.055565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:48:04.356179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:04.356179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:04.365973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:04.365973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:04.370266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:04.370523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:07.019237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:07.019330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:07.023703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:07.023952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:07.028122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:07.028372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.494102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.496679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.498834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.501325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:48:08.758439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.760445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.762358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:48:08.764092: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:48:09.904140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.905266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.906751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.908178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.909530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.911878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.913257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.914769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.916144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.917855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.919048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.920334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.921738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.923326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.924900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.926356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.927567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.929025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.930695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.931756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.933031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.934225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.935747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:48:09.936781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:48:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13897)
[2023-10-04 13:49:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13896)
[2023-10-04 13:50:51 +0000] [684] [ERROR] Worker (pid:13897) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:50:51 +0000] [13954] [INFO] Booting worker with pid: 13954
[2023-10-04 13:50:51 +0000] [684] [ERROR] Worker (pid:13896) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:50:51 +0000] [13960] [INFO] Booting worker with pid: 13960
2023-10-04 13:51:10.399072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:51:10.399072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:51:12.142751: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:51:12.142757: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:51:14.849244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:14.849244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:14.863479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:14.863528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:14.867779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:14.868031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:17.693158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:17.693190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:17.697704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:17.697966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:17.702312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:17.702574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.363324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.365615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.367677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.370058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:51:20.379884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.381963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.383755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:20.385585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:51:21.463603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.465154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.466659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.468140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.469619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.471133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.472606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.474105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.475613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.477109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.478627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.480116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.481612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.483115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.484594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.486062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.487556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.489042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.490537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.492041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.493522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.495035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.496522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:21.498005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:51:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13954)
[2023-10-04 13:51:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:13960)
[2023-10-04 13:51:22 +0000] [684] [ERROR] Worker (pid:13960) was sent code 134!
[2023-10-04 13:51:22 +0000] [14091] [INFO] Booting worker with pid: 14091
[2023-10-04 13:51:22 +0000] [684] [ERROR] Worker (pid:13954) was sent code 134!
[2023-10-04 13:51:22 +0000] [14092] [INFO] Booting worker with pid: 14092
2023-10-04 13:51:38.974796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:51:38.974796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:51:40.819461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:51:40.819468: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:51:43.543680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:43.543680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:43.554544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:43.554546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:43.558841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:43.559103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:46.578082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:46.578933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:46.581858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:46.583414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:46.586329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:46.587890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.042456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.045251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.047040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.048669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:51:48.297620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.300232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.302572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:51:48.304554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:51:49.383654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.384826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.385851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.386930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.387963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.388983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.389996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.391065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.392081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.393089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.394100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.395186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.396227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.397239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.398276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.399335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.400346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.401719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.402794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.403810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.405095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.406110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.407218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:51:49.408233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:52:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14091)
[2023-10-04 13:53:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14092)
[2023-10-04 13:54:42 +0000] [684] [ERROR] Worker (pid:14091) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:54:42 +0000] [14150] [INFO] Booting worker with pid: 14150
[2023-10-04 13:54:42 +0000] [684] [ERROR] Worker (pid:14092) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:54:42 +0000] [14151] [INFO] Booting worker with pid: 14151
2023-10-04 13:55:02.031628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:55:02.031628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:55:03.865036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:55:03.865036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:55:06.810838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:06.810838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:06.827893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:06.827913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:06.832312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:06.832577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:09.812207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:09.812208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:09.816733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:09.816986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:09.821281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:09.821540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 13:55:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14150)
[2023-10-04 13:55:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14151)
[2023-10-04 13:55:13 +0000] [684] [ERROR] Worker (pid:14150) was sent code 134!
[2023-10-04 13:55:13 +0000] [14254] [INFO] Booting worker with pid: 14254
[2023-10-04 13:55:13 +0000] [684] [ERROR] Worker (pid:14151) was sent code 134!
[2023-10-04 13:55:13 +0000] [14255] [INFO] Booting worker with pid: 14255
2023-10-04 13:55:24.144510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:55:24.159965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:55:25.683812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:55:25.683812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:55:26.926622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:26.926628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:26.936086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:26.936090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:26.940406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:26.940659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:29.010940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:29.011077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:29.015472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:29.015710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:29.019933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:29.020179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.490092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.492337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.494229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.496036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:55:30.542645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.544659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.546703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:55:30.548614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:55:31.661697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.662846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.663875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.665020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.666290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.667949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.669420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.671070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.672693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.674340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.675817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.676981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.678681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.680334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.681998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.683699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.685329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.686535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.687747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.689110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.690787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.692441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.694090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:55:31.695783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:56:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14254)
[2023-10-04 13:57:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14255)
[2023-10-04 13:58:35 +0000] [684] [ERROR] Worker (pid:14254) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:58:35 +0000] [14316] [INFO] Booting worker with pid: 14316
[2023-10-04 13:58:35 +0000] [684] [ERROR] Worker (pid:14255) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 13:58:35 +0000] [14317] [INFO] Booting worker with pid: 14317
2023-10-04 13:58:55.204000: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:58:55.204000: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:58:56.951671: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:58:56.951672: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:58:59.647586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:58:59.647586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:58:59.661867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:58:59.661907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:58:59.666190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:58:59.666442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:02.658107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:02.658195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:02.662605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:02.662845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:02.667046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:02.667302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.264889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.267108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.268958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.271208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:59:05.309900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.311761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.313563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:05.315270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 13:59:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14316)
[2023-10-04 13:59:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14317)
[2023-10-04 13:59:06 +0000] [684] [ERROR] Worker (pid:14317) was sent code 134!
[2023-10-04 13:59:06 +0000] [14435] [INFO] Booting worker with pid: 14435
[2023-10-04 13:59:06 +0000] [684] [ERROR] Worker (pid:14316) was sent code 134!
[2023-10-04 13:59:06 +0000] [14436] [INFO] Booting worker with pid: 14436
2023-10-04 13:59:18.926653: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:59:18.931801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 13:59:20.265936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:59:20.265936: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 13:59:21.555383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:21.555383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:21.564308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:21.564314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:21.568644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:21.568893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:23.583472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:23.583995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:23.587698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:23.588389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:23.592166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:23.592859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:24.989719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:24.992549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:24.995038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:24.996997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:59:25.113484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:25.115693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:25.117431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 13:59:25.119110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 13:59:26.177271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.178393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.179452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.180463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.181489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.182522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.183583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.184580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.185594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.186631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.187653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.188675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.189707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.190769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.191800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.192820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.193839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.194902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.195934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.196946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.197970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.199031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.200047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 13:59:26.201062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 13:59:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14435)
[2023-10-04 14:00:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14436)
[2023-10-04 14:02:19 +0000] [684] [ERROR] Worker (pid:14435) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:02:19 +0000] [14503] [INFO] Booting worker with pid: 14503
[2023-10-04 14:02:19 +0000] [684] [ERROR] Worker (pid:14436) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:02:19 +0000] [14504] [INFO] Booting worker with pid: 14504
2023-10-04 14:02:38.647154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:02:38.647154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:02:40.407337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:02:40.407337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:02:43.209748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:43.209748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:43.228844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:43.228889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:43.233294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:43.233557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:46.258558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:46.258712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:46.263026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:46.263273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:46.267441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:46.267689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.008537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.011424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.013353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.015521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:02:49.066918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.069640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.072208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:02:49.074099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:02:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14503)
[2023-10-04 14:02:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14504)
[2023-10-04 14:02:50 +0000] [684] [ERROR] Worker (pid:14503) was sent code 134!
[2023-10-04 14:02:50 +0000] [14601] [INFO] Booting worker with pid: 14601
[2023-10-04 14:02:50 +0000] [684] [ERROR] Worker (pid:14504) was sent code 134!
[2023-10-04 14:02:50 +0000] [14602] [INFO] Booting worker with pid: 14602
2023-10-04 14:03:02.706400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:03:02.706400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:03:03.771702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:03:03.771702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:03:05.041654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:05.041654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:05.050346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:05.050356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:05.054674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:05.054933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:06.857274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:06.858480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:06.860863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:06.862906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:06.864496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:06.868369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.250585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.253169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.255130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.256977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:03:08.466315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.468201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.469801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:03:08.471529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:03:09.576122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.577172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.578147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.579161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.580135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.581103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.582067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.583080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.584045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.585006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.585966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.586999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.587983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.588974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.589943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.590983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.591951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.592939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.594180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.595667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.597161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.598783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.600124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:03:09.601715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:03:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14601)
[2023-10-04 14:04:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14602)
[2023-10-04 14:05:55 +0000] [684] [ERROR] Worker (pid:14601) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:05:55 +0000] [14685] [INFO] Booting worker with pid: 14685
[2023-10-04 14:05:56 +0000] [684] [ERROR] Worker (pid:14602) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:05:56 +0000] [14686] [INFO] Booting worker with pid: 14686
2023-10-04 14:06:15.834764: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:06:15.834764: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:06:17.773183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:06:17.773183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:06:20.590842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:20.590842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:20.608667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:20.608669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:20.613038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:20.613294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:23.665812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:23.665812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:23.670279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:23.670522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:23.674697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:23.674952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 14:06:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14685)
[2023-10-04 14:06:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14686)
[2023-10-04 14:06:26 +0000] [684] [ERROR] Worker (pid:14685) was sent code 134!
[2023-10-04 14:06:26 +0000] [14798] [INFO] Booting worker with pid: 14798
[2023-10-04 14:06:26 +0000] [684] [ERROR] Worker (pid:14686) was sent code 134!
[2023-10-04 14:06:26 +0000] [14799] [INFO] Booting worker with pid: 14799
2023-10-04 14:06:37.412693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:06:37.412689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:06:38.630010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:06:38.630010: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:06:39.788635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:39.788635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:39.799430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:39.799430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:39.803900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:39.804117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:41.757225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:41.758840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:41.760459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:41.763489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:41.765052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:41.768055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.362915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.365420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.367400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.369293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:06:43.531704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.533775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.535762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:06:43.537482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:06:44.706464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.707543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.708525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.709495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.710483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.711567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.712545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.713516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.714493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.715487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.716454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.717420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.718414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.719415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.720826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.722459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.724117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.725748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.727405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.729047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.730745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.732495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.734198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:06:44.735558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:07:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14798)
[2023-10-04 14:08:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14799)
[2023-10-04 14:09:34 +0000] [684] [ERROR] Worker (pid:14798) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:09:34 +0000] [14852] [INFO] Booting worker with pid: 14852
[2023-10-04 14:09:34 +0000] [684] [ERROR] Worker (pid:14799) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:09:34 +0000] [14854] [INFO] Booting worker with pid: 14854
2023-10-04 14:09:54.312702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:09:54.312702: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:09:56.252948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:09:56.252948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:09:59.155540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:09:59.155540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:09:59.172424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:09:59.172429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:09:59.176787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:09:59.177025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:02.220117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:02.220158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:02.224831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:02.225048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:02.229469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:02.229674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 14:10:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14852)
[2023-10-04 14:10:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14854)
[2023-10-04 14:10:05 +0000] [684] [ERROR] Worker (pid:14852) was sent code 134!
[2023-10-04 14:10:05 +0000] [14955] [INFO] Booting worker with pid: 14955
[2023-10-04 14:10:05 +0000] [684] [ERROR] Worker (pid:14854) was sent code 134!
[2023-10-04 14:10:05 +0000] [14956] [INFO] Booting worker with pid: 14956
2023-10-04 14:10:16.378473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:10:16.378473: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:10:17.658007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:10:17.658009: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:10:18.965645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:18.965645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:18.975377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:18.975377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:18.979917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:18.980131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:21.108887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:21.108986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:21.113535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:21.113778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:21.118210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:21.118449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.707884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.710633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.712694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.715316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:10:22.974801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.976805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.978714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:10:22.980594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:10:24.125584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.126690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.127748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.128680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.129600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.130541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.131509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.132450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.133378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.134308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.135283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.136211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.137155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.138072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.139048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.139976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.140905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.141927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.143558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.145044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.146686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.148068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.149668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:10:24.151320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:11:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14955)
[2023-10-04 14:12:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:14956)
[2023-10-04 14:14:17 +0000] [684] [ERROR] Worker (pid:14955) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:14:17 +0000] [15025] [INFO] Booting worker with pid: 15025
[2023-10-04 14:14:17 +0000] [684] [ERROR] Worker (pid:14956) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:14:17 +0000] [15027] [INFO] Booting worker with pid: 15027
2023-10-04 14:14:36.574828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:14:36.574826: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:14:38.400270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:14:38.400270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:14:41.136826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:41.136826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:41.151763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:41.151763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:41.156107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:41.156352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:44.028879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:44.028965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:44.033400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:44.033635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:44.037847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:44.038086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.715995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.716001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.721196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.721460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.726104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.726386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:14:46.730987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:14:46.731270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:14:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15025)
[2023-10-04 14:14:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15027)
[2023-10-04 14:14:48 +0000] [684] [ERROR] Worker (pid:15027) was sent code 134!
[2023-10-04 14:14:48 +0000] [15126] [INFO] Booting worker with pid: 15126
[2023-10-04 14:14:48 +0000] [684] [ERROR] Worker (pid:15025) was sent code 134!
[2023-10-04 14:14:48 +0000] [15127] [INFO] Booting worker with pid: 15127
2023-10-04 14:15:00.815091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:15:00.815215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:15:02.121536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:15:02.121536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:15:03.444219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:03.444219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:03.453963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:03.453963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:03.458456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:03.458714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:05.494242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:05.494716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:05.498654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:05.499334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:05.503148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:05.503850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.098670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.101981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.104225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.106528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:15:07.141411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.143451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.145609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:15:07.147608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:15:08.285901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.287258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.288229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.289193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.290163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.291263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.292277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.293280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.294288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.295339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.296334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.297341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.298363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.299406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.300428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.301431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.302499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.303632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.304711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.305722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.306797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.307838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.308880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:15:08.309888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:15:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15126)
[2023-10-04 14:16:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15127)
[2023-10-04 14:17:38 +0000] [684] [ERROR] Worker (pid:15127) was sent code 134!
[2023-10-04 14:17:38 +0000] [15233] [INFO] Booting worker with pid: 15233
[2023-10-04 14:17:38 +0000] [684] [ERROR] Worker (pid:15126) was sent code 134!
[2023-10-04 14:17:38 +0000] [15234] [INFO] Booting worker with pid: 15234
2023-10-04 14:17:57.375685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:17:57.375685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:17:59.212600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:17:59.212600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:18:02.285410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:02.285410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:02.300343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:02.300343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:02.304668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:02.304921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:05.309240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:05.309240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:05.313676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:05.313917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:05.318102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:05.318419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.108728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.111540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.113698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.115596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:18:08.186385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.188759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.190642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:08.192776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:18:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15233)
[2023-10-04 14:18:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15234)
[2023-10-04 14:18:09 +0000] [684] [ERROR] Worker (pid:15233) was sent code 134!
[2023-10-04 14:18:09 +0000] [15339] [INFO] Booting worker with pid: 15339
[2023-10-04 14:18:09 +0000] [684] [ERROR] Worker (pid:15234) was sent code 134!
[2023-10-04 14:18:09 +0000] [15340] [INFO] Booting worker with pid: 15340
2023-10-04 14:18:21.776323: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:18:21.794216: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:18:23.131790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:18:23.131790: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:18:24.494741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:24.494741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:24.504886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:24.504886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:24.509307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:24.509539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:26.775964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:26.775964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:26.780383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:26.780633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:26.784750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:26.784962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.330402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.333354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.335344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.337099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:18:28.425323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.427482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.429228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:18:28.431295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:18:29.576156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.577248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.578271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.579346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.580523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.582455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.584343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.586185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.588043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.589843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.591477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.593290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.595108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.596932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.597998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.599046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.600026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.601014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.602013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.603052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.604026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.605005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.606011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:18:29.607044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:19:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15339)
[2023-10-04 14:20:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15340)
[2023-10-04 14:21:36 +0000] [684] [ERROR] Worker (pid:15339) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:21:36 +0000] [15394] [INFO] Booting worker with pid: 15394
[2023-10-04 14:21:36 +0000] [684] [ERROR] Worker (pid:15340) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:21:36 +0000] [15395] [INFO] Booting worker with pid: 15395
2023-10-04 14:21:55.513214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:21:55.513214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:21:57.381611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:21:57.381611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:22:00.293543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:00.293543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:00.309230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:00.309230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:00.313815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:00.314033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:03.505978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:03.506081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:03.510376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:03.510651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:03.514775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:03.515024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 14:22:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15394)
[2023-10-04 14:22:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15395)
[2023-10-04 14:22:07 +0000] [684] [ERROR] Worker (pid:15394) was sent code 134!
[2023-10-04 14:22:07 +0000] [15496] [INFO] Booting worker with pid: 15496
[2023-10-04 14:22:07 +0000] [684] [ERROR] Worker (pid:15395) was sent code 134!
[2023-10-04 14:22:07 +0000] [15497] [INFO] Booting worker with pid: 15497
2023-10-04 14:22:18.808030: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:22:18.824961: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:22:20.174472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:22:20.174472: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:22:21.596972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:21.596972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:21.606427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:21.606427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:21.610740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:21.611001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:23.601659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:23.601659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:23.606140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:23.606367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:23.610550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:23.610833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.105285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.107724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.109791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.111710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:22:25.322582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.324675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.326510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:22:25.328290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:22:26.443756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.444905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.445941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.447036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.448061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.449085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.450106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.451190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.452221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.453231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.454271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.455367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.456451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.457456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.458494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.459548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.460568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.461605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.462692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.463709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.464722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.465742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.466814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:22:26.467860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:22:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15496)
[2023-10-04 14:24:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15497)
[2023-10-04 14:25:20 +0000] [684] [ERROR] Worker (pid:15496) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:25:20 +0000] [15565] [INFO] Booting worker with pid: 15565
[2023-10-04 14:25:21 +0000] [684] [ERROR] Worker (pid:15497) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:25:21 +0000] [15566] [INFO] Booting worker with pid: 15566
2023-10-04 14:25:40.668974: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:25:40.668974: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:25:42.486258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:25:42.486258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:25:45.342884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:45.342884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:45.359992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:45.359992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:45.364363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:45.364617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:48.433821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:48.433888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:48.438360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:48.438558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:48.442845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:48.443048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.181349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.185118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.187909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.190366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18988 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:25:51.288942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.290973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.292606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:25:51.294026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:25:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15565)
[2023-10-04 14:25:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15566)
[2023-10-04 14:25:52 +0000] [684] [ERROR] Worker (pid:15566) was sent code 134!
[2023-10-04 14:25:52 +0000] [15661] [INFO] Booting worker with pid: 15661
[2023-10-04 14:25:52 +0000] [684] [ERROR] Worker (pid:15565) was sent code 134!
[2023-10-04 14:25:52 +0000] [15662] [INFO] Booting worker with pid: 15662
2023-10-04 14:26:04.640375: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:26:04.657678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:26:05.809027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:26:05.809027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:26:06.992432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:06.992432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:07.003384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:07.003384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:07.007992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:07.008214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:09.184135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:09.184888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:09.187927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:09.189490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:09.192424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:09.193976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.615508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.617802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.620097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.622705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:26:10.789885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.791884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.793658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:26:10.795497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:26:11.950390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.951986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.953607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.955257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.956846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.958449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.960072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.961655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.963253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.964455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.966067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.967729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.969389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.971053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.972679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.974322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.975513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.976445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.977374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.978315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.979292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.980214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.981139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:26:11.982080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:26:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15661)
[2023-10-04 14:27:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15662)
[2023-10-04 14:28:17 +0000] [684] [ERROR] Worker (pid:15662) was sent code 134!
[2023-10-04 14:28:17 +0000] [15758] [INFO] Booting worker with pid: 15758
[2023-10-04 14:28:17 +0000] [684] [ERROR] Worker (pid:15661) was sent code 134!
[2023-10-04 14:28:17 +0000] [15767] [INFO] Booting worker with pid: 15767
2023-10-04 14:28:35.904459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:28:35.904459: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:28:37.694389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:28:37.694388: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:28:40.479457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:40.479457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:40.492269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:40.492270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:40.496742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:40.496964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:43.481839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:43.481839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:43.486675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:43.486850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:43.491249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:43.491472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.252219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.255203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.257461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.259609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:28:46.316975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.318951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.320865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:28:46.322704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:28:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15758)
[2023-10-04 14:28:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15767)
[2023-10-04 14:28:47 +0000] [684] [ERROR] Worker (pid:15767) was sent code 134!
[2023-10-04 14:28:47 +0000] [15842] [INFO] Booting worker with pid: 15842
[2023-10-04 14:28:47 +0000] [684] [ERROR] Worker (pid:15758) was sent code 134!
[2023-10-04 14:28:47 +0000] [15843] [INFO] Booting worker with pid: 15843
2023-10-04 14:29:00.645119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:29:00.661082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:29:02.220117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:29:02.220117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:29:03.702903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:03.702903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:03.713012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:03.713012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:03.717635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:03.717860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:05.761264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:05.762005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:05.765095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:05.766658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:05.769525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:05.771109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.321358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.324046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.325926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.328114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:29:07.435797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.437614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.439614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:29:07.441480: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:29:08.564651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.565740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.566823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.567825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.568824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.569833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.570885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.571877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.572872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.573863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.574935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.575934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.576946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.577939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.579006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.580017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.581015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.582022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.583115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.584116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.585104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.586095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.587145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:29:08.588141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:29:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15842)
[2023-10-04 14:30:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15843)
[2023-10-04 14:32:15 +0000] [684] [ERROR] Worker (pid:15842) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:32:15 +0000] [15927] [INFO] Booting worker with pid: 15927
[2023-10-04 14:32:15 +0000] [684] [ERROR] Worker (pid:15843) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:32:15 +0000] [15928] [INFO] Booting worker with pid: 15928
2023-10-04 14:32:34.965695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:32:34.965695: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:32:36.725549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:32:36.725549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:32:39.596390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:39.596390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:39.611883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:39.611883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:39.616494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:39.616713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:42.524108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:42.524176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:42.528599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:42.528828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:42.533063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:42.533278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.304897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.307288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.309069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.310735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:32:45.411261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.413855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.415743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:32:45.417686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:32:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15927)
[2023-10-04 14:32:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:15928)
[2023-10-04 14:32:46 +0000] [684] [ERROR] Worker (pid:15928) was sent code 134!
[2023-10-04 14:32:46 +0000] [16025] [INFO] Booting worker with pid: 16025
[2023-10-04 14:32:46 +0000] [684] [ERROR] Worker (pid:15927) was sent code 134!
[2023-10-04 14:32:46 +0000] [16026] [INFO] Booting worker with pid: 16026
2023-10-04 14:32:59.071801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:32:59.089313: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:33:00.446133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:33:00.446133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:33:01.815807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:01.815807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:01.824789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:01.824789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:01.829254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:01.829479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:03.954671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:03.955206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:03.959022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:03.959680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:03.963440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:03.964143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.528020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.530542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.532455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.534095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:33:05.573405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.575356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.577123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:33:05.579064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:33:06.743376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.744682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.745659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.746697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.747681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.748679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.749654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.750688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.751742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.752799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.753801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.754861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.755894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.756948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.757992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.759096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.760118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.761130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.762174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.763243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.764244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.765243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.766270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:33:06.767308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:33:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16025)
[2023-10-04 14:34:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16026)
[2023-10-04 14:35:49 +0000] [684] [ERROR] Worker (pid:16025) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:35:50 +0000] [16112] [INFO] Booting worker with pid: 16112
[2023-10-04 14:35:50 +0000] [684] [ERROR] Worker (pid:16026) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:35:50 +0000] [16113] [INFO] Booting worker with pid: 16113
2023-10-04 14:36:09.396457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:36:09.396457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:36:11.150113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:36:11.150112: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:36:13.869450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:13.869450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:13.884160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:13.884160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:13.888531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:13.888781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:16.767270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:16.767270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:16.771851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:16.772107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:16.776424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:16.776680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.460589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.462808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.466092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.467846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.471050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.472818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:19.476098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:36:19.476880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:36:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16112)
[2023-10-04 14:36:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16113)
[2023-10-04 14:36:20 +0000] [684] [ERROR] Worker (pid:16113) was sent code 134!
[2023-10-04 14:36:20 +0000] [16238] [INFO] Booting worker with pid: 16238
[2023-10-04 14:36:20 +0000] [684] [ERROR] Worker (pid:16112) was sent code 134!
[2023-10-04 14:36:20 +0000] [16239] [INFO] Booting worker with pid: 16239
2023-10-04 14:36:33.650151: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:36:33.650156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:36:34.757247: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:36:34.757248: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:36:35.960762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:35.960762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:35.970047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:35.970047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:35.974480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:35.974728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:38.050676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:38.050755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:38.055229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:38.055459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:38.059711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:38.059933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.652862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.656084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.658636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.660925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:36:39.697388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.700140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.702714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:36:39.705359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:36:40.886418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.887724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.888710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.889689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.890723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.891730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.892856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.893984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.895101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.896103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.897099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.898102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.899168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.900166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.901173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.902196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.903229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.904221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.905215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.906236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.907275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.908274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.909301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:36:40.910339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:37:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16239)
[2023-10-04 14:38:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16238)
[2023-10-04 14:39:32 +0000] [684] [ERROR] Worker (pid:16239) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:39:32 +0000] [16294] [INFO] Booting worker with pid: 16294
[2023-10-04 14:39:33 +0000] [684] [ERROR] Worker (pid:16238) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:39:33 +0000] [16315] [INFO] Booting worker with pid: 16315
2023-10-04 14:39:52.461108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:39:52.461107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:39:54.254509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:39:54.254514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:39:57.137235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:39:57.137235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:39:57.155214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:39:57.155214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:39:57.159714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:39:57.159964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:00.275233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:00.275273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:00.279666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:00.279919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:00.284115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:00.284334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.090371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.092757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.094763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.096702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:40:03.165287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.167427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.169273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:03.171202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:40:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16294)
[2023-10-04 14:40:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16315)
[2023-10-04 14:40:04 +0000] [684] [ERROR] Worker (pid:16294) was sent code 134!
[2023-10-04 14:40:04 +0000] [16410] [INFO] Booting worker with pid: 16410
[2023-10-04 14:40:04 +0000] [684] [ERROR] Worker (pid:16315) was sent code 134!
[2023-10-04 14:40:04 +0000] [16411] [INFO] Booting worker with pid: 16411
2023-10-04 14:40:17.020999: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:40:17.031812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:40:18.315908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:40:18.315908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:40:19.689811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:19.689811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:19.699463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:19.699463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:19.703821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:19.704084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:21.799751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:21.799996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:21.803725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:21.804370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:21.807927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:21.808576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.230870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.233874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.236590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.239186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:40:23.530628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.532830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.534759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:40:23.536711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:40:24.665980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.667135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.668236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.669316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.670388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.671506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.672562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.673654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.674785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.675905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.676953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.678008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.679130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.680186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.681238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.682310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.683404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.684492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.685567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.686653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.687715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.688782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.689857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:40:24.691029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:40:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16410)
[2023-10-04 14:41:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16411)
[2023-10-04 14:41:27 +0000] [684] [ERROR] Worker (pid:16410) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:41:27 +0000] [16472] [INFO] Booting worker with pid: 16472
[2023-10-04 14:41:27 +0000] [684] [ERROR] Worker (pid:16411) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:41:27 +0000] [16473] [INFO] Booting worker with pid: 16473
2023-10-04 14:41:46.715225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:41:46.715225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:41:48.490571: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:41:48.490575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:41:51.307868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:51.307868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:51.325180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:51.325186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:51.329589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:51.329834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:54.314038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:54.314038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:54.318564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:54.318777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:54.323015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:54.323227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.851694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.854836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.856630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.858682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:41:56.890807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.892826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.894682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:41:56.896270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:41:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16472)
[2023-10-04 14:41:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16473)
[2023-10-04 14:41:58 +0000] [684] [ERROR] Worker (pid:16472) was sent code 134!
[2023-10-04 14:41:58 +0000] [16560] [INFO] Booting worker with pid: 16560
[2023-10-04 14:41:58 +0000] [684] [ERROR] Worker (pid:16473) was sent code 134!
[2023-10-04 14:41:58 +0000] [16561] [INFO] Booting worker with pid: 16561
2023-10-04 14:42:10.916227: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:42:11.933087: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:42:12.078121: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:42:12.773455: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:42:13.268344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:13.274701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:13.276799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:13.478708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:13.482252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:13.484682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:15.215417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:15.215630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:15.219860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:15.220098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:15.224294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:15.224550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.660505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.663279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.665278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.667424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:42:16.813882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.815735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.817489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:42:16.819323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:42:17.937980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.939132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.940151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.941156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.942175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.943212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.944213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.945213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.946238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.947301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.948319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.949339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.950358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.951385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.952381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.953366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.954373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.955560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.956616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.957650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.958707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.959710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.960751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:42:17.961787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:42:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16560)
[2023-10-04 14:43:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16561)
[2023-10-04 14:45:19 +0000] [684] [ERROR] Worker (pid:16560) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:45:19 +0000] [16646] [INFO] Booting worker with pid: 16646
[2023-10-04 14:45:20 +0000] [684] [ERROR] Worker (pid:16561) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:45:20 +0000] [16655] [INFO] Booting worker with pid: 16655
2023-10-04 14:45:39.535706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:45:39.535706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:45:41.406212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:45:41.406212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:45:44.443760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:44.443760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:44.459992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:44.459992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:44.464541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:44.464786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:47.668164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:47.668198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:47.672675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:47.672991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:47.676886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:45:47.677561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 14:45:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16646)
[2023-10-04 14:45:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16655)
[2023-10-04 14:45:50 +0000] [684] [ERROR] Worker (pid:16655) was sent code 134!
[2023-10-04 14:45:50 +0000] [16800] [INFO] Booting worker with pid: 16800
[2023-10-04 14:45:50 +0000] [684] [ERROR] Worker (pid:16646) was sent code 134!
[2023-10-04 14:45:50 +0000] [16801] [INFO] Booting worker with pid: 16801
2023-10-04 14:46:00.997966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:46:01.016522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:46:02.640376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:46:02.640376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:46:04.140845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:04.140845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:04.149050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:04.149050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:04.153542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:04.153778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:06.377952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:06.378937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:06.381590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:06.383563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:06.386003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:06.388104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.039003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.042949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.044833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.046687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:46:08.079414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.081583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.084243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:46:08.086928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:46:09.298573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.299689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.300750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.301911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.303568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.305246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.306890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.308544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.310191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.311561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.312879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.314510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.316237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.317874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.319500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.320925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.322187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.323790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.325410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.327088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.328744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.330394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.332074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:46:09.333784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:46:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16800)
[2023-10-04 14:47:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16801)
[2023-10-04 14:48:58 +0000] [684] [ERROR] Worker (pid:16800) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:48:58 +0000] [16887] [INFO] Booting worker with pid: 16887
[2023-10-04 14:48:58 +0000] [684] [ERROR] Worker (pid:16801) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:48:58 +0000] [16888] [INFO] Booting worker with pid: 16888
2023-10-04 14:49:17.639215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:49:17.639215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:49:19.424646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:49:19.424646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:49:22.138478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:22.138478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:22.154435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:22.154435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:22.158783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:22.159029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:25.053208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:25.053310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:25.057647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:25.057907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:25.061867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:25.062078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.726881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.730738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.732770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.734691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:49:27.785636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.788043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.790725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:27.792711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:49:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16887)
[2023-10-04 14:49:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:16888)
[2023-10-04 14:49:29 +0000] [684] [ERROR] Worker (pid:16888) was sent code 134!
[2023-10-04 14:49:29 +0000] [17013] [INFO] Booting worker with pid: 17013
[2023-10-04 14:49:29 +0000] [684] [ERROR] Worker (pid:16887) was sent code 134!
[2023-10-04 14:49:29 +0000] [17014] [INFO] Booting worker with pid: 17014
2023-10-04 14:49:42.055509: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:49:42.073086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:49:43.432032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:49:43.432032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:49:44.613576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:44.613576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:44.623660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:44.623660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:44.628195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:44.628460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:46.646744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:46.647202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:46.651063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:46.651732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:46.655535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:46.656270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.067586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.069939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.072051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.074016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:49:48.266329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.268959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.271141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:49:48.272986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:49:49.408373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.409578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.410642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.411654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.412655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.413654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.414709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.415703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.416693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.417687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.418735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.419724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.420738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.421727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.422798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.423796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.424791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.425782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.426833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.427823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.428818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.429810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.430861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:49:49.431867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:50:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17013)
[2023-10-04 14:51:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17014)
[2023-10-04 14:53:32 +0000] [684] [ERROR] Worker (pid:17013) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:53:32 +0000] [17072] [INFO] Booting worker with pid: 17072
[2023-10-04 14:53:33 +0000] [684] [ERROR] Worker (pid:17014) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:53:33 +0000] [17079] [INFO] Booting worker with pid: 17079
2023-10-04 14:53:52.440689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:53:52.440689: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:53:54.227827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:53:54.227826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:53:56.975565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:56.975565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:56.992475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:56.992475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:56.996862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:56.997113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:59.819875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:59.819875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:59.824508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:59.824749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:59.829112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:53:59.829343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.526291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.528314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.529968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.532153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:54:02.609966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.612723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.615337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:02.617900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 14:54:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17072)
[2023-10-04 14:54:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17079)
[2023-10-04 14:54:04 +0000] [684] [ERROR] Worker (pid:17079) was sent code 134!
[2023-10-04 14:54:04 +0000] [17194] [INFO] Booting worker with pid: 17194
[2023-10-04 14:54:04 +0000] [684] [ERROR] Worker (pid:17072) was sent code 134!
[2023-10-04 14:54:04 +0000] [17195] [INFO] Booting worker with pid: 17195
2023-10-04 14:54:17.182255: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:54:17.200330: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:54:18.448379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:54:18.448379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:54:19.948197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:19.948197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:19.957525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:19.957525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:19.961923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:19.962166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:22.067112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:22.067554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:22.071293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:22.071976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:22.075701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:22.076393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.700402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.702749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.704483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.706373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:54:23.724284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.726165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.728068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:54:23.729808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:54:24.859145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.860442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.861416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.862397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.863401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.864620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.866260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.867713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.869162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.870824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.872430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.873993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.875651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.877251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.878884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.880161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.881755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.883396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.884874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.886509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.888139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.889735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.891360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:54:24.892956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:54:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17194)
[2023-10-04 14:56:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17195)
[2023-10-04 14:57:15 +0000] [684] [ERROR] Worker (pid:17194) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:57:15 +0000] [17261] [INFO] Booting worker with pid: 17261
[2023-10-04 14:57:15 +0000] [684] [ERROR] Worker (pid:17195) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 14:57:15 +0000] [17262] [INFO] Booting worker with pid: 17262
2023-10-04 14:57:34.403369: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:57:34.403369: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:57:36.085093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:57:36.085092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:57:38.772093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:38.772093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:38.788338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:38.788364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:38.792608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:38.792858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:41.588471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:41.588578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:41.592874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:41.593120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:41.597310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:41.597560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.188730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.191868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.193940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.196231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:57:44.239023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.241051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.242989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:57:44.244722: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:57:45.303719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.304845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.305886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.306974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.308019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.309057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.310086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.311209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.312241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.313268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.314318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.315398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.316468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.317497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.318619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.319657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.320685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.321711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.322792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.323825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.324854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.325905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.326990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:57:45.328041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:57:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17261)
[2023-10-04 14:57:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17262)
[2023-10-04 14:57:46 +0000] [684] [ERROR] Worker (pid:17262) was sent code 134!
[2023-10-04 14:57:46 +0000] [17361] [INFO] Booting worker with pid: 17361
[2023-10-04 14:57:46 +0000] [684] [ERROR] Worker (pid:17261) was sent code 134!
[2023-10-04 14:57:46 +0000] [17362] [INFO] Booting worker with pid: 17362
2023-10-04 14:57:59.236419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:57:59.236419: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 14:58:00.816541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:58:00.816541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 14:58:03.252988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:03.252988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:03.264991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:03.264998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:03.269244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:03.269501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:05.961611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:05.961611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:05.966002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:05.966251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:05.970437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:05.970701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.458667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.462887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.464733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.466469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:58:07.572841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.575253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.577118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 14:58:07.578930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 14:58:08.697375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.699056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.700475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.702086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.703717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.705312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.706402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.708027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.709625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.711253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.712853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.714457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.716107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.717708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.719031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.720649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.722295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.724074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.725714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.727375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.728986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.729958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.730972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 14:58:08.731941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 14:58:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17362)
[2023-10-04 14:59:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17361)
[2023-10-04 15:01:04 +0000] [684] [ERROR] Worker (pid:17362) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:01:04 +0000] [17416] [INFO] Booting worker with pid: 17416
[2023-10-04 15:01:05 +0000] [684] [ERROR] Worker (pid:17361) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:01:05 +0000] [17423] [INFO] Booting worker with pid: 17423
2023-10-04 15:01:23.883674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:01:23.883674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:01:25.642624: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:01:25.642640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:01:28.376537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:28.376537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:28.392479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:28.392485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:28.396749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:28.397002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:31.218831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:31.218843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:31.223302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:31.223551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:31.227843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:31.228106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.927862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.930244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.932701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.935240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:01:33.937479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.939546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.941803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:33.944255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:01:35.053116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.054403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.055408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.056373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.057319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.058394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.059404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.060364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.061326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.062337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.063373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.064389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.065399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.066414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.067452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.068455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.069452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.070459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.071480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.072477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.073468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.074492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.075526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:01:35.076523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:01:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17416)
[2023-10-04 15:01:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17423)
[2023-10-04 15:01:36 +0000] [684] [ERROR] Worker (pid:17423) was sent code 134!
[2023-10-04 15:01:36 +0000] [17514] [INFO] Booting worker with pid: 17514
[2023-10-04 15:01:36 +0000] [684] [ERROR] Worker (pid:17416) was sent code 134!
[2023-10-04 15:01:36 +0000] [17515] [INFO] Booting worker with pid: 17515
2023-10-04 15:01:50.453534: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:01:50.453537: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:01:52.269465: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:01:52.269462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:01:54.926700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:54.926700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:54.938515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:54.938515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:54.942993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:54.943255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:57.753501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:57.754011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:57.757812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:57.758454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:57.762300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:57.762963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.231667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.234177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.236045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.237875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:01:59.357049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.359041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.360864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:01:59.362674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:02:00.497739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.498871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.499853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.500827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.502295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.503979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.505453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.507140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.508747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.510116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.511771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.513383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.515199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.516846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.518504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.520134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.521864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.523688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.525432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.527201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.528862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.530520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.532255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:02:00.533990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:02:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17515)
[2023-10-04 15:03:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17514)
[2023-10-04 15:04:42 +0000] [684] [ERROR] Worker (pid:17515) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:04:42 +0000] [17570] [INFO] Booting worker with pid: 17570
[2023-10-04 15:04:43 +0000] [684] [ERROR] Worker (pid:17514) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:04:43 +0000] [17591] [INFO] Booting worker with pid: 17591
2023-10-04 15:05:01.982640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:05:01.982640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:05:03.857289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:05:03.857291: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:05:06.779956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:06.779956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:06.796136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:06.796136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:06.800568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:06.800807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:09.875222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:09.875222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:09.879748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:09.880014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:09.884463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:09.884672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.630393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.633265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.635374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.637437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:05:12.668913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.671222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.672967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:12.674648: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
[2023-10-04 15:05:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17570)
[2023-10-04 15:05:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17591)
[2023-10-04 15:05:13 +0000] [684] [ERROR] Worker (pid:17591) was sent code 134!
[2023-10-04 15:05:13 +0000] [17686] [INFO] Booting worker with pid: 17686
[2023-10-04 15:05:13 +0000] [684] [ERROR] Worker (pid:17570) was sent code 134!
[2023-10-04 15:05:13 +0000] [17687] [INFO] Booting worker with pid: 17687
2023-10-04 15:05:26.681934: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:05:26.698996: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:05:28.019104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:05:28.019104: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:05:29.313794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:29.313794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:29.323542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:29.323542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:29.327972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:29.328195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:31.251385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:31.251385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:31.255853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:31.256088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:31.260267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:31.260520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.810357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.813969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.816421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.818682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:05:32.851577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.853568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.855644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:05:32.857520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:05:34.004723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.006015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.007102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.008086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.009073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.010064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.011134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.012154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.013156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.014172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.015230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.016238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.017265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.018278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.019327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.020322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.021318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.022328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.023475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.024563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.025624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.026703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.027739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:05:34.028848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:06:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17686)
[2023-10-04 15:07:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17687)
[2023-10-04 15:10:00 +0000] [684] [ERROR] Worker (pid:17686) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:10:00 +0000] [17752] [INFO] Booting worker with pid: 17752
[2023-10-04 15:10:01 +0000] [684] [ERROR] Worker (pid:17687) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:10:01 +0000] [17754] [INFO] Booting worker with pid: 17754
2023-10-04 15:10:24.378718: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:10:24.378718: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:10:26.269735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:10:26.269736: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:10:29.076532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:29.076532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:29.090040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:29.090069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:29.094533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:29.094797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:10:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17752)
[2023-10-04 15:10:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17754)
[2023-10-04 15:10:31 +0000] [684] [ERROR] Worker (pid:17754) was sent code 134!
[2023-10-04 15:10:31 +0000] [17831] [INFO] Booting worker with pid: 17831
[2023-10-04 15:10:31 +0000] [684] [ERROR] Worker (pid:17752) was sent code 134!
[2023-10-04 15:10:31 +0000] [17832] [INFO] Booting worker with pid: 17832
2023-10-04 15:10:37.648495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:10:37.692945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:10:38.484175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:10:38.515808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:10:39.181539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:39.184713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:39.186652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:39.209431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:39.212638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:39.214729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:41.203694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:41.203735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:41.208263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:41.208499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:41.212711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:41.212956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.851383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.853520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.855666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.857341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:10:43.947082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.949176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.951228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:10:43.952981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:10:45.108572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.109895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.111503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.113488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.115093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.116829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.118974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.120279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.122251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.123803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.125355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.127377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.128586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.130581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.132577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.134216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.135878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.137553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.139227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.140860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.143908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.145582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.146750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:10:45.147896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:11:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17831)
[2023-10-04 15:13:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17832)
[2023-10-04 15:15:43 +0000] [684] [ERROR] Worker (pid:17831) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:15:43 +0000] [17886] [INFO] Booting worker with pid: 17886
[2023-10-04 15:15:43 +0000] [684] [ERROR] Worker (pid:17832) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:15:43 +0000] [17888] [INFO] Booting worker with pid: 17888
2023-10-04 15:16:07.614012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:16:07.614012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:16:09.448951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:16:09.448951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:16:12.228501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:12.228501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:12.244154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:12.244154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:12.248760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:12.248973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:16:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17886)
[2023-10-04 15:16:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:17888)
[2023-10-04 15:16:14 +0000] [684] [ERROR] Worker (pid:17886) was sent code 134!
[2023-10-04 15:16:14 +0000] [18031] [INFO] Booting worker with pid: 18031
[2023-10-04 15:16:14 +0000] [684] [ERROR] Worker (pid:17888) was sent code 134!
[2023-10-04 15:16:14 +0000] [18032] [INFO] Booting worker with pid: 18032
2023-10-04 15:16:20.227575: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:16:20.249822: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:16:21.061045: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:16:21.077535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:16:21.762897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:21.766135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:21.768208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:21.773884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:21.777028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:21.779122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:24.104577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:24.104596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:24.108980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:24.109220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:24.113421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:24.113660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.697265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.700106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.702658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.704870: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:16:26.753419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.755650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.757518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:16:26.759705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:16:27.879927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.881127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.882241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.883343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.884369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.885487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.886549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.887628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.888714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.889733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.890822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.891847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.892883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.893920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.894987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.896007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.897028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.898047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.899101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.900125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.901216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.902281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.903317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:16:27.904330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:17:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18032)
[2023-10-04 15:18:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18031)
[2023-10-04 15:20:31 +0000] [684] [ERROR] Worker (pid:18032) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:20:31 +0000] [18120] [INFO] Booting worker with pid: 18120
[2023-10-04 15:20:32 +0000] [684] [ERROR] Worker (pid:18031) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:20:32 +0000] [18126] [INFO] Booting worker with pid: 18126
2023-10-04 15:20:55.691391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:20:55.691391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:20:57.467025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:20:57.467026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:21:00.243534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:00.243534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:00.261080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:00.261080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:00.265444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:00.265694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:21:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18120)
[2023-10-04 15:21:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18126)
[2023-10-04 15:21:02 +0000] [684] [ERROR] Worker (pid:18120) was sent code 134!
[2023-10-04 15:21:02 +0000] [18215] [INFO] Booting worker with pid: 18215
[2023-10-04 15:21:03 +0000] [684] [ERROR] Worker (pid:18126) was sent code 134!
[2023-10-04 15:21:03 +0000] [18216] [INFO] Booting worker with pid: 18216
2023-10-04 15:21:09.070198: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:21:09.111855: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:21:09.896144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:21:09.939018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:21:10.584151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:10.587599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:10.589694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:10.631692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:10.634702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:10.636775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:12.459006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:12.459113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:12.463670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:12.463870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:12.468083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:12.468326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.140618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.140619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.145865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.146145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.150746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.151018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:21:15.155595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:21:15.155852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:21:16.322490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.323619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.324653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.325728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.326854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.327898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.328926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.329960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.331027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.332036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.333040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.334051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.335102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.336107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.337106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.338097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.339152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.340147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.341161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.342220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.343361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.344447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.345481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:21:16.346514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:21:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18215)
[2023-10-04 15:23:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18216)
[2023-10-04 15:26:03 +0000] [684] [ERROR] Worker (pid:18215) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:26:03 +0000] [18281] [INFO] Booting worker with pid: 18281
[2023-10-04 15:26:03 +0000] [684] [ERROR] Worker (pid:18216) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:26:03 +0000] [18283] [INFO] Booting worker with pid: 18283
2023-10-04 15:26:27.636506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:26:27.636506: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:26:29.344819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:26:29.344825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:26:32.130065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:32.130065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:32.146179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:32.146182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:32.150649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:32.150884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:26:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18281)
[2023-10-04 15:26:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18283)
[2023-10-04 15:26:34 +0000] [684] [ERROR] Worker (pid:18283) was sent code 134!
[2023-10-04 15:26:34 +0000] [18425] [INFO] Booting worker with pid: 18425
[2023-10-04 15:26:34 +0000] [684] [ERROR] Worker (pid:18281) was sent code 134!
[2023-10-04 15:26:34 +0000] [18426] [INFO] Booting worker with pid: 18426
2023-10-04 15:26:40.102891: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:26:40.108154: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:26:40.894199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:26:40.899257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:26:41.553479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:41.554563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:41.557995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:41.558837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:41.561369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:41.563384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:43.704399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:43.704446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:43.708769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:43.709019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:43.713175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:43.713426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.487487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.490373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.492375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.494940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:26:46.526159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.528815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.531379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:26:46.533897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:26:47.632668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.633939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.634955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.635926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.636887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.637930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.639255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.640428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.641875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.643242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.644398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.645454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.646820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.648266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.649895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.651360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.652958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.654568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.656194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.657779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.659184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.660346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.661523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:26:47.662611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:27:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18425)
[2023-10-04 15:29:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18426)
[2023-10-04 15:31:39 +0000] [684] [ERROR] Worker (pid:18425) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:31:39 +0000] [18487] [INFO] Booting worker with pid: 18487
[2023-10-04 15:31:39 +0000] [684] [ERROR] Worker (pid:18426) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:31:39 +0000] [18492] [INFO] Booting worker with pid: 18492
2023-10-04 15:32:03.774582: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:32:03.774582: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:32:05.520855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:32:05.520855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:32:08.333380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:08.333380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:08.350384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:08.350417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:08.354840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:08.355103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:32:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18487)
[2023-10-04 15:32:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18492)
[2023-10-04 15:32:10 +0000] [684] [ERROR] Worker (pid:18492) was sent code 134!
[2023-10-04 15:32:10 +0000] [18664] [INFO] Booting worker with pid: 18664
[2023-10-04 15:32:10 +0000] [684] [ERROR] Worker (pid:18487) was sent code 134!
[2023-10-04 15:32:10 +0000] [18665] [INFO] Booting worker with pid: 18665
2023-10-04 15:32:16.157112: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:32:16.961899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:32:17.207164: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:32:17.622288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:17.625411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:17.627413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:18.008565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:32:18.662562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:18.665707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:18.667757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:19.870716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:19.872865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:19.874759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:20.162322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:20.164857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:20.167103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.610752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.613097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.614884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.616697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:32:22.624594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.626750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.628732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:32:22.630585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:32:23.762771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.764482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.765976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.767662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.768958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.770153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.771558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.772936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.774630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.775900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.777057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.778745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.780424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.782084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.783647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.784854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.786073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.787261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.788420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.789578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.790776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.791929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.793071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:32:23.794262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:33:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18665)
[2023-10-04 15:34:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18664)
[2023-10-04 15:35:36 +0000] [684] [ERROR] Worker (pid:18665) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:35:36 +0000] [18750] [INFO] Booting worker with pid: 18750
[2023-10-04 15:35:37 +0000] [684] [ERROR] Worker (pid:18664) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:35:37 +0000] [18751] [INFO] Booting worker with pid: 18751
2023-10-04 15:36:00.026249: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:36:00.026249: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:36:01.754878: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:36:01.754881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:36:04.533704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:04.533704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:04.545661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:04.545712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:04.549969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:04.550225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:07.316311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:07.316380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:07.320714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:07.320965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:07.325145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:07.325394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:36:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18750)
[2023-10-04 15:36:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18751)
[2023-10-04 15:36:07 +0000] [684] [ERROR] Worker (pid:18750) was sent code 134!
[2023-10-04 15:36:07 +0000] [18852] [INFO] Booting worker with pid: 18852
[2023-10-04 15:36:07 +0000] [684] [ERROR] Worker (pid:18751) was sent code 134!
[2023-10-04 15:36:07 +0000] [18853] [INFO] Booting worker with pid: 18853
2023-10-04 15:36:13.705168: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:36:13.727540: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:36:14.493841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:36:14.512534: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:36:15.146760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:15.149866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:15.151930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:15.166692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:15.169707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:15.171782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:16.603233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:16.605379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:16.607488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:16.612496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:16.614930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:16.617257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.221433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.224134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.226493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.228815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:36:19.320357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.322171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.323878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:36:19.325603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:36:20.400397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.401507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.402538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.403593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.404604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.405618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.406673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.407929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.408943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.409960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.411024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.412036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.413417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.414433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.415852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.416864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.417875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.419094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.420111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.421127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.422514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.423592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.424604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:36:20.425627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:36:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18852)
[2023-10-04 15:38:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18853)
[2023-10-04 15:40:08 +0000] [684] [ERROR] Worker (pid:18852) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:40:08 +0000] [18918] [INFO] Booting worker with pid: 18918
[2023-10-04 15:40:08 +0000] [684] [ERROR] Worker (pid:18853) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:40:08 +0000] [18920] [INFO] Booting worker with pid: 18920
2023-10-04 15:40:31.879783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:40:31.879783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:40:33.647113: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:40:33.647118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:40:36.452896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:36.452896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:36.468482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:36.468491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:36.472804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:36.473061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:40:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18918)
[2023-10-04 15:40:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18920)
[2023-10-04 15:40:39 +0000] [684] [ERROR] Worker (pid:18920) was sent code 134!
[2023-10-04 15:40:39 +0000] [18989] [INFO] Booting worker with pid: 18989
[2023-10-04 15:40:39 +0000] [684] [ERROR] Worker (pid:18918) was sent code 134!
[2023-10-04 15:40:39 +0000] [18990] [INFO] Booting worker with pid: 18990
2023-10-04 15:40:45.422970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:40:45.422970: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:40:46.230133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:40:46.244493: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:40:46.912291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:46.915543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:46.917492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:46.936017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:46.939134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:46.941026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:48.684103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:48.684103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:48.688515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:48.688766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:48.692954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:48.693198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.183830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.187194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.190099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.192857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19018 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:40:51.430184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.432209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.433975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:40:51.435677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:40:52.682384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.683562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.685016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.686481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.688181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.690037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.691250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.692863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.694496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.696099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.697519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.698605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.699826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.701959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.703401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.704891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.706217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.708244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.709359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.710392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.712251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.713498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.714666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.716227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:40:52.717251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:41:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18989)
[2023-10-04 15:43:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:18990)
[2023-10-04 15:45:24 +0000] [684] [ERROR] Worker (pid:18989) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:45:24 +0000] [19045] [INFO] Booting worker with pid: 19045
[2023-10-04 15:45:24 +0000] [684] [ERROR] Worker (pid:18990) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:45:24 +0000] [19048] [INFO] Booting worker with pid: 19048
2023-10-04 15:45:48.439259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:45:48.439259: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:45:50.281091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:45:50.281092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:45:53.257242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:45:53.257242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:45:53.273623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:45:53.273623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:45:53.278220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:45:53.278461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:45:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19045)
[2023-10-04 15:45:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19048)
[2023-10-04 15:45:55 +0000] [684] [ERROR] Worker (pid:19048) was sent code 134!
[2023-10-04 15:45:55 +0000] [19191] [INFO] Booting worker with pid: 19191
[2023-10-04 15:45:55 +0000] [684] [ERROR] Worker (pid:19045) was sent code 134!
[2023-10-04 15:45:55 +0000] [19192] [INFO] Booting worker with pid: 19192
2023-10-04 15:46:01.245628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:46:01.329186: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:46:02.078948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:46:02.155029: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:46:02.774251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:02.777381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:02.779659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:02.842968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:02.846211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:02.848260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:05.077244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:05.077244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:05.081649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:05.081902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:05.086105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:05.086355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.691715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.694117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.696042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.697762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:46:07.697827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.699942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.701777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:46:07.703677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:46:08.852546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.854486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.855592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.856556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.857529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.858515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.859513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.860494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.861476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.862460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.863507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.864892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.866046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.867199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.868313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.869372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.870458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.871506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.872523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.873540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.874619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.875921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.877001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:46:08.878024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:46:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19191)
[2023-10-04 15:48:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19192)
[2023-10-04 15:50:19 +0000] [684] [ERROR] Worker (pid:19191) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:50:19 +0000] [19270] [INFO] Booting worker with pid: 19270
[2023-10-04 15:50:20 +0000] [684] [ERROR] Worker (pid:19192) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:50:20 +0000] [19280] [INFO] Booting worker with pid: 19280
2023-10-04 15:50:43.459159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:50:43.459159: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:50:45.201927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:50:45.201927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:50:48.094295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:48.094295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:48.108931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:48.108931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:48.113328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:48.113591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:50:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19270)
[2023-10-04 15:50:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19280)
[2023-10-04 15:50:50 +0000] [684] [ERROR] Worker (pid:19270) was sent code 134!
[2023-10-04 15:50:50 +0000] [19356] [INFO] Booting worker with pid: 19356
[2023-10-04 15:50:50 +0000] [684] [ERROR] Worker (pid:19280) was sent code 134!
[2023-10-04 15:50:50 +0000] [19357] [INFO] Booting worker with pid: 19357
2023-10-04 15:50:56.814913: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:50:56.824075: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:50:57.631410: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:50:57.642974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:50:58.307702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:58.310899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:58.311748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:58.314044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:58.316955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:50:58.319236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:00.133249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:00.133351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:00.137701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:00.137965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:00.142256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:00.142450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.874041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.876842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.879171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.881050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:51:02.881527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.884056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.886305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:51:02.888024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:51:04.002005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.003385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.004374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.005346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.006334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.007653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.009277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.010953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.012544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.014186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.015777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.017386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.019038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.020592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.021909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.023050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.024086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.025254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.026698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.028231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.029813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.031134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.032537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:51:04.034131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:51:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19356)
[2023-10-04 15:53:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19357)
[2023-10-04 15:55:01 +0000] [684] [ERROR] Worker (pid:19356) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:55:01 +0000] [19437] [INFO] Booting worker with pid: 19437
[2023-10-04 15:55:01 +0000] [684] [ERROR] Worker (pid:19357) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:55:01 +0000] [19441] [INFO] Booting worker with pid: 19441
2023-10-04 15:55:24.981600: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:55:24.981601: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:55:26.797595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:55:26.797595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:55:29.640081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:29.640081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:29.653426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:29.653439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:29.657828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:29.658058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 15:55:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19437)
[2023-10-04 15:55:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19441)
[2023-10-04 15:55:32 +0000] [684] [ERROR] Worker (pid:19437) was sent code 134!
[2023-10-04 15:55:32 +0000] [19512] [INFO] Booting worker with pid: 19512
[2023-10-04 15:55:32 +0000] [684] [ERROR] Worker (pid:19441) was sent code 134!
[2023-10-04 15:55:32 +0000] [19513] [INFO] Booting worker with pid: 19513
2023-10-04 15:55:38.112086: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:55:38.205560: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 15:55:38.939675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:55:39.035120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 15:55:39.618388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:39.621514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:39.623617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:39.728879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:39.732300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:39.734661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:41.649068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:41.649091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:41.653483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:41.653730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:41.657942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:41.658207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.287829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.290744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.293304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.295738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:55:44.432266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.434310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.436161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 15:55:44.437844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 15:55:45.538580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.539856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.540890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.542335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.543419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.544605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.546605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.547625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.548661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.549847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.550913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.552453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.553653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.554733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.556140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.557161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.558228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.559486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.560833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.561850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.563689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.565063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.566080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 15:55:45.567197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 15:56:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19512)
[2023-10-04 15:58:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19513)
[2023-10-04 15:59:46 +0000] [684] [ERROR] Worker (pid:19512) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:59:46 +0000] [19568] [INFO] Booting worker with pid: 19568
[2023-10-04 15:59:46 +0000] [684] [ERROR] Worker (pid:19513) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 15:59:46 +0000] [19569] [INFO] Booting worker with pid: 19569
2023-10-04 16:00:09.484763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:00:09.484763: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:00:11.212590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:00:11.212593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:00:13.943300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:13.943300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:13.959260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:13.959260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:13.963616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:13.963871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:00:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19568)
[2023-10-04 16:00:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19569)
[2023-10-04 16:00:16 +0000] [684] [ERROR] Worker (pid:19568) was sent code 134!
[2023-10-04 16:00:16 +0000] [19668] [INFO] Booting worker with pid: 19668
[2023-10-04 16:00:16 +0000] [684] [ERROR] Worker (pid:19569) was sent code 134!
[2023-10-04 16:00:16 +0000] [19669] [INFO] Booting worker with pid: 19669
2023-10-04 16:00:22.743783: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:00:22.815434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:00:23.571335: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:00:23.641939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:00:24.272987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:24.276430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:24.278639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:24.331350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:24.334733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:24.336772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:25.987374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:25.987373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:25.991796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:25.992042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:25.996235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:25.996488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.596001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.598932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.601084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.603354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:00:28.724329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.726439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.728365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:00:28.730078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:00:29.876544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.877850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.880018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.882222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.884720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.886291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.888323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.890114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.891917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.893718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.895778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.897703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.899522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.901345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.903292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.905178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.907120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.909561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.911463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.914051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.915932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.918331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.919482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:00:29.920960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:01:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19668)
[2023-10-04 16:01:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19669)
[2023-10-04 16:03:31 +0000] [684] [ERROR] Worker (pid:19668) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:03:31 +0000] [19722] [INFO] Booting worker with pid: 19722
[2023-10-04 16:03:31 +0000] [684] [ERROR] Worker (pid:19669) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:03:31 +0000] [19725] [INFO] Booting worker with pid: 19725
2023-10-04 16:03:55.218853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:03:55.218853: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:03:57.027487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:03:57.027491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:03:59.733447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:03:59.733447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:03:59.747686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:03:59.747696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:03:59.752001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:03:59.752249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:04:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19722)
[2023-10-04 16:04:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19725)
[2023-10-04 16:04:02 +0000] [684] [ERROR] Worker (pid:19722) was sent code 134!
[2023-10-04 16:04:02 +0000] [19794] [INFO] Booting worker with pid: 19794
[2023-10-04 16:04:02 +0000] [684] [ERROR] Worker (pid:19725) was sent code 134!
[2023-10-04 16:04:02 +0000] [19795] [INFO] Booting worker with pid: 19795
2023-10-04 16:04:08.336551: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:04:08.367471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:04:09.166916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:04:09.193429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:04:09.856184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:09.859356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:09.861588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:09.889697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:09.892830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:09.895011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:11.909622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:11.909656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:11.914063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:11.914326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:11.918550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:11.918793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.904564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.906784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.908645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.910565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:04:14.929650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.932324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.934940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:04:14.937456: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:04:16.076142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.077438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.078442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.079461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.080435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.081410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.082390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.083395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.084872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.086126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.087461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.088723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.090078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.091573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.093181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.094318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.095905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.097141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.098610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.099794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.101357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.102572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.103652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:04:16.104991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:04:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19794)
[2023-10-04 16:06:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19795)
[2023-10-04 16:08:40 +0000] [684] [ERROR] Worker (pid:19794) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:08:40 +0000] [19885] [INFO] Booting worker with pid: 19885
[2023-10-04 16:08:41 +0000] [684] [ERROR] Worker (pid:19795) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:08:41 +0000] [19886] [INFO] Booting worker with pid: 19886
2023-10-04 16:09:04.991479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:09:04.991479: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:09:06.854529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:09:06.854529: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:09:09.601355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:09.601355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:09.616557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:09.616557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:09.620876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:09.621128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:09:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19885)
[2023-10-04 16:09:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:19886)
[2023-10-04 16:09:11 +0000] [684] [ERROR] Worker (pid:19886) was sent code 134!
[2023-10-04 16:09:11 +0000] [20044] [INFO] Booting worker with pid: 20044
[2023-10-04 16:09:11 +0000] [684] [ERROR] Worker (pid:19885) was sent code 134!
[2023-10-04 16:09:11 +0000] [20045] [INFO] Booting worker with pid: 20045
2023-10-04 16:09:17.592389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:09:17.599778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:09:18.426788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:09:18.433027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:09:19.124290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:19.127387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:19.128303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:19.130526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:19.133369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:19.135290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:21.445385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:21.445385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:21.449807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:21.450057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:21.454312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:21.454553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.210910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.213195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.215180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.216966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:09:24.332764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.335457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.337936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:09:24.340404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:09:25.474801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.475910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.476938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.478467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.479786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.481057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.482445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.483830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.484855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.485875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.487618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.488989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.490401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.491847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.493254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.494646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.496021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.497740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.499182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.500640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.501656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.503063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.504087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:09:25.505097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:10:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20044)
[2023-10-04 16:11:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20045)
[2023-10-04 16:12:53 +0000] [684] [ERROR] Worker (pid:20044) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:12:53 +0000] [20145] [INFO] Booting worker with pid: 20145
[2023-10-04 16:12:53 +0000] [684] [ERROR] Worker (pid:20045) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:12:53 +0000] [20162] [INFO] Booting worker with pid: 20162
2023-10-04 16:13:16.819356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:13:16.819356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:13:18.649642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:13:18.649642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:13:21.446953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:21.446953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:21.464105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:21.464105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:21.468656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:21.468893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:13:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20145)
[2023-10-04 16:13:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20162)
[2023-10-04 16:13:23 +0000] [684] [ERROR] Worker (pid:20162) was sent code 134!
[2023-10-04 16:13:23 +0000] [20280] [INFO] Booting worker with pid: 20280
[2023-10-04 16:13:23 +0000] [684] [ERROR] Worker (pid:20145) was sent code 134!
[2023-10-04 16:13:23 +0000] [20281] [INFO] Booting worker with pid: 20281
2023-10-04 16:13:29.851434: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:13:29.873666: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:13:30.687370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:13:30.705296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:13:31.381953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:31.385193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:31.387410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:31.397334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:31.400519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:31.402488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:33.566935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:33.566935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:33.571396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:33.571649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:33.575852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:33.576101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.423006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.425810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.428009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.430067: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:13:36.478159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.480180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.482051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:13:36.483814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:13:37.617165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.620209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.622838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.625637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.628668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.631466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.633869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.636804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.638897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.640980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.642556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.644557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.646166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.647860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.649085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.650745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.652828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.654101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.655799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.657030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.658687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.660253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.661416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:13:37.662584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:14:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20280)
[2023-10-04 16:15:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20281)
[2023-10-04 16:17:20 +0000] [684] [ERROR] Worker (pid:20280) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:17:20 +0000] [20334] [INFO] Booting worker with pid: 20334
[2023-10-04 16:17:21 +0000] [684] [ERROR] Worker (pid:20281) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:17:21 +0000] [20337] [INFO] Booting worker with pid: 20337
2023-10-04 16:17:44.774788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:17:44.774788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:17:46.666920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:17:46.666920: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:17:49.590560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:49.590560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:49.605865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:49.605910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:49.610303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:49.610538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:17:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20334)
[2023-10-04 16:17:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20337)
[2023-10-04 16:17:51 +0000] [684] [ERROR] Worker (pid:20337) was sent code 134!
[2023-10-04 16:17:51 +0000] [20407] [INFO] Booting worker with pid: 20407
[2023-10-04 16:17:51 +0000] [684] [ERROR] Worker (pid:20334) was sent code 134!
[2023-10-04 16:17:51 +0000] [20408] [INFO] Booting worker with pid: 20408
2023-10-04 16:17:57.677056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:17:57.693733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:17:58.504549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:17:58.527856: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:17:59.195076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:59.198459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:59.200574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:59.221393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:59.224641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:17:59.226734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:01.590870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:01.590870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:01.595366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:01.595663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:01.599897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:01.600117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.414303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.416513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.418253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.420102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:18:04.425137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.427304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.429084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:18:04.430911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:18:05.597345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.598696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.600261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.601505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.602575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.603677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.605120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.606572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.607931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.608995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.610186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.611565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.612844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.614006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.615339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.616646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.617834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.619103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.620684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.621844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.623029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.624133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.625180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:18:05.626200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:18:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20407)
[2023-10-04 16:20:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20408)
[2023-10-04 16:23:14 +0000] [684] [ERROR] Worker (pid:20407) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:23:14 +0000] [20496] [INFO] Booting worker with pid: 20496
[2023-10-04 16:23:15 +0000] [684] [ERROR] Worker (pid:20408) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:23:15 +0000] [20497] [INFO] Booting worker with pid: 20497
2023-10-04 16:23:38.807914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:23:38.807915: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:23:40.570974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:23:40.570974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:23:43.457685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:43.457686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:43.472937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:43.472937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:43.477469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:43.477729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:23:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20496)
[2023-10-04 16:23:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20497)
[2023-10-04 16:23:45 +0000] [684] [ERROR] Worker (pid:20496) was sent code 134!
[2023-10-04 16:23:45 +0000] [20641] [INFO] Booting worker with pid: 20641
[2023-10-04 16:23:45 +0000] [684] [ERROR] Worker (pid:20497) was sent code 134!
[2023-10-04 16:23:45 +0000] [20642] [INFO] Booting worker with pid: 20642
2023-10-04 16:23:51.609094: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:23:51.609093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:23:52.431261: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:23:52.431904: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:23:53.292992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:53.293981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:53.297687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:53.298736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:53.301214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:53.303211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:55.262627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:55.262643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:55.267124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:55.267348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:55.271503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:55.271756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.031174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.031177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.036315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.036587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.041327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.041600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:23:58.046146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:23:58.046363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:23:59.213989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.215296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.216283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.217268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.218246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.219283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.220266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.221292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.222308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.223372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.224367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.225378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.226417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.227452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.228469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.229471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.230481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.231523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.232534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.233543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.234611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.235610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.236609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:23:59.237622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:24:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20641)
[2023-10-04 16:24:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20642)
[2023-10-04 16:24:46 +0000] [684] [ERROR] Worker (pid:20641) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:24:46 +0000] [20699] [INFO] Booting worker with pid: 20699
[2023-10-04 16:24:46 +0000] [684] [ERROR] Worker (pid:20642) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:24:46 +0000] [20700] [INFO] Booting worker with pid: 20700
2023-10-04 16:25:10.143000: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:25:10.143000: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:25:11.861913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:25:11.861913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:25:14.979571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:14.979571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:14.993528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:14.993527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:14.997961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:14.998168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:25:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20699)
[2023-10-04 16:25:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20700)
[2023-10-04 16:25:17 +0000] [684] [ERROR] Worker (pid:20700) was sent code 134!
[2023-10-04 16:25:17 +0000] [20799] [INFO] Booting worker with pid: 20799
[2023-10-04 16:25:17 +0000] [684] [ERROR] Worker (pid:20699) was sent code 134!
[2023-10-04 16:25:17 +0000] [20800] [INFO] Booting worker with pid: 20800
2023-10-04 16:25:23.395124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:25:23.416022: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:25:24.222156: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:25:24.233730: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:25:25.075022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:25.075777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:25.079797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:25.080464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:25.083152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:25.084636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:26.887825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:26.887859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:26.892327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:26.892542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:26.896781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:26.897005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.664010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.666901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.669451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.671790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:25:29.824867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.827173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.828988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:25:29.830674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:25:30.941698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.942939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.944288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.946918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.950491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.952802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.955295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.957975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.960481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.961561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.963401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.965538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.968095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.970635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.972731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.974529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.976339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.978024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.979561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.980645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.981714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.983504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.984581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.985644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:25:30.986760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:26:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20799)
[2023-10-04 16:27:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20800)
[2023-10-04 16:29:46 +0000] [684] [ERROR] Worker (pid:20799) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:29:46 +0000] [20855] [INFO] Booting worker with pid: 20855
[2023-10-04 16:29:46 +0000] [684] [ERROR] Worker (pid:20800) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:29:46 +0000] [20858] [INFO] Booting worker with pid: 20858
2023-10-04 16:30:10.099958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:30:10.099958: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:30:11.909951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:30:11.909966: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:30:15.006056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:15.006054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:15.020231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:15.020279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:15.024546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:15.024823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:30:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20855)
[2023-10-04 16:30:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:20858)
[2023-10-04 16:30:16 +0000] [684] [ERROR] Worker (pid:20855) was sent code 134!
[2023-10-04 16:30:16 +0000] [21028] [INFO] Booting worker with pid: 21028
[2023-10-04 16:30:17 +0000] [684] [ERROR] Worker (pid:20858) was sent code 134!
[2023-10-04 16:30:17 +0000] [21029] [INFO] Booting worker with pid: 21029
2023-10-04 16:30:22.911528: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:30:22.999943: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:30:23.752238: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:30:23.833528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:30:24.615447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:24.618811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:24.621121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:24.695441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:24.698754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:24.701031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:26.733156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:26.733156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:26.737693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:26.737908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:26.742098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:26.742349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.285281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.288040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.290627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.292281: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:30:29.471324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.474043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.476740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:30:29.479285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:30:30.612585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.613697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.615091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.616385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.617409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.618841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.620221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.621817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.623334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.624773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.626181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.627640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.629048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.630688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.632462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.633864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.635353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.636987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.638340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.639770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.641509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.642911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.643943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:30:30.644962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:31:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21028)
[2023-10-04 16:32:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21029)
[2023-10-04 16:34:08 +0000] [684] [ERROR] Worker (pid:21028) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:34:08 +0000] [21092] [INFO] Booting worker with pid: 21092
[2023-10-04 16:34:08 +0000] [684] [ERROR] Worker (pid:21029) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:34:08 +0000] [21093] [INFO] Booting worker with pid: 21093
2023-10-04 16:34:31.329392: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:34:31.329392: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:34:33.108945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:34:33.108945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:34:36.046755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:36.046755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:36.062252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:36.062253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:36.066754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:36.067004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:34:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21092)
[2023-10-04 16:34:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21093)
[2023-10-04 16:34:38 +0000] [684] [ERROR] Worker (pid:21092) was sent code 134!
[2023-10-04 16:34:38 +0000] [21162] [INFO] Booting worker with pid: 21162
[2023-10-04 16:34:38 +0000] [684] [ERROR] Worker (pid:21093) was sent code 134!
[2023-10-04 16:34:38 +0000] [21163] [INFO] Booting worker with pid: 21163
2023-10-04 16:34:44.707720: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:34:44.730985: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:34:45.545607: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:34:45.557979: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:34:46.405210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:46.407868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:46.408685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:46.412149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:46.412239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:46.416457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:47.967578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:47.967578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:47.972035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:47.972297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:47.976430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:47.976677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.608188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.610916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.613475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.615560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.616771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:34:50.618678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.620739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:34:50.623232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:34:51.784243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.785524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.786514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.787527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.788500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.789478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.790542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.791595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.792609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.793603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.794734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.795752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.796787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.797795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.798881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.799907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.800917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.801934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.803092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.804131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.805161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.806182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.807252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:34:51.808267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:35:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21162)
[2023-10-04 16:37:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21163)
[2023-10-04 16:38:43 +0000] [684] [ERROR] Worker (pid:21162) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:38:43 +0000] [21218] [INFO] Booting worker with pid: 21218
[2023-10-04 16:38:44 +0000] [684] [ERROR] Worker (pid:21163) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:38:44 +0000] [21219] [INFO] Booting worker with pid: 21219
2023-10-04 16:39:07.365411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:39:07.365411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:39:09.177871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:39:09.177871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:39:12.074363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:12.074363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:12.092082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:12.092101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:12.096461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:12.096715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:39:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21218)
[2023-10-04 16:39:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21219)
[2023-10-04 16:39:14 +0000] [684] [ERROR] Worker (pid:21219) was sent code 134!
[2023-10-04 16:39:14 +0000] [21296] [INFO] Booting worker with pid: 21296
[2023-10-04 16:39:14 +0000] [684] [ERROR] Worker (pid:21218) was sent code 134!
[2023-10-04 16:39:14 +0000] [21310] [INFO] Booting worker with pid: 21310
2023-10-04 16:39:20.639056: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:39:20.711877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:39:21.475599: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:39:21.543584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:39:22.333679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:22.337014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:22.339184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:22.398517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:22.401815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:22.404058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:24.042762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:24.042762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:24.047199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:24.047455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:24.051641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:24.051897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.605760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.607941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.610024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.612313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:39:26.677725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.679693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.681468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:39:26.683448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:39:27.800522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.801674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.802818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.803879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.804935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.806010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.807132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.808208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.809386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.810459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.811578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.812620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.813673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.814758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.815808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.816859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.817925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.819032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.820079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.821119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.822164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.823270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.824328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:39:27.825379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:39:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21296)
[2023-10-04 16:41:48 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21310)
[2023-10-04 16:44:00 +0000] [684] [ERROR] Worker (pid:21296) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:44:00 +0000] [21372] [INFO] Booting worker with pid: 21372
[2023-10-04 16:44:01 +0000] [684] [ERROR] Worker (pid:21310) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:44:01 +0000] [21375] [INFO] Booting worker with pid: 21375
2023-10-04 16:44:25.147884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:44:25.147884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:44:26.927229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:44:26.927229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:44:29.873663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:29.873663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:29.888408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:29.888422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:29.893052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:29.893273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:44:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21372)
[2023-10-04 16:44:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21375)
[2023-10-04 16:44:31 +0000] [684] [ERROR] Worker (pid:21372) was sent code 134!
[2023-10-04 16:44:31 +0000] [21541] [INFO] Booting worker with pid: 21541
[2023-10-04 16:44:31 +0000] [684] [ERROR] Worker (pid:21375) was sent code 134!
[2023-10-04 16:44:31 +0000] [21542] [INFO] Booting worker with pid: 21542
2023-10-04 16:44:37.638289: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:44:37.739031: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:44:38.469118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:44:38.577212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:44:39.332592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:39.336067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:39.338181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:39.427823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:39.431143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:39.433287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:41.519116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:41.519116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:41.523681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:41.523937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:41.528282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:41.528536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.185336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.187243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.189006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.192337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.193774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.197330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:44:44.199003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:44:44.200383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:44:45.357596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.358947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.359941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.360924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.361910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.363065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.364775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.366510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.367775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.369185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.370342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.371783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.373527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.375226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.376487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.378116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.379538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.380822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.381995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.383192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.384617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.386160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.387436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:44:45.388967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:45:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21541)
[2023-10-04 16:47:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21542)
[2023-10-04 16:49:17 +0000] [684] [ERROR] Worker (pid:21541) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:49:17 +0000] [21600] [INFO] Booting worker with pid: 21600
[2023-10-04 16:49:18 +0000] [684] [ERROR] Worker (pid:21542) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:49:18 +0000] [21601] [INFO] Booting worker with pid: 21601
2023-10-04 16:49:41.712280: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:49:41.712280: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:49:43.499222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:49:43.499226: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:49:46.438614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:46.438617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:46.452441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:46.452441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:46.456871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:46.457121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:49:48 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21600)
[2023-10-04 16:49:48 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21601)
[2023-10-04 16:49:48 +0000] [684] [ERROR] Worker (pid:21600) was sent code 134!
[2023-10-04 16:49:48 +0000] [21745] [INFO] Booting worker with pid: 21745
[2023-10-04 16:49:48 +0000] [684] [ERROR] Worker (pid:21601) was sent code 134!
[2023-10-04 16:49:48 +0000] [21746] [INFO] Booting worker with pid: 21746
2023-10-04 16:49:54.380276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:49:54.472735: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:49:55.203667: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:49:55.314827: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:49:56.063521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:56.066895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:56.068930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:56.174679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:56.177785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:56.180006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:58.025832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:58.025832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:58.030306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:58.030546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:58.034881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:49:58.035095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:00.983823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:00.986647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:00.988583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:00.990969: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:50:01.025995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:01.028564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:01.030396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:50:01.032014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:50:02.159104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.160218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.161294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.162869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.163927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.164967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.166295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.168106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.169645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.170979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.172695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.173754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.174873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.176154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.177472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.179180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.181067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.182541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.184090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.185612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.188010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.189213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.190520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:50:02.191578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:50:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21745)
[2023-10-04 16:53:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21746)
[2023-10-04 16:55:09 +0000] [684] [ERROR] Worker (pid:21745) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:55:09 +0000] [21839] [INFO] Booting worker with pid: 21839
[2023-10-04 16:55:09 +0000] [684] [ERROR] Worker (pid:21746) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:55:09 +0000] [21840] [INFO] Booting worker with pid: 21840
2023-10-04 16:55:33.223295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:55:33.223295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:55:35.178175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:55:35.178172: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:55:38.145356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:38.145356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:38.160770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:38.160770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:38.165306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:38.165502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:55:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21839)
[2023-10-04 16:55:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21840)
[2023-10-04 16:55:40 +0000] [684] [ERROR] Worker (pid:21840) was sent code 134!
[2023-10-04 16:55:40 +0000] [21982] [INFO] Booting worker with pid: 21982
[2023-10-04 16:55:40 +0000] [684] [ERROR] Worker (pid:21839) was sent code 134!
[2023-10-04 16:55:40 +0000] [21983] [INFO] Booting worker with pid: 21983
2023-10-04 16:55:46.128888: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:55:46.143873: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:55:46.962451: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:55:46.973584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:55:47.829917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:47.833013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:47.833261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:47.837095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:47.838776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:47.841799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:49.905016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:49.905023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:49.909350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:49.909596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:49.913556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:49.913772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.527587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.530443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.532996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.535492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:55:52.590836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.593630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.596274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:55:52.598896: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 16:55:53.712397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.713433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.714466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.715488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.716506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.717680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.718800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.719847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.720872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.721897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.722998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.724038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.725090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.726123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.727206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.728230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.729242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.730292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.731435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.732475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.733524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.734559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.735620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 16:55:53.736624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 16:56:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21982)
[2023-10-04 16:57:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:21983)
[2023-10-04 16:59:21 +0000] [684] [ERROR] Worker (pid:21983) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:59:21 +0000] [22042] [INFO] Booting worker with pid: 22042
[2023-10-04 16:59:21 +0000] [684] [ERROR] Worker (pid:21982) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 16:59:21 +0000] [22043] [INFO] Booting worker with pid: 22043
2023-10-04 16:59:45.229731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:59:45.229730: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:59:47.099393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:59:47.099398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:59:50.212941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:59:50.212941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:59:50.227786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:59:50.227786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:59:50.232234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 16:59:50.232446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 16:59:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22042)
[2023-10-04 16:59:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22043)
[2023-10-04 16:59:52 +0000] [684] [ERROR] Worker (pid:22042) was sent code 134!
[2023-10-04 16:59:52 +0000] [22112] [INFO] Booting worker with pid: 22112
[2023-10-04 16:59:52 +0000] [684] [ERROR] Worker (pid:22043) was sent code 134!
[2023-10-04 16:59:52 +0000] [22113] [INFO] Booting worker with pid: 22113
2023-10-04 16:59:58.329381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:59:58.376221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 16:59:59.156139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 16:59:59.204026: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:00:00.015498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:00.019030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:00.021124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:00.060885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:00.064119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:00.066199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:01.946616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:01.946644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:01.951242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:01.951455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:01.955683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:01.955921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.090917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.093725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.096253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.098803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:00:05.255663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.257764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.259798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:00:05.261903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:00:06.441299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.443627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.445246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.446691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.448529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.449697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.451566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.453342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.454802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.456573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.457979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.459798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.461231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.463059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.464530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.466341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.467812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.469681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.471342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.472671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.474180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.475986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.477059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.478090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:00:06.479233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:00:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22112)
[2023-10-04 17:00:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22113)
[2023-10-04 17:01:02 +0000] [684] [ERROR] Worker (pid:22112) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:01:02 +0000] [22195] [INFO] Booting worker with pid: 22195
[2023-10-04 17:01:02 +0000] [684] [ERROR] Worker (pid:22113) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:01:02 +0000] [22196] [INFO] Booting worker with pid: 22196
2023-10-04 17:01:25.410238: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:01:25.410238: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:01:27.214490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:01:27.214490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:01:30.319104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:30.319104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:30.333806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:30.333828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:30.338287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:30.338552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:01:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22195)
[2023-10-04 17:01:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22196)
[2023-10-04 17:01:32 +0000] [684] [ERROR] Worker (pid:22196) was sent code 134!
[2023-10-04 17:01:32 +0000] [22256] [INFO] Booting worker with pid: 22256
[2023-10-04 17:01:32 +0000] [684] [ERROR] Worker (pid:22195) was sent code 134!
[2023-10-04 17:01:32 +0000] [22257] [INFO] Booting worker with pid: 22257
2023-10-04 17:01:38.638227: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:01:38.692295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:01:39.472754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:01:39.524355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:01:40.331671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:40.334979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:40.337067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:40.375639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:40.378770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:40.380481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:42.132864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:42.132873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:42.137405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:42.137665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:42.141929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:42.142295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.696206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.698832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.700862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.703208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:01:44.734536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.736567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.738478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:01:44.740349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:01:45.851184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.852933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.854105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.855773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.857396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.859025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.860332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.861902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.863575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.865195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.866787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.868131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.869744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.871394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.873006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.874665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.876256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.877868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.879488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.881080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.882668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.884000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.885646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:01:45.887295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:02:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22256)
[2023-10-04 17:04:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22257)
[2023-10-04 17:06:24 +0000] [684] [ERROR] Worker (pid:22256) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:06:24 +0000] [22311] [INFO] Booting worker with pid: 22311
[2023-10-04 17:06:25 +0000] [684] [ERROR] Worker (pid:22257) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:06:25 +0000] [22315] [INFO] Booting worker with pid: 22315
2023-10-04 17:06:49.091225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:06:49.091225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:06:50.904781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:06:50.904781: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:06:53.900514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:06:53.900513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:06:53.915785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:06:53.915785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:06:53.920049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:06:53.920250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:06:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22311)
[2023-10-04 17:06:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22315)
[2023-10-04 17:06:55 +0000] [684] [ERROR] Worker (pid:22315) was sent code 134!
[2023-10-04 17:06:55 +0000] [22455] [INFO] Booting worker with pid: 22455
[2023-10-04 17:06:55 +0000] [684] [ERROR] Worker (pid:22311) was sent code 134!
[2023-10-04 17:06:55 +0000] [22456] [INFO] Booting worker with pid: 22456
2023-10-04 17:07:01.574264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:07:01.642981: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:07:02.407822: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:07:02.457462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:07:03.264112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:03.267493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:03.269662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:03.300659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:03.303771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:03.305688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:05.629171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:05.629337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:05.633667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:05.633879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:05.638117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:05.638353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.380688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.383256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.385742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.387765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:07:08.449621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.451730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.453432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:07:08.455214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:07:09.589472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.590863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.592235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.593292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.594633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.595959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.598109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.599815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.601119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.602364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.603453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.604515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.606006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.607096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.608162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.609639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.611154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.612475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.613712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.615043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.616102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.617654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.618758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:07:09.619806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:07:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22455)
[2023-10-04 17:08:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22456)
[2023-10-04 17:10:14 +0000] [684] [ERROR] Worker (pid:22455) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:10:14 +0000] [22543] [INFO] Booting worker with pid: 22543
[2023-10-04 17:10:14 +0000] [684] [ERROR] Worker (pid:22456) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:10:14 +0000] [22544] [INFO] Booting worker with pid: 22544
2023-10-04 17:10:37.957265: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:10:37.957265: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:10:39.842907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:10:39.842911: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:10:42.916981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:42.916981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:42.931633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:42.931633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:42.936089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:42.936313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:10:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22543)
[2023-10-04 17:10:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22544)
[2023-10-04 17:10:45 +0000] [684] [ERROR] Worker (pid:22543) was sent code 134!
[2023-10-04 17:10:45 +0000] [22619] [INFO] Booting worker with pid: 22619
[2023-10-04 17:10:45 +0000] [684] [ERROR] Worker (pid:22544) was sent code 134!
[2023-10-04 17:10:45 +0000] [22620] [INFO] Booting worker with pid: 22620
2023-10-04 17:10:51.216663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:10:51.216663: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:10:52.036821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:10:52.051657: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:10:52.878707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:52.881975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:52.884087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:52.909717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:52.912818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:52.914713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:54.930833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:54.930833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:54.935333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:54.935564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:54.939905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:54.940122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.656582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.656585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.661831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.662108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.666977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.667253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:10:57.671966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:10:57.672219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:10:58.826136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.827475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.828466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.829451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.830440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.831457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.832451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.833433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.834461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.835591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.836612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.837632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.838713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.839718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.840749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.841752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.842801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.843821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.844837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.845841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.846889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.847917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.848938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:10:58.850014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:11:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22619)
[2023-10-04 17:13:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22620)
[2023-10-04 17:14:56 +0000] [684] [ERROR] Worker (pid:22619) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:14:56 +0000] [22676] [INFO] Booting worker with pid: 22676
[2023-10-04 17:14:56 +0000] [684] [ERROR] Worker (pid:22620) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:14:56 +0000] [22678] [INFO] Booting worker with pid: 22678
2023-10-04 17:15:19.949838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:15:19.949838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:15:21.784258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:15:21.784258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:15:24.801526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:24.801526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:24.818864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:24.818864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:24.823395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:24.823655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:15:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22676)
[2023-10-04 17:15:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22678)
[2023-10-04 17:15:26 +0000] [684] [ERROR] Worker (pid:22676) was sent code 134!
[2023-10-04 17:15:26 +0000] [22778] [INFO] Booting worker with pid: 22778
[2023-10-04 17:15:27 +0000] [684] [ERROR] Worker (pid:22678) was sent code 134!
[2023-10-04 17:15:27 +0000] [22779] [INFO] Booting worker with pid: 22779
2023-10-04 17:15:33.009009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:15:33.041669: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:15:33.838701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:15:33.869487: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:15:34.692225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:34.695502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:34.697526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:34.723083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:34.726186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:34.728260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:36.594143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:36.594151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:36.598566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:36.598833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:36.603076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:36.603343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.146144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.148332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.150245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.152026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:15:39.167673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.170291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.172817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:15:39.175418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:15:40.277804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.279178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.280200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.281204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.282198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.283269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.284272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.285279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.286375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.287519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.288577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.289630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.290745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.291828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.292889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.293963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.295077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.296114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.297155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.298196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.299313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.300377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.301430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:15:40.302499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:16:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22778)
[2023-10-04 17:18:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22779)
[2023-10-04 17:20:56 +0000] [684] [ERROR] Worker (pid:22778) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:20:56 +0000] [22840] [INFO] Booting worker with pid: 22840
[2023-10-04 17:20:56 +0000] [684] [ERROR] Worker (pid:22779) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:20:56 +0000] [22844] [INFO] Booting worker with pid: 22844
2023-10-04 17:21:20.215580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:21:20.215580: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:21:21.963462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:21:21.963458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:21:24.841890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:24.841890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:24.859274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:24.859274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:24.863712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:24.863964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:21:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22840)
[2023-10-04 17:21:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:22844)
[2023-10-04 17:21:27 +0000] [684] [ERROR] Worker (pid:22840) was sent code 134!
[2023-10-04 17:21:27 +0000] [23019] [INFO] Booting worker with pid: 23019
[2023-10-04 17:21:27 +0000] [684] [ERROR] Worker (pid:22844) was sent code 134!
[2023-10-04 17:21:27 +0000] [23020] [INFO] Booting worker with pid: 23020
2023-10-04 17:21:33.054425: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:21:33.095097: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:21:33.877746: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:21:33.915100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:21:34.736181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:34.739464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:34.741554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:34.770030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:34.773187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:34.775463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:36.643048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:36.643057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:36.647680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:36.647902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:36.652256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:36.652493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.864640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.867503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.870022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.872130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:21:39.884957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.886964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.888848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:21:39.890719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:21:41.017133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.018442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.019457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.020463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.021450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.022439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.023455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.024514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.025554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.026817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.027872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.028909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.029954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.031028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.032055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.033063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.034091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.035144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.036169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.037203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.038227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.039278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.040298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:21:41.041301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:22:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23019)
[2023-10-04 17:22:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23020)
[2023-10-04 17:24:07 +0000] [684] [ERROR] Worker (pid:23019) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:24:07 +0000] [23078] [INFO] Booting worker with pid: 23078
[2023-10-04 17:24:08 +0000] [684] [ERROR] Worker (pid:23020) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:24:08 +0000] [23079] [INFO] Booting worker with pid: 23079
2023-10-04 17:24:31.347301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:24:31.347301: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:24:33.120593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:24:33.120592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:24:36.042313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:36.042313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:36.058724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:36.058724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:36.063100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:36.063331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:24:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23078)
[2023-10-04 17:24:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23079)
[2023-10-04 17:24:38 +0000] [684] [ERROR] Worker (pid:23078) was sent code 134!
[2023-10-04 17:24:38 +0000] [23164] [INFO] Booting worker with pid: 23164
[2023-10-04 17:24:38 +0000] [684] [ERROR] Worker (pid:23079) was sent code 134!
[2023-10-04 17:24:38 +0000] [23165] [INFO] Booting worker with pid: 23165
2023-10-04 17:24:44.611937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:24:44.643103: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:24:45.437940: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:24:45.463004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:24:46.289241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:46.292678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:46.295019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:46.314396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:46.317481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:46.319665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:47.910304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:47.910406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:47.914837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:47.915101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:47.919369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:47.919612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.619043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.621262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.623411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.625848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:24:50.670022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.672120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.673757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:24:50.675363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:24:51.779853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.781137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.782104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.783109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.784069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.785045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.786018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.787099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.788130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.789147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.790158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.791235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.792247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.793254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.794413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.795487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.796500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.797526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.798569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.799646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.800657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.801659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.802708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:24:51.803705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:25:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23164)
[2023-10-04 17:26:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23165)
[2023-10-04 17:26:17 +0000] [684] [ERROR] Worker (pid:23164) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:26:17 +0000] [23215] [INFO] Booting worker with pid: 23215
[2023-10-04 17:26:17 +0000] [684] [ERROR] Worker (pid:23165) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:26:17 +0000] [23216] [INFO] Booting worker with pid: 23216
2023-10-04 17:26:41.761736: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:26:41.761736: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:26:43.633818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:26:43.633818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:26:46.678919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:46.678919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:46.695166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:46.695166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:46.699501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:46.699755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:26:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23215)
[2023-10-04 17:26:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23216)
[2023-10-04 17:26:48 +0000] [684] [ERROR] Worker (pid:23216) was sent code 134!
[2023-10-04 17:26:48 +0000] [23277] [INFO] Booting worker with pid: 23277
[2023-10-04 17:26:48 +0000] [684] [ERROR] Worker (pid:23215) was sent code 134!
[2023-10-04 17:26:48 +0000] [23278] [INFO] Booting worker with pid: 23278
2023-10-04 17:26:54.333365: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:26:54.336827: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:26:55.155835: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:26:55.157399: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:26:56.005453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:56.005897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:56.010862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:56.011147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:56.015028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:56.015705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:58.276349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:58.276391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:58.280846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:58.281046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:58.285239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:26:58.285464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:00.828406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:00.831230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:00.833727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:00.836078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:27:01.042174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:01.044911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:01.047458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:27:01.050043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:27:02.158369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.159708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.160751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.161779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.162886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.163957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.165012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.166081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.167163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.168840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.169940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.171327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.172733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.174426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.175858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.177202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.178557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.179972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.181358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.183069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.184436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.185800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.186869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.187885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:27:02.188928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:27:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23277)
[2023-10-04 17:28:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23278)
[2023-10-04 17:29:06 +0000] [684] [ERROR] Worker (pid:23277) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:29:06 +0000] [23358] [INFO] Booting worker with pid: 23358
[2023-10-04 17:29:06 +0000] [684] [ERROR] Worker (pid:23278) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:29:06 +0000] [23362] [INFO] Booting worker with pid: 23362
2023-10-04 17:29:29.906107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:29:29.906107: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:29:31.737819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:29:31.737819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:29:34.818836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:34.818836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:34.837793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:34.837814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:34.842092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:34.842345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:29:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23358)
[2023-10-04 17:29:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23362)
[2023-10-04 17:29:37 +0000] [684] [ERROR] Worker (pid:23358) was sent code 134!
[2023-10-04 17:29:37 +0000] [23431] [INFO] Booting worker with pid: 23431
[2023-10-04 17:29:37 +0000] [684] [ERROR] Worker (pid:23362) was sent code 134!
[2023-10-04 17:29:37 +0000] [23432] [INFO] Booting worker with pid: 23432
2023-10-04 17:29:43.375411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:29:43.391589: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:29:44.200442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:29:44.201592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:29:45.049167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:45.050475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:45.053496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:45.054708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:45.056810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:45.059279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:46.816280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:46.816336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:46.820752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:46.821021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:46.825241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:46.825463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.425492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.428303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.430873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.433382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:29:49.482107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.484801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.487306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:29:49.489040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:29:50.602859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.604136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.605130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.606120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.607121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.608171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.609153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.610189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.611306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.612314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.613320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.614351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.615425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.616426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.617438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.618455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.619503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.620518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.621600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.622645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.623667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.624684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.625710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:29:50.626785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:30:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23431)
[2023-10-04 17:32:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23432)
[2023-10-04 17:35:14 +0000] [684] [ERROR] Worker (pid:23431) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:35:14 +0000] [23491] [INFO] Booting worker with pid: 23491
[2023-10-04 17:35:15 +0000] [684] [ERROR] Worker (pid:23432) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:35:15 +0000] [23493] [INFO] Booting worker with pid: 23493
2023-10-04 17:35:38.823281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:35:38.823281: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:35:40.590934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:35:40.590935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:35:43.441810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:43.441810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:43.455417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:43.455417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:43.459766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:43.460023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:35:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23491)
[2023-10-04 17:35:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23493)
[2023-10-04 17:35:45 +0000] [684] [ERROR] Worker (pid:23491) was sent code 134!
[2023-10-04 17:35:45 +0000] [23637] [INFO] Booting worker with pid: 23637
[2023-10-04 17:35:45 +0000] [684] [ERROR] Worker (pid:23493) was sent code 134!
[2023-10-04 17:35:45 +0000] [23638] [INFO] Booting worker with pid: 23638
2023-10-04 17:35:51.623287: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:35:51.674734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:35:52.455131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:35:52.495955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:35:53.308159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:53.311510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:53.313561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:53.349761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:53.352919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:53.354932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:55.133230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:55.133275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:55.137764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:55.137971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:55.142236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:55.142434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.692221: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.695320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.698089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.700535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:35:57.793768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.795704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.797334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:35:57.799082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:35:58.890081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.891235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.892268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.893293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.894338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.895404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.896438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.897479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.898513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.899565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.900618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.901669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.902779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.903819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.904858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.905898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.906987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.908036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.909087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.910130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.911241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.912290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.913335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:35:58.914396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:36:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23638)
[2023-10-04 17:38:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23637)
[2023-10-04 17:39:56 +0000] [684] [ERROR] Worker (pid:23638) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:39:56 +0000] [23702] [INFO] Booting worker with pid: 23702
[2023-10-04 17:39:56 +0000] [684] [ERROR] Worker (pid:23637) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:39:56 +0000] [23763] [INFO] Booting worker with pid: 23763
2023-10-04 17:40:19.546226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:40:19.546226: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:40:21.343239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:40:21.343239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:40:24.493175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:24.493175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:24.506714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:24.506714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:24.511210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:24.511472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:40:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23702)
[2023-10-04 17:40:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23763)
[2023-10-04 17:40:27 +0000] [684] [ERROR] Worker (pid:23702) was sent code 134!
[2023-10-04 17:40:27 +0000] [23863] [INFO] Booting worker with pid: 23863
[2023-10-04 17:40:27 +0000] [684] [ERROR] Worker (pid:23763) was sent code 134!
[2023-10-04 17:40:27 +0000] [23864] [INFO] Booting worker with pid: 23864
2023-10-04 17:40:33.186006: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:40:33.229925: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:40:34.013802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:40:34.052576: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:40:34.871406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:34.875036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:34.877255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:34.916959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:34.920202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:34.922297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:36.457529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:36.457529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:36.462001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:36.462235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:36.466495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:36.466742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.089508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.092204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.094924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.097374: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:40:39.134458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.136397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.138121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:40:39.139835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:40:40.245497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.246842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.247832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.248834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.249864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.250958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.252001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.253035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.254072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.255163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.256188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.257239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.258289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.259401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.260445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.261471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.262494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.263543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.264545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.265539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.266547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.267616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.268624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:40:40.269662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:41:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23863)
[2023-10-04 17:43:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23864)
[2023-10-04 17:45:21 +0000] [684] [ERROR] Worker (pid:23863) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:45:21 +0000] [23919] [INFO] Booting worker with pid: 23919
[2023-10-04 17:45:21 +0000] [684] [ERROR] Worker (pid:23864) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:45:21 +0000] [23922] [INFO] Booting worker with pid: 23922
2023-10-04 17:45:45.181725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:45:45.181725: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:45:46.948296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:45:46.948296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:45:49.880360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:49.880360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:49.895158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:49.895158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:49.899640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:49.899853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:45:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23919)
[2023-10-04 17:45:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:23922)
[2023-10-04 17:45:51 +0000] [684] [ERROR] Worker (pid:23919) was sent code 134!
[2023-10-04 17:45:51 +0000] [24062] [INFO] Booting worker with pid: 24062
[2023-10-04 17:45:52 +0000] [684] [ERROR] Worker (pid:23922) was sent code 134!
[2023-10-04 17:45:52 +0000] [24063] [INFO] Booting worker with pid: 24063
2023-10-04 17:45:57.871264: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:45:57.963851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:45:58.687774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:45:58.791863: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:45:59.526448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:59.529860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:59.531947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:59.646852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:59.650006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:45:59.652201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:01.505482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:01.505574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:01.509946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:01.510154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:01.514362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:01.514622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.303253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.305214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.307186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.308925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:46:04.361133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.363243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.364861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:46:04.366617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:46:05.488073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.489392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.490393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.491417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.492394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.493368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.494371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.495402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.496372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.497342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.498312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.499325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.500310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.501339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.502380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.503447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.504466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.505469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.506489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.507533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.508547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.509555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.510561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:46:05.511594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:46:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24062)
[2023-10-04 17:48:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24063)
[2023-10-04 17:48:57 +0000] [684] [ERROR] Worker (pid:24063) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:48:57 +0000] [24169] [INFO] Booting worker with pid: 24169
[2023-10-04 17:48:57 +0000] [684] [ERROR] Worker (pid:24062) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:48:57 +0000] [24170] [INFO] Booting worker with pid: 24170
2023-10-04 17:49:20.300350: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:49:20.300346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:49:22.382456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:49:22.382459: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:49:25.242008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:25.242008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:25.255514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:25.255514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:25.259876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:25.260129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:27.934543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:27.934543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:27.938997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:27.939271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:27.943486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:27.943734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:49:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24169)
[2023-10-04 17:49:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24170)
[2023-10-04 17:49:28 +0000] [684] [ERROR] Worker (pid:24170) was sent code 134!
[2023-10-04 17:49:28 +0000] [24261] [INFO] Booting worker with pid: 24261
[2023-10-04 17:49:28 +0000] [684] [ERROR] Worker (pid:24169) was sent code 134!
[2023-10-04 17:49:28 +0000] [24262] [INFO] Booting worker with pid: 24262
2023-10-04 17:49:34.330372: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:49:34.421511: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:49:35.152786: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:49:35.240485: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:49:35.998446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:36.001736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:36.003974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:36.099187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:36.102439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:36.104641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:37.352406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:37.354663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:37.356752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:37.446039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:37.450368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:37.455290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:39.963663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:39.966485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:39.968677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:39.971170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:49:40.070991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:40.073075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:40.075113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:49:40.077039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:49:41.206460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.207607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.209672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.211196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.213081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.214550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.216475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.218367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.219903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.221372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.223297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.224819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.226798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.228948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.231150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.233281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.234636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.236116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.237581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.239047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.240917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.242463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.243546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:49:41.244568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:50:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24261)
[2023-10-04 17:52:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24262)
[2023-10-04 17:54:35 +0000] [684] [ERROR] Worker (pid:24261) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:54:35 +0000] [24317] [INFO] Booting worker with pid: 24317
[2023-10-04 17:54:35 +0000] [684] [ERROR] Worker (pid:24262) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:54:35 +0000] [24318] [INFO] Booting worker with pid: 24318
2023-10-04 17:54:59.094751: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:54:59.094751: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:55:00.824923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:55:00.824927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:55:03.739802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:03.739802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:03.754385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:03.754401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:03.758885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:03.759142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:55:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24317)
[2023-10-04 17:55:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24318)
[2023-10-04 17:55:05 +0000] [684] [ERROR] Worker (pid:24318) was sent code 134!
[2023-10-04 17:55:05 +0000] [24461] [INFO] Booting worker with pid: 24461
[2023-10-04 17:55:05 +0000] [684] [ERROR] Worker (pid:24317) was sent code 134!
[2023-10-04 17:55:05 +0000] [24462] [INFO] Booting worker with pid: 24462
2023-10-04 17:55:11.924622: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:55:11.972679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:55:12.741391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:55:12.789772: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:55:13.593422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:13.596781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:13.598823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:13.649168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:13.652567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:13.654699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:15.579719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:15.579735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:15.584232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:15.584448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:15.588760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:15.589006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.254384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.256737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.258563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.260288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:55:18.327034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.329702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.332242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:55:18.334728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:55:19.503802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.505756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.507529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.509379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.511341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.513299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.515288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.517247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.519171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.520805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.522741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.524268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.526324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.527844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.529691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.531197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.532674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.534674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.536141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.538006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.539986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.541460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.542541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:55:19.543665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 17:55:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24461)
[2023-10-04 17:57:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24462)
[2023-10-04 17:59:13 +0000] [684] [ERROR] Worker (pid:24461) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:59:13 +0000] [24552] [INFO] Booting worker with pid: 24552
[2023-10-04 17:59:14 +0000] [684] [ERROR] Worker (pid:24462) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 17:59:14 +0000] [24553] [INFO] Booting worker with pid: 24553
2023-10-04 17:59:37.376015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:59:37.376015: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:59:39.299490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:59:39.299490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:59:42.341154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:42.341154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:42.355824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:42.355824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:42.360182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:42.360431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 17:59:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24552)
[2023-10-04 17:59:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24553)
[2023-10-04 17:59:44 +0000] [684] [ERROR] Worker (pid:24553) was sent code 134!
[2023-10-04 17:59:44 +0000] [24623] [INFO] Booting worker with pid: 24623
[2023-10-04 17:59:44 +0000] [684] [ERROR] Worker (pid:24552) was sent code 134!
[2023-10-04 17:59:44 +0000] [24624] [INFO] Booting worker with pid: 24624
2023-10-04 17:59:50.562839: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:59:50.595501: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 17:59:51.388494: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:59:51.415483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 17:59:52.248027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:52.251542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:52.253490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:52.271093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:52.274307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:52.276433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:54.099393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:54.099543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:54.103861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:54.104101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:54.108298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:54.108554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.904561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.907275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.909761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.912344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:59:56.913224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.915761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.918237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 17:59:56.920765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 17:59:58.063613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.064863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.066036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.067142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.068464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.070106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.071753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.073309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.074988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.076605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.078193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.079447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.080623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.081772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.082909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.083976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.085487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.087042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.088120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.089190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.090458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.092198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.093260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 17:59:58.094323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:00:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24624)
[2023-10-04 18:01:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24623)
[2023-10-04 18:02:47 +0000] [684] [ERROR] Worker (pid:24624) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:02:47 +0000] [24679] [INFO] Booting worker with pid: 24679
[2023-10-04 18:02:47 +0000] [684] [ERROR] Worker (pid:24623) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:02:47 +0000] [24680] [INFO] Booting worker with pid: 24680
2023-10-04 18:03:10.764548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:03:10.764548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:03:12.725938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:03:12.725937: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:03:15.679332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:15.679332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:15.694024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:15.694024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:15.698427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:15.698696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:03:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24679)
[2023-10-04 18:03:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24680)
[2023-10-04 18:03:18 +0000] [684] [ERROR] Worker (pid:24680) was sent code 134!
[2023-10-04 18:03:18 +0000] [24779] [INFO] Booting worker with pid: 24779
[2023-10-04 18:03:18 +0000] [684] [ERROR] Worker (pid:24679) was sent code 134!
[2023-10-04 18:03:18 +0000] [24780] [INFO] Booting worker with pid: 24780
2023-10-04 18:03:24.225230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:03:24.243124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:03:25.050865: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:03:25.051649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:03:25.892001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:25.894496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:25.895235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:25.898935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:25.899048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:25.903476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:27.688662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:27.688662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:27.693060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:27.693308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:27.697509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:27.697754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.491981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.491981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.497131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.497386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.502016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.502301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:03:30.506958: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:03:30.507218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:03:31.639324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.640610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.641619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.642681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.643682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.644693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.645889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.647521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.648951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.650282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.651844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.652980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.654428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.655574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.657093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.658701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.660082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.661297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.662383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.663587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.664812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.665929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.667312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:03:31.668678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:04:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24779)
[2023-10-04 18:05:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24780)
[2023-10-04 18:08:21 +0000] [684] [ERROR] Worker (pid:24779) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:08:21 +0000] [24835] [INFO] Booting worker with pid: 24835
[2023-10-04 18:08:22 +0000] [684] [ERROR] Worker (pid:24780) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:08:22 +0000] [24837] [INFO] Booting worker with pid: 24837
2023-10-04 18:08:45.998900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:08:45.998900: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:08:47.799712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:08:47.799718: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:08:50.656095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:08:50.656095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:08:50.673577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:08:50.673577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:08:50.678162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:08:50.678417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:08:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24835)
[2023-10-04 18:08:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24837)
[2023-10-04 18:08:52 +0000] [684] [ERROR] Worker (pid:24835) was sent code 134!
[2023-10-04 18:08:52 +0000] [24981] [INFO] Booting worker with pid: 24981
[2023-10-04 18:08:52 +0000] [684] [ERROR] Worker (pid:24837) was sent code 134!
[2023-10-04 18:08:52 +0000] [24982] [INFO] Booting worker with pid: 24982
2023-10-04 18:08:58.640681: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:08:58.735611: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:08:59.465171: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:08:59.553449: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:09:00.304732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:00.308219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:00.310300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:00.402126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:00.405278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:00.407366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:02.276530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:02.276596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:02.280936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:02.281177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:02.285363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:02.285610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.003228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.005938: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.008360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.010701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:09:05.087835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.090468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.093231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:09:05.095694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:09:06.224362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.225431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.226485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.227543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.229648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.231047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.232139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.233429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.234785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.236301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.237790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.239081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.240393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.241874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.243232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.244540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.245586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.246887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.248180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.249439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.250491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.252036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.253192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:09:06.254245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:09:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24981)
[2023-10-04 18:11:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:24982)
[2023-10-04 18:14:28 +0000] [684] [ERROR] Worker (pid:24981) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:14:28 +0000] [25066] [INFO] Booting worker with pid: 25066
[2023-10-04 18:14:29 +0000] [684] [ERROR] Worker (pid:24982) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:14:29 +0000] [25075] [INFO] Booting worker with pid: 25075
2023-10-04 18:14:52.648312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:14:52.648312: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:14:54.409743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:14:54.409743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:14:57.395585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:14:57.395585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:14:57.410584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:14:57.410655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:14:57.414950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:14:57.415207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:14:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25066)
[2023-10-04 18:14:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25075)
[2023-10-04 18:14:59 +0000] [684] [ERROR] Worker (pid:25066) was sent code 134!
[2023-10-04 18:14:59 +0000] [25226] [INFO] Booting worker with pid: 25226
[2023-10-04 18:14:59 +0000] [684] [ERROR] Worker (pid:25075) was sent code 134!
[2023-10-04 18:14:59 +0000] [25227] [INFO] Booting worker with pid: 25227
2023-10-04 18:15:05.394403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:15:05.416448: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:15:06.220567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:15:06.233945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:15:07.083182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:07.086070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:07.086396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:07.090165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:07.091817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:07.094390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:09.119224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:09.119403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:09.123646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:09.123883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:09.128095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:09.128344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.851744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.853779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.855767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.857609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:15:11.951189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.953105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.954924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:15:11.956549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:15:13.040855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.042078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.043157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.044171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.045218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.046248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.047323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.048364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.049386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.050413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.051469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.052481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.053511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.054532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.055577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.056990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.058006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.059406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.061155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.062579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.064004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.065408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.066472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:15:13.067527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:16:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25226)
[2023-10-04 18:18:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25227)
[2023-10-04 18:20:24 +0000] [684] [ERROR] Worker (pid:25226) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:20:24 +0000] [25311] [INFO] Booting worker with pid: 25311
[2023-10-04 18:20:24 +0000] [684] [ERROR] Worker (pid:25227) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:20:24 +0000] [25317] [INFO] Booting worker with pid: 25317
2023-10-04 18:20:48.378550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:20:48.378550: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:20:50.231040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:20:50.231040: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:20:53.220089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:20:53.220089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:20:53.236041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:20:53.236041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:20:53.240533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:20:53.240763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:20:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25311)
[2023-10-04 18:20:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25317)
[2023-10-04 18:20:55 +0000] [684] [ERROR] Worker (pid:25317) was sent code 134!
[2023-10-04 18:20:55 +0000] [25473] [INFO] Booting worker with pid: 25473
[2023-10-04 18:20:55 +0000] [684] [ERROR] Worker (pid:25311) was sent code 134!
[2023-10-04 18:20:55 +0000] [25474] [INFO] Booting worker with pid: 25474
2023-10-04 18:21:01.161414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:21:01.197576: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:21:01.982224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:21:02.027579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:21:02.847138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:02.850679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:02.852801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:02.879162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:02.882380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:02.884400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:04.920098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:04.920098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:04.924510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:04.924774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:04.928969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:04.929236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.697952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.700000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.701633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.703611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:21:07.773652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.775833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.777677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:21:07.779780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:21:08.888074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.889153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.890239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.891396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.892474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.893526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.894579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.895638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.896686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.897723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.899431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.900527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.902123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.904254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.905355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.906965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.908358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.909535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.911201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.912265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.913335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.914415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.915551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:21:08.916611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:21:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25473)
[2023-10-04 18:23:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25474)
[2023-10-04 18:25:46 +0000] [684] [ERROR] Worker (pid:25473) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:25:46 +0000] [25556] [INFO] Booting worker with pid: 25556
[2023-10-04 18:25:46 +0000] [684] [ERROR] Worker (pid:25474) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:25:46 +0000] [25565] [INFO] Booting worker with pid: 25565
2023-10-04 18:26:10.178782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:26:10.178782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:26:11.934528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:26:11.934528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:26:14.859673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:14.859673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:14.876047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:14.876047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:14.880443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:14.880693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:26:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25556)
[2023-10-04 18:26:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25565)
[2023-10-04 18:26:16 +0000] [684] [ERROR] Worker (pid:25556) was sent code 134!
[2023-10-04 18:26:16 +0000] [25737] [INFO] Booting worker with pid: 25737
[2023-10-04 18:26:17 +0000] [684] [ERROR] Worker (pid:25565) was sent code 134!
[2023-10-04 18:26:17 +0000] [25738] [INFO] Booting worker with pid: 25738
2023-10-04 18:26:23.029865: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:26:23.141051: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:26:23.852303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:26:23.972541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:26:24.698105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:24.701408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:24.703460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:24.834014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:24.837283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:24.839518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:26.607908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:26.607923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:26.612319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:26.612572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:26.616774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:26.617029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.483755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.486306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.488304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.490361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:26:29.528373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.530390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.532080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:26:29.533727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:26:30.641819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.642970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.643974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.644976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.646045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.647139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.648155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.649170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.650215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.651303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.652338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.653366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.654428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.655493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.656547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.657568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.658642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.659674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.660686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.661711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.662775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.663787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.664806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:26:30.665854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:27:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25737)
[2023-10-04 18:28:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25738)
[2023-10-04 18:31:12 +0000] [684] [ERROR] Worker (pid:25737) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:31:13 +0000] [25798] [INFO] Booting worker with pid: 25798
[2023-10-04 18:31:13 +0000] [684] [ERROR] Worker (pid:25738) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:31:13 +0000] [25799] [INFO] Booting worker with pid: 25799
2023-10-04 18:31:37.312184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:31:37.312184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:31:39.218941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:31:39.218941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:31:42.222519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:42.222519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:42.241389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:42.241392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:42.245736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:42.245987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:31:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25798)
[2023-10-04 18:31:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25799)
[2023-10-04 18:31:43 +0000] [684] [ERROR] Worker (pid:25799) was sent code 134!
[2023-10-04 18:31:43 +0000] [25947] [INFO] Booting worker with pid: 25947
[2023-10-04 18:31:43 +0000] [684] [ERROR] Worker (pid:25798) was sent code 134!
[2023-10-04 18:31:43 +0000] [25948] [INFO] Booting worker with pid: 25948
2023-10-04 18:31:49.682249: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:31:49.711109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:31:50.504462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:31:50.537319: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:31:51.353680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:51.356909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:51.358890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:51.386082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:51.389487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:51.391793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:53.723289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:53.723436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:53.727718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:53.727967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:53.732187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:53.732425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.559858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.562647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.564699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.567015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:31:56.574397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.576255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.577843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:31:56.579530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:31:57.713759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.715380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.717011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.718151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.719271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.720561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.722083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.723257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.724583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.725744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.726935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.728013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.729176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.730327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.731493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.732599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.733667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.734837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.735920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.737016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.738155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.739773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.740855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:31:57.741992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:32:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25947)
[2023-10-04 18:34:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:25948)
[2023-10-04 18:36:41 +0000] [684] [ERROR] Worker (pid:25947) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:36:41 +0000] [26005] [INFO] Booting worker with pid: 26005
[2023-10-04 18:36:41 +0000] [684] [ERROR] Worker (pid:25948) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:36:41 +0000] [26007] [INFO] Booting worker with pid: 26007
2023-10-04 18:37:05.535901: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:37:05.535902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:37:07.347923: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:37:07.347927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:37:10.442143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:10.442143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:10.459249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:10.459249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:10.463673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:10.463904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:37:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26005)
[2023-10-04 18:37:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26007)
[2023-10-04 18:37:12 +0000] [684] [ERROR] Worker (pid:26005) was sent code 134!
[2023-10-04 18:37:12 +0000] [26149] [INFO] Booting worker with pid: 26149
[2023-10-04 18:37:12 +0000] [684] [ERROR] Worker (pid:26007) was sent code 134!
[2023-10-04 18:37:12 +0000] [26150] [INFO] Booting worker with pid: 26150
2023-10-04 18:37:18.387413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:37:18.505252: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:37:19.216589: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:37:19.355290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:37:20.062236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:20.065596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:20.067953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:20.218689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:20.221479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:20.223211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:22.329413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:22.329521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:22.333863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:22.334121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:22.338302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:22.338567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.056997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.059388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.061824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.064045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:37:25.075698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.077840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.079798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:37:25.081660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:37:26.208784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.210137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.211354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.212451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.213850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.215490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.217095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.218750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.220369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.221916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.223612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.224976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.226095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.227261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.228430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.229559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.230640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.231733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.232781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.233850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.234966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.236047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.237093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:37:26.238149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:38:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26149)
[2023-10-04 18:39:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26150)
[2023-10-04 18:41:20 +0000] [684] [ERROR] Worker (pid:26149) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:41:20 +0000] [26241] [INFO] Booting worker with pid: 26241
[2023-10-04 18:41:20 +0000] [684] [ERROR] Worker (pid:26150) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:41:20 +0000] [26243] [INFO] Booting worker with pid: 26243
2023-10-04 18:41:44.146674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:41:44.146674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:41:45.947143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:41:45.947143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:41:49.005224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:49.005224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:49.020747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:49.020766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:49.025184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:49.025434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:41:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26241)
[2023-10-04 18:41:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26243)
[2023-10-04 18:41:51 +0000] [684] [ERROR] Worker (pid:26243) was sent code 134!
[2023-10-04 18:41:51 +0000] [26313] [INFO] Booting worker with pid: 26313
[2023-10-04 18:41:51 +0000] [684] [ERROR] Worker (pid:26241) was sent code 134!
[2023-10-04 18:41:51 +0000] [26314] [INFO] Booting worker with pid: 26314
2023-10-04 18:41:56.993482: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:41:57.037237: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:41:57.817533: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:41:57.849546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:41:58.685891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:58.689120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:58.691332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:58.696305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:58.699427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:41:58.701342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:00.748396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:00.748548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:00.752858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:00.753080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:00.757427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:00.757621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.534205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.537055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.539586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.542037: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:42:03.704591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.707359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.709896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:42:03.712412: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:42:04.956573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.958367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.960368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.962291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.964219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.966828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.968796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.970133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.972333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.973588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.974968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.976172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.977881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.979038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.980483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.981991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.983231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.984490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.986061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.988183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.989515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.990609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.991767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.992928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:42:04.994378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:42:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26313)
[2023-10-04 18:44:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26314)
[2023-10-04 18:46:50 +0000] [684] [ERROR] Worker (pid:26313) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:46:50 +0000] [26397] [INFO] Booting worker with pid: 26397
[2023-10-04 18:46:50 +0000] [684] [ERROR] Worker (pid:26314) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:46:50 +0000] [26399] [INFO] Booting worker with pid: 26399
2023-10-04 18:47:14.554643: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:47:14.554643: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:47:16.427623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:47:16.427623: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:47:19.735006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:19.735006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:19.753800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:19.753800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:19.758482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:19.758710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:47:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26397)
[2023-10-04 18:47:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26399)
[2023-10-04 18:47:20 +0000] [684] [ERROR] Worker (pid:26397) was sent code 134!
[2023-10-04 18:47:20 +0000] [26570] [INFO] Booting worker with pid: 26570
[2023-10-04 18:47:21 +0000] [684] [ERROR] Worker (pid:26399) was sent code 134!
[2023-10-04 18:47:21 +0000] [26571] [INFO] Booting worker with pid: 26571
2023-10-04 18:47:26.956318: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:47:26.982390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:47:27.775362: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:47:27.809502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:47:28.617929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:28.621302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:28.623500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:28.666796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:28.670057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:28.672135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:30.854721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:30.854721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:30.859219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:30.859438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:30.863657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:30.863887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.503297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.506231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.508653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.511138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:47:33.566322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.568316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.569969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:47:33.571863: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:47:34.687317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.688435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.689479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.690528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.691570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.692594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.693619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.694689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.695725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.697240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.698307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.699390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.700437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.701810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.703254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.704667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.705682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.707442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.708817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.709830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.711642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.713025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.714037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:47:34.715090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:48:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26570)
[2023-10-04 18:50:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26571)
[2023-10-04 18:51:25 +0000] [684] [ERROR] Worker (pid:26570) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:51:25 +0000] [26629] [INFO] Booting worker with pid: 26629
[2023-10-04 18:51:25 +0000] [684] [ERROR] Worker (pid:26571) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:51:25 +0000] [26632] [INFO] Booting worker with pid: 26632
2023-10-04 18:51:49.066230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:51:49.066230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:51:50.821502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:51:50.821502: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:51:53.702288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:51:53.702288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:51:53.717606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:51:53.717606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:51:53.722132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:51:53.722383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:51:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26629)
[2023-10-04 18:51:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26632)
[2023-10-04 18:51:56 +0000] [684] [ERROR] Worker (pid:26629) was sent code 134!
[2023-10-04 18:51:56 +0000] [26705] [INFO] Booting worker with pid: 26705
[2023-10-04 18:51:56 +0000] [684] [ERROR] Worker (pid:26632) was sent code 134!
[2023-10-04 18:51:56 +0000] [26706] [INFO] Booting worker with pid: 26706
2023-10-04 18:52:02.273353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:52:02.348371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:52:03.092289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:52:03.170272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:52:03.930697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:03.934068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:03.936350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:04.023117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:04.026453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:04.028498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:05.598745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:05.598742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:05.603332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:05.603552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:05.607888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:05.608135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.283348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.286025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.288035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.289846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:52:08.350955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.352862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.354709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:52:08.356290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:52:09.449824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.451009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.452025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.453030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.454053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.455121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.456148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.457159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.458176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.459243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.460251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.461292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.462321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.463396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.464438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.465455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.466496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.467544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.468552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.469557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.470575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.471591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.472593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:52:09.473613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:52:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26705)
[2023-10-04 18:54:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26706)
[2023-10-04 18:56:08 +0000] [684] [ERROR] Worker (pid:26705) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:56:08 +0000] [26760] [INFO] Booting worker with pid: 26760
[2023-10-04 18:56:08 +0000] [684] [ERROR] Worker (pid:26706) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 18:56:08 +0000] [26773] [INFO] Booting worker with pid: 26773
2023-10-04 18:56:31.999624: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:56:31.999624: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:56:33.759638: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:56:33.759642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:56:36.730928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:36.730928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:36.747219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:36.747251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:36.751644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:36.751881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 18:56:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26760)
[2023-10-04 18:56:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26773)
[2023-10-04 18:56:39 +0000] [684] [ERROR] Worker (pid:26773) was sent code 134!
[2023-10-04 18:56:39 +0000] [26860] [INFO] Booting worker with pid: 26860
[2023-10-04 18:56:39 +0000] [684] [ERROR] Worker (pid:26760) was sent code 134!
[2023-10-04 18:56:39 +0000] [26861] [INFO] Booting worker with pid: 26861
2023-10-04 18:56:45.145352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:56:45.199279: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 18:56:45.975324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:56:46.016020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 18:56:46.821819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:46.825110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:46.827191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:46.854247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:46.857417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:46.859696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:48.551142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:48.551262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:48.555693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:48.555952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:48.560144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:48.560403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.280051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.282469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.284377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.285894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:56:51.414658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.416393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.418015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 18:56:51.419556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 18:56:52.617157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.619156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.621400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.623001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.625008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.627091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.629819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.631975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.634622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.636760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.639022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.640746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.642438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.644096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.645749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.647471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.649162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.650891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.652485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.654444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.656431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.658097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.659900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 18:56:52.661687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 18:57:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26860)
[2023-10-04 18:59:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26861)
[2023-10-04 19:01:15 +0000] [684] [ERROR] Worker (pid:26860) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:01:15 +0000] [26918] [INFO] Booting worker with pid: 26918
[2023-10-04 19:01:15 +0000] [684] [ERROR] Worker (pid:26861) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:01:15 +0000] [26919] [INFO] Booting worker with pid: 26919
2023-10-04 19:01:39.245674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:01:39.245674: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:01:41.140813: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:01:41.140817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:01:44.252454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:44.252454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:44.267256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:44.267256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:44.271587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:44.271838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:01:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26918)
[2023-10-04 19:01:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26919)
[2023-10-04 19:01:46 +0000] [684] [ERROR] Worker (pid:26919) was sent code 134!
[2023-10-04 19:01:46 +0000] [26990] [INFO] Booting worker with pid: 26990
[2023-10-04 19:01:46 +0000] [684] [ERROR] Worker (pid:26918) was sent code 134!
[2023-10-04 19:01:46 +0000] [26991] [INFO] Booting worker with pid: 26991
2023-10-04 19:01:52.303573: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:01:52.381142: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:01:53.124318: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:01:53.247558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:01:53.975107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:53.978398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:53.980551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:54.107730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:54.110859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:54.112763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:56.034056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:56.034254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:56.038486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:56.038728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:56.042944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:56.043197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.837305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.839607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.841398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.843195: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:01:58.908768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.910753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.912569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:01:58.914376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:02:00.058827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.060004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.061053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.062076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.063153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.064675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.066190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.067924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.068966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.070010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.071447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.072469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.073508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.074538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.075933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.077295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.078712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.080083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.081469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.082892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.084260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.085964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.087037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:02:00.088062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:02:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26990)
[2023-10-04 19:04:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:26991)
[2023-10-04 19:04:47 +0000] [684] [ERROR] Worker (pid:26991) was sent code 134!
[2023-10-04 19:04:47 +0000] [27095] [INFO] Booting worker with pid: 27095
[2023-10-04 19:04:47 +0000] [684] [ERROR] Worker (pid:26990) was sent code 134!
[2023-10-04 19:04:47 +0000] [27096] [INFO] Booting worker with pid: 27096
2023-10-04 19:05:10.222574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:05:10.222574: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:05:12.128358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:05:12.128358: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:05:15.227657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:15.227657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:15.243331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:15.243331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:15.247659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:15.247923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:05:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27095)
[2023-10-04 19:05:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27096)
[2023-10-04 19:05:18 +0000] [684] [ERROR] Worker (pid:27095) was sent code 134!
[2023-10-04 19:05:18 +0000] [27175] [INFO] Booting worker with pid: 27175
[2023-10-04 19:05:18 +0000] [684] [ERROR] Worker (pid:27096) was sent code 134!
[2023-10-04 19:05:18 +0000] [27176] [INFO] Booting worker with pid: 27176
2023-10-04 19:05:24.197786: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:05:24.205135: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:05:25.017858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:05:25.019660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:05:25.865218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:25.865586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:25.870569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:25.870952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:25.874911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:25.875607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:27.338012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:27.338044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:27.342431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:27.342685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:27.346883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:27.347120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.200906: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.204834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.207278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.209738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:05:30.300085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.302105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.304092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:05:30.305826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:05:31.409052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.410446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.412003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.413597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.415646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.416833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.418880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.420844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.422032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.424101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.425705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.427768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.428973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.431023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.433078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.435083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.437091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.438291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.440356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.441536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.443586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.444769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.445934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:05:31.447151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:05:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27175)
[2023-10-04 19:06:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27176)
[2023-10-04 19:07:48 +0000] [684] [ERROR] Worker (pid:27175) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:07:48 +0000] [27227] [INFO] Booting worker with pid: 27227
[2023-10-04 19:07:48 +0000] [684] [ERROR] Worker (pid:27176) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:07:48 +0000] [27228] [INFO] Booting worker with pid: 27228
2023-10-04 19:08:11.732676: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:08:11.732676: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:08:13.552432: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:08:13.552431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:08:16.722262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:16.722262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:16.736893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:16.736893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:16.741268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:16.741519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:08:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27227)
[2023-10-04 19:08:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27228)
[2023-10-04 19:08:18 +0000] [684] [ERROR] Worker (pid:27228) was sent code 134!
[2023-10-04 19:08:18 +0000] [27329] [INFO] Booting worker with pid: 27329
[2023-10-04 19:08:19 +0000] [684] [ERROR] Worker (pid:27227) was sent code 134!
[2023-10-04 19:08:19 +0000] [27330] [INFO] Booting worker with pid: 27330
2023-10-04 19:08:25.010945: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:08:25.057788: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:08:25.842563: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:08:25.890237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:08:26.694010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:26.697490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:26.699650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:26.744566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:26.747671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:26.749738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:28.567714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:28.567710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:28.572141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:28.572390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:28.576691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:28.576918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.223318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.226866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.228649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.230877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19010 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:08:31.377813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.379921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.381795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:08:31.383710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:08:32.578889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.580442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.581471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.582829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.584196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.585230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.586604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.588245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.589553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.590954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.592941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.594001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.595705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.596717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.597739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.598795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.599814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.600835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.602054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.603352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.604373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.605394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.606413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.607439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:08:32.608436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:09:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27329)
[2023-10-04 19:11:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27330)
[2023-10-04 19:13:56 +0000] [684] [ERROR] Worker (pid:27329) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:13:56 +0000] [27387] [INFO] Booting worker with pid: 27387
[2023-10-04 19:13:57 +0000] [684] [ERROR] Worker (pid:27330) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:13:57 +0000] [27389] [INFO] Booting worker with pid: 27389
2023-10-04 19:14:20.754696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:14:20.754696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:14:22.525947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:14:22.525947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:14:25.563454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:25.563454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:25.577403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:25.577403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:25.581857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:25.582077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:14:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27387)
[2023-10-04 19:14:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27389)
[2023-10-04 19:14:27 +0000] [684] [ERROR] Worker (pid:27389) was sent code 134!
[2023-10-04 19:14:27 +0000] [27566] [INFO] Booting worker with pid: 27566
[2023-10-04 19:14:27 +0000] [684] [ERROR] Worker (pid:27387) was sent code 134!
[2023-10-04 19:14:27 +0000] [27567] [INFO] Booting worker with pid: 27567
2023-10-04 19:14:33.375130: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:14:33.411524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:14:34.193253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:14:34.229404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:14:35.042044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:35.045555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:35.047712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:35.083136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:35.086382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:35.088492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:37.174853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:37.174853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:37.179255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:37.179512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:37.183674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:37.183925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.986638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.989049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.989811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.994036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.994803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.999097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:14:39.999804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:14:40.001544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:14:41.148911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.150214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.151288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.152314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.153334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.154402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.155450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.156469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.157479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.158543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.159605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.160637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.161684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.162775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.163827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.164874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.165915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.167007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.168068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.169099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.170121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.171200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.172249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:14:41.173298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:15:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27566)
[2023-10-04 19:16:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27567)
[2023-10-04 19:18:42 +0000] [684] [ERROR] Worker (pid:27566) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:18:42 +0000] [27624] [INFO] Booting worker with pid: 27624
[2023-10-04 19:18:42 +0000] [684] [ERROR] Worker (pid:27567) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:18:42 +0000] [27627] [INFO] Booting worker with pid: 27627
2023-10-04 19:19:05.789519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:19:05.789519: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:19:07.587773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:19:07.587780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:19:10.567063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:10.567064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:10.583356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:10.583356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:10.587768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:10.588009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:19:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27624)
[2023-10-04 19:19:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27627)
[2023-10-04 19:19:12 +0000] [684] [ERROR] Worker (pid:27627) was sent code 134!
[2023-10-04 19:19:12 +0000] [27699] [INFO] Booting worker with pid: 27699
[2023-10-04 19:19:12 +0000] [684] [ERROR] Worker (pid:27624) was sent code 134!
[2023-10-04 19:19:12 +0000] [27700] [INFO] Booting worker with pid: 27700
2023-10-04 19:19:18.836236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:19:18.976869: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:19:19.659302: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:19:19.802456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:19:20.501758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:20.505180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:20.507550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:20.646367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:20.649619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:20.651719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:22.346432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:22.346433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:22.350861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:22.351114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:22.355330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:22.355578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.132527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.135306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.137571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.139596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:19:25.145716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.147741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.149444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:19:25.151246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:19:26.285225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.286546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.287557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.288528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.289501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.290498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.291773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.293120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.294540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.295977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.297497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.299131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.300574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.302168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.303643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.304808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.306185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.307258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.308566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.309694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.311296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.312719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.314008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:19:26.315596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:20:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27700)
[2023-10-04 19:22:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27699)
[2023-10-04 19:24:51 +0000] [684] [ERROR] Worker (pid:27700) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:24:51 +0000] [27785] [INFO] Booting worker with pid: 27785
[2023-10-04 19:24:52 +0000] [684] [ERROR] Worker (pid:27699) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:24:52 +0000] [27791] [INFO] Booting worker with pid: 27791
2023-10-04 19:25:15.835072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:25:15.835072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:25:17.637685: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:25:17.637680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:25:20.752561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:20.752561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:20.817014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:20.817025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:20.821469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:20.821715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:25:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27785)
[2023-10-04 19:25:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27791)
[2023-10-04 19:25:22 +0000] [684] [ERROR] Worker (pid:27791) was sent code 134!
[2023-10-04 19:25:22 +0000] [27959] [INFO] Booting worker with pid: 27959
[2023-10-04 19:25:22 +0000] [684] [ERROR] Worker (pid:27785) was sent code 134!
[2023-10-04 19:25:22 +0000] [27960] [INFO] Booting worker with pid: 27960
2023-10-04 19:25:28.648393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:25:28.658322: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:25:29.474042: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:25:29.476978: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:25:30.318645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:30.318655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:30.324053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:30.324177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:30.328367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:30.328618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:32.770379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:32.770399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:32.774806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:32.775069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:32.779286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:32.779522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.463531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.466295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.468789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.470806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:25:35.544230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.546293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.548564: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:25:35.550959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:25:36.455267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.456406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.457466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.458529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.459611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.460671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.461729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.462847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.463959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.465227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.466321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.467412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.468650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.469707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.471366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.472423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.473812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.475099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.476727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.477778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.479281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.480521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.481591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:25:36.482699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:26:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27959)
[2023-10-04 19:27:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:27960)
[2023-10-04 19:27:54 +0000] [684] [ERROR] Worker (pid:27959) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:27:54 +0000] [28017] [INFO] Booting worker with pid: 28017
[2023-10-04 19:27:55 +0000] [684] [ERROR] Worker (pid:27960) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:27:55 +0000] [28018] [INFO] Booting worker with pid: 28018
2023-10-04 19:28:18.465715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:28:18.465715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:28:20.186367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:28:20.186366: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:28:23.071830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:23.071831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:23.086073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:23.086106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:23.090482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:23.090734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:28:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28017)
[2023-10-04 19:28:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28018)
[2023-10-04 19:28:25 +0000] [684] [ERROR] Worker (pid:28018) was sent code 134!
[2023-10-04 19:28:25 +0000] [28117] [INFO] Booting worker with pid: 28117
[2023-10-04 19:28:25 +0000] [684] [ERROR] Worker (pid:28017) was sent code 134!
[2023-10-04 19:28:25 +0000] [28118] [INFO] Booting worker with pid: 28118
2023-10-04 19:28:31.685627: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:28:31.756973: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:28:32.507035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:28:32.576139: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:28:33.354350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:33.357651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:33.359968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:33.423832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:33.426970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:33.429122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:35.279554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:35.279575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:35.283978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:35.284204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:35.288380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:35.288594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:37.943071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:37.946392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:37.949112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:37.951724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:28:38.067069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:38.069753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:38.072226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:28:38.074409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:28:39.016700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.018853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.019943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.021412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.022955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.024470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.026001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.027678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.029197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.031159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.032664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.034579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.036571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.038032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.039943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.041411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.043306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.044811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.046334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.047834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.049892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.051434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.052530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:28:39.053687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:29:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28118)
[2023-10-04 19:30:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28117)
[2023-10-04 19:30:12 +0000] [684] [ERROR] Worker (pid:28118) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:30:12 +0000] [28172] [INFO] Booting worker with pid: 28172
[2023-10-04 19:30:13 +0000] [684] [ERROR] Worker (pid:28117) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:30:13 +0000] [28193] [INFO] Booting worker with pid: 28193
2023-10-04 19:30:35.861904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:30:35.861904: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:30:37.659464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:30:37.659464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:30:40.813403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:40.813403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:40.827166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:40.827166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:40.831568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:40.831819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:30:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28172)
[2023-10-04 19:30:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28193)
[2023-10-04 19:30:43 +0000] [684] [ERROR] Worker (pid:28172) was sent code 134!
[2023-10-04 19:30:43 +0000] [28247] [INFO] Booting worker with pid: 28247
[2023-10-04 19:30:43 +0000] [684] [ERROR] Worker (pid:28193) was sent code 134!
[2023-10-04 19:30:43 +0000] [28248] [INFO] Booting worker with pid: 28248
2023-10-04 19:30:49.918563: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:30:49.937339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:30:50.751841: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:30:50.765712: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:30:51.605643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:51.608845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:51.610916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:51.614112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:51.617029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:51.619061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:53.301459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:53.301459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:53.305960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:53.306185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:53.310568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:53.310780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.105949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.108860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.110674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.112212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:30:56.195636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.197521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.199432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:30:56.201133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:30:57.094429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.095560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.096581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.097651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.098766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.099796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.100817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.101851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.102971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.104013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.105042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.106050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.107121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.108121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.109137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.110143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.111182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.112182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.113171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.114170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.115253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.116276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.117311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:30:57.118323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:31:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28247)
[2023-10-04 19:33:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28248)
[2023-10-04 19:35:25 +0000] [684] [ERROR] Worker (pid:28247) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:35:25 +0000] [28303] [INFO] Booting worker with pid: 28303
[2023-10-04 19:35:26 +0000] [684] [ERROR] Worker (pid:28248) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:35:26 +0000] [28304] [INFO] Booting worker with pid: 28304
2023-10-04 19:35:49.760096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:35:49.760096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:35:51.510249: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:35:51.510253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:35:54.451034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:35:54.451034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:35:54.468159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:35:54.468159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:35:54.472582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:35:54.472823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:35:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28303)
[2023-10-04 19:35:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28304)
[2023-10-04 19:35:56 +0000] [684] [ERROR] Worker (pid:28304) was sent code 134!
[2023-10-04 19:35:56 +0000] [28445] [INFO] Booting worker with pid: 28445
[2023-10-04 19:35:56 +0000] [684] [ERROR] Worker (pid:28303) was sent code 134!
[2023-10-04 19:35:56 +0000] [28446] [INFO] Booting worker with pid: 28446
2023-10-04 19:36:02.692387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:36:02.709650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:36:03.526466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:36:03.536640: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:36:04.386415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:04.387834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:04.390334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:04.391601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:04.393658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:04.396125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:06.593504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:06.593626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:06.598067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:06.598322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:06.602664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:06.602913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.508534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.510984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.512929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.514757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:36:09.522696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.525352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.527927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:36:09.530393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:36:10.455326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.456752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.457848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.458966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.459989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.461020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.462036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.463243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.464264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.465275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.466305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.467369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.468394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.469412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.470443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.471498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.472518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.473545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.474565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.475611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.476614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.477607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.478646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:36:10.479642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:36:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28445)
[2023-10-04 19:38:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28446)
[2023-10-04 19:40:41 +0000] [684] [ERROR] Worker (pid:28445) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:40:41 +0000] [28535] [INFO] Booting worker with pid: 28535
[2023-10-04 19:40:41 +0000] [684] [ERROR] Worker (pid:28446) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:40:41 +0000] [28538] [INFO] Booting worker with pid: 28538
2023-10-04 19:41:05.490651: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:41:05.490649: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:41:07.317424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:41:07.317427: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:41:10.387130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:10.387130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:10.402841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:10.402866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:10.407238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:10.407492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:41:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28535)
[2023-10-04 19:41:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28538)
[2023-10-04 19:41:12 +0000] [684] [ERROR] Worker (pid:28538) was sent code 134!
[2023-10-04 19:41:12 +0000] [28678] [INFO] Booting worker with pid: 28678
[2023-10-04 19:41:12 +0000] [684] [ERROR] Worker (pid:28535) was sent code 134!
[2023-10-04 19:41:12 +0000] [28679] [INFO] Booting worker with pid: 28679
2023-10-04 19:41:18.231315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:41:18.337818: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:41:19.061079: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:41:19.179763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:41:19.916946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:19.920350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:19.922485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:20.051925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:20.055105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:20.057359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:22.344868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:22.344868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:22.349318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:22.349571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:22.353756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:22.354013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.199636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.202546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.205018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.207542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:41:25.222484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.225161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.227392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:41:25.229956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:41:26.155698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.157128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.158173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.159279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.160305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.161357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.162413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.163483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.164513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.165538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.166582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.167654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.168730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.169734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.170851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.171891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.172899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.173907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.174989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.176012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.177034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.178090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.179187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:41:26.180226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:41:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28679)
[2023-10-04 19:43:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28678)
[2023-10-04 19:45:38 +0000] [684] [ERROR] Worker (pid:28679) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:45:38 +0000] [28766] [INFO] Booting worker with pid: 28766
[2023-10-04 19:45:38 +0000] [684] [ERROR] Worker (pid:28678) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:45:38 +0000] [28769] [INFO] Booting worker with pid: 28769
2023-10-04 19:46:01.919404: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:46:01.919403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:46:03.771948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:46:03.771948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:46:06.752149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:06.752149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:06.769183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:06.769183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:06.773774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:06.774001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:46:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28766)
[2023-10-04 19:46:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28769)
[2023-10-04 19:46:09 +0000] [684] [ERROR] Worker (pid:28766) was sent code 134!
[2023-10-04 19:46:09 +0000] [28859] [INFO] Booting worker with pid: 28859
[2023-10-04 19:46:09 +0000] [684] [ERROR] Worker (pid:28769) was sent code 134!
[2023-10-04 19:46:09 +0000] [28860] [INFO] Booting worker with pid: 28860
2023-10-04 19:46:15.286986: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:46:15.373471: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:46:16.115085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:46:16.196829: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:46:16.973851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:16.977403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:16.979752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:17.052062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:17.055243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:17.057367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:18.947021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:18.947052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:18.951513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:18.951722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:18.956200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:18.956384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.659312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.661683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.663771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.665917: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:46:21.707326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.709240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.711086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:46:21.712822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:46:22.616810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.618110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.619090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.620015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.620933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.621858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.622866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.624273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.625408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.627024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.628613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.629920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.631382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.632473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.633907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.635045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.636578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.638134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.639772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.640892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.641964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.643087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.644443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:46:22.645725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:46:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28859)
[2023-10-04 19:48:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28860)
[2023-10-04 19:50:38 +0000] [684] [ERROR] Worker (pid:28859) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:50:38 +0000] [28926] [INFO] Booting worker with pid: 28926
[2023-10-04 19:50:38 +0000] [684] [ERROR] Worker (pid:28860) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:50:38 +0000] [28927] [INFO] Booting worker with pid: 28927
2023-10-04 19:51:02.065940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:51:02.065940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:51:03.907419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:51:03.907419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:51:06.846491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:06.846491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:06.864304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:06.864304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:06.868933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:06.869152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:51:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28926)
[2023-10-04 19:51:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28927)
[2023-10-04 19:51:09 +0000] [684] [ERROR] Worker (pid:28927) was sent code 134!
[2023-10-04 19:51:09 +0000] [28999] [INFO] Booting worker with pid: 28999
[2023-10-04 19:51:09 +0000] [684] [ERROR] Worker (pid:28926) was sent code 134!
[2023-10-04 19:51:09 +0000] [29000] [INFO] Booting worker with pid: 29000
2023-10-04 19:51:15.212126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:51:15.280203: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:51:16.043884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:51:16.113838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:51:16.908499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:16.911731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:16.913452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:16.976946: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:16.980269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:16.982350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:18.879895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:18.880020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:18.884380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:18.884650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:18.888905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:18.889130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.607555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.611266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.614064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.616219: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:51:21.643559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.645493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.647236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:51:21.649040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:51:22.608587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.609947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.611320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.612683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.614255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.615815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.616844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.617957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.619137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.620507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.621779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.622865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.624501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.625621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.626856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.627916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.629621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.631283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.632866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.634189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.635826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.637411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.638513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:51:22.639680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:51:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:28999)
[2023-10-04 19:53:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29000)
[2023-10-04 19:54:09 +0000] [684] [ERROR] Worker (pid:28999) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:54:09 +0000] [29082] [INFO] Booting worker with pid: 29082
[2023-10-04 19:54:10 +0000] [684] [ERROR] Worker (pid:29000) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:54:10 +0000] [29083] [INFO] Booting worker with pid: 29083
2023-10-04 19:54:33.190466: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:54:33.190466: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:54:34.945818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:54:34.945818: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:54:38.002155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:38.002155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:38.020753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:38.020753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:38.025201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:38.025435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:54:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29082)
[2023-10-04 19:54:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29083)
[2023-10-04 19:54:40 +0000] [684] [ERROR] Worker (pid:29082) was sent code 134!
[2023-10-04 19:54:40 +0000] [29154] [INFO] Booting worker with pid: 29154
[2023-10-04 19:54:40 +0000] [684] [ERROR] Worker (pid:29083) was sent code 134!
[2023-10-04 19:54:40 +0000] [29155] [INFO] Booting worker with pid: 29155
2023-10-04 19:54:46.507223: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:54:46.534481: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:54:47.327802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:54:47.355188: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:54:48.180291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:48.183526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:48.185426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:48.206374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:48.209498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:48.211646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:50.155152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:50.155152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:50.159720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:50.159967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:50.164190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:50.164443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.939976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.942520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.944384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.946293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:54:52.951943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.954008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.955859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:54:52.957705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:54:53.890125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.891462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.892440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.893446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.895024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.896543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.897977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.899125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.900591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.901683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.902949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.904102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.905706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.907288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.908687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.910276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.911770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.912860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.914014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.915319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.916429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.918095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.919377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:54:53.920722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 19:55:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29155)
[2023-10-04 19:57:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29154)
[2023-10-04 19:59:01 +0000] [684] [ERROR] Worker (pid:29155) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:59:01 +0000] [29210] [INFO] Booting worker with pid: 29210
[2023-10-04 19:59:02 +0000] [684] [ERROR] Worker (pid:29154) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 19:59:02 +0000] [29222] [INFO] Booting worker with pid: 29222
2023-10-04 19:59:25.322046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:59:25.322046: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:59:27.179276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:59:27.179276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:59:30.194346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:30.194346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:30.208731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:30.208731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:30.213307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:30.213563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 19:59:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29210)
[2023-10-04 19:59:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29222)
[2023-10-04 19:59:32 +0000] [684] [ERROR] Worker (pid:29210) was sent code 134!
[2023-10-04 19:59:32 +0000] [29285] [INFO] Booting worker with pid: 29285
[2023-10-04 19:59:32 +0000] [684] [ERROR] Worker (pid:29222) was sent code 134!
[2023-10-04 19:59:32 +0000] [29286] [INFO] Booting worker with pid: 29286
2023-10-04 19:59:38.923389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:59:38.949804: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 19:59:39.744834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:59:39.768364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 19:59:40.600548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:40.603865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:40.606095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:40.614747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:40.617844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:40.620106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:42.381139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:42.381139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:42.385561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:42.385797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:42.390012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:42.390268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.098696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.101136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.103444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.105303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:59:45.157504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.159766: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.161647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 19:59:45.163605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 19:59:46.085773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.086973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.088030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.089654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.090771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.091820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.094917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.097154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.098686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.100356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.101667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.103027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.104329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.106017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.107545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.108609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.110177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.111342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.112451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.113768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.115268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.116635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.117694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 19:59:46.118803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:00:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29285)
[2023-10-04 20:02:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29286)
[2023-10-04 20:04:24 +0000] [684] [ERROR] Worker (pid:29285) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:04:24 +0000] [29340] [INFO] Booting worker with pid: 29340
[2023-10-04 20:04:24 +0000] [684] [ERROR] Worker (pid:29286) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:04:24 +0000] [29343] [INFO] Booting worker with pid: 29343
2023-10-04 20:04:48.254523: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:04:48.254523: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:04:50.044838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:04:50.044838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:04:53.036647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:04:53.036647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:04:53.051228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:04:53.051228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:04:53.055777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:04:53.056025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:04:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29340)
[2023-10-04 20:04:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29343)
[2023-10-04 20:04:54 +0000] [684] [ERROR] Worker (pid:29343) was sent code 134!
[2023-10-04 20:04:54 +0000] [29483] [INFO] Booting worker with pid: 29483
[2023-10-04 20:04:54 +0000] [684] [ERROR] Worker (pid:29340) was sent code 134!
[2023-10-04 20:04:54 +0000] [29484] [INFO] Booting worker with pid: 29484
2023-10-04 20:05:00.963122: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:05:00.976844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:05:01.780371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:05:01.805855: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:05:02.634910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:02.638269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:02.640308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:02.653436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:02.656676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:02.658775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:04.921744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:04.921744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:04.926254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:04.926615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:04.930499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:04.931222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.604523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.606620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.608580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.610222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:05:07.631414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.633321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.635124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:05:07.637560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:05:08.585096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.586417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.587468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.588470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.589443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.590452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.591492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.592515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.593541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.594631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.595654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.596675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.597713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.598774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.599862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.600898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.601916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.602980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.604027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.605078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.606108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.607234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.608250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:05:08.609315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:05:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29483)
[2023-10-04 20:07:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29484)
[2023-10-04 20:09:42 +0000] [684] [ERROR] Worker (pid:29483) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:09:42 +0000] [29564] [INFO] Booting worker with pid: 29564
[2023-10-04 20:09:42 +0000] [684] [ERROR] Worker (pid:29484) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:09:42 +0000] [29567] [INFO] Booting worker with pid: 29567
2023-10-04 20:10:06.596032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:10:06.596032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:10:08.392203: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:10:08.392206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:10:11.414457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:11.414457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:11.432034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:11.432034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:11.436512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:11.436752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:10:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29564)
[2023-10-04 20:10:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29567)
[2023-10-04 20:10:13 +0000] [684] [ERROR] Worker (pid:29564) was sent code 134!
[2023-10-04 20:10:13 +0000] [29715] [INFO] Booting worker with pid: 29715
[2023-10-04 20:10:13 +0000] [684] [ERROR] Worker (pid:29567) was sent code 134!
[2023-10-04 20:10:13 +0000] [29716] [INFO] Booting worker with pid: 29716
2023-10-04 20:10:19.011759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:10:19.095610: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:10:19.832077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:10:19.925592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:10:20.671152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:20.674406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:20.676536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:20.782270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:20.785413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:20.787681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:23.070498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:23.070535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:23.074896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:23.075143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:23.079306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:23.079550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.925656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.925667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.930775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.931043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.935840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.936123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:10:25.940914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:10:25.941178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:10:26.885793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.887107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.888076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.889039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.890003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.891415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.892671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.894171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.895793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.897359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.898738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.900311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.901470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.902849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.904029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.905084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.906310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.907922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.909149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.910216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.911754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.913373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.914702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:10:26.915825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:11:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29715)
[2023-10-04 20:12:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29716)
[2023-10-04 20:15:19 +0000] [684] [ERROR] Worker (pid:29715) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:15:19 +0000] [29809] [INFO] Booting worker with pid: 29809
[2023-10-04 20:15:20 +0000] [684] [ERROR] Worker (pid:29716) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:15:20 +0000] [29811] [INFO] Booting worker with pid: 29811
2023-10-04 20:15:43.869120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:15:43.869120: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:15:45.654004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:15:45.654004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:15:48.696222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:48.696222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:48.712706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:48.712705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:48.717115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:48.717386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:15:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29809)
[2023-10-04 20:15:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29811)
[2023-10-04 20:15:50 +0000] [684] [ERROR] Worker (pid:29811) was sent code 134!
[2023-10-04 20:15:50 +0000] [29951] [INFO] Booting worker with pid: 29951
[2023-10-04 20:15:50 +0000] [684] [ERROR] Worker (pid:29809) was sent code 134!
[2023-10-04 20:15:50 +0000] [29952] [INFO] Booting worker with pid: 29952
2023-10-04 20:15:56.567467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:15:56.591927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:15:57.395296: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:15:57.407796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:15:58.245757: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:58.247611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:58.249380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:58.251854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:58.252466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:15:58.256078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:00.693285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:00.693285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:00.697749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:00.697977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:00.702238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:00.702490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.506817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.508998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.510718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.512581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:16:03.517301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.519822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.522257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:16:03.524308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:16:04.449836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.451159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.452132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.453126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.454094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.455093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.456064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.457030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.457993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.459005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.459972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.460932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.461954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.463034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.464041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.465037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.466025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.467072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.468067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.469069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.470076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.471138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.472149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:16:04.473155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:16:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29951)
[2023-10-04 20:18:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:29952)
[2023-10-04 20:21:25 +0000] [684] [ERROR] Worker (pid:29951) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:21:25 +0000] [30036] [INFO] Booting worker with pid: 30036
[2023-10-04 20:21:25 +0000] [684] [ERROR] Worker (pid:29952) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:21:25 +0000] [30046] [INFO] Booting worker with pid: 30046
2023-10-04 20:21:49.436779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:21:49.436779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:21:51.245123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:21:51.245123: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:21:54.224826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:21:54.224826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:21:54.239512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:21:54.239532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:21:54.243843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:21:54.244096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:21:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30036)
[2023-10-04 20:21:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30046)
[2023-10-04 20:21:56 +0000] [684] [ERROR] Worker (pid:30046) was sent code 134!
[2023-10-04 20:21:56 +0000] [30189] [INFO] Booting worker with pid: 30189
[2023-10-04 20:21:56 +0000] [684] [ERROR] Worker (pid:30036) was sent code 134!
[2023-10-04 20:21:56 +0000] [30190] [INFO] Booting worker with pid: 30190
2023-10-04 20:22:02.169384: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:22:02.177133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:22:02.992251: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:22:02.995277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:22:03.841677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:03.842870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:03.845993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:03.847089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:03.849490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:03.851472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:06.055659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:06.055659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:06.060066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:06.060317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:06.064505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:06.064752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.708874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.712071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.714760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.717342: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18996 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:22:08.779776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.781519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.783186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:22:08.784807: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:22:09.704378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.705515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.706615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.707670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.708733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.710403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.712306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.714199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.716218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.718110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.719618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.721086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.722674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.724610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.726115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.727611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.730025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.731643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.732679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.734708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.736203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.737599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.738669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:22:09.739735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:22:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30189)
[2023-10-04 20:24:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30190)
[2023-10-04 20:25:42 +0000] [684] [ERROR] Worker (pid:30189) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:25:42 +0000] [30270] [INFO] Booting worker with pid: 30270
[2023-10-04 20:25:42 +0000] [684] [ERROR] Worker (pid:30190) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:25:42 +0000] [30279] [INFO] Booting worker with pid: 30279
2023-10-04 20:26:05.986401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:26:05.986401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:26:07.796486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:26:07.796486: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:26:10.757314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:10.757314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:10.776537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:10.776537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:10.780923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:10.781175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:26:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30270)
[2023-10-04 20:26:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30279)
[2023-10-04 20:26:13 +0000] [684] [ERROR] Worker (pid:30270) was sent code 134!
[2023-10-04 20:26:13 +0000] [30348] [INFO] Booting worker with pid: 30348
[2023-10-04 20:26:13 +0000] [684] [ERROR] Worker (pid:30279) was sent code 134!
[2023-10-04 20:26:13 +0000] [30369] [INFO] Booting worker with pid: 30369
2023-10-04 20:26:19.362403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:26:19.413090: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:26:20.190183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:26:20.229301: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:26:21.050540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:21.053904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:21.056044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:21.084799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:21.087983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:21.090057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:23.009880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:23.009880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:23.014313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:23.014569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:23.018771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:23.019038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.725052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.727838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.729924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.732626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:26:25.775695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.778375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.780939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:26:25.783499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:26:26.719704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.721177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.722412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.724070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.725631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.727269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.728576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.729811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.731176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.732868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.734126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.735578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.737345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.739131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.741320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.743119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.744954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.746742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.748211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.749826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.751069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.752369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.753539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:26:26.754760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:27:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30348)
[2023-10-04 20:28:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30369)
[2023-10-04 20:28:51 +0000] [684] [ERROR] Worker (pid:30348) was sent code 134!
[2023-10-04 20:28:51 +0000] [30454] [INFO] Booting worker with pid: 30454
[2023-10-04 20:28:51 +0000] [684] [ERROR] Worker (pid:30369) was sent code 134!
[2023-10-04 20:28:51 +0000] [30455] [INFO] Booting worker with pid: 30455
2023-10-04 20:29:13.398678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:29:13.398678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:29:15.249490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:29:15.249490: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:29:18.192779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:18.192779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:18.206572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:18.206641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:18.211075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:18.211345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:29:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30454)
[2023-10-04 20:29:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30455)
[2023-10-04 20:29:21 +0000] [684] [ERROR] Worker (pid:30455) was sent code 134!
[2023-10-04 20:29:21 +0000] [30536] [INFO] Booting worker with pid: 30536
[2023-10-04 20:29:21 +0000] [684] [ERROR] Worker (pid:30454) was sent code 134!
[2023-10-04 20:29:21 +0000] [30537] [INFO] Booting worker with pid: 30537
2023-10-04 20:29:27.701857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:29:27.730257: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:29:28.532059: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:29:28.554512: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:29:29.384718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:29.388242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:29.390489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:29.413461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:29.416812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:29.419042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:30.947862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:30.949870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:30.951533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:30.972643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:30.976849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:30.978908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.040485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.043470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.045913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.048114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:29:34.139205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.141172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.143084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:29:34.145019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:29:35.042745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.043919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.045005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.046032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.047107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.048140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.049169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.050202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.051266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.052287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.053312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.054849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.055960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.057338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.058751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.060127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.061507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.063052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.064096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.066022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.067515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.068955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.070421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:29:35.071720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:30:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30537)
[2023-10-04 20:33:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30536)
[2023-10-04 20:34:54 +0000] [684] [ERROR] Worker (pid:30537) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:34:54 +0000] [30592] [INFO] Booting worker with pid: 30592
[2023-10-04 20:34:55 +0000] [684] [ERROR] Worker (pid:30536) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:34:55 +0000] [30594] [INFO] Booting worker with pid: 30594
2023-10-04 20:35:18.452352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:35:18.452352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:35:20.242743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:35:20.242743: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:35:23.370050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:23.370050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:23.387260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:23.387260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:23.391667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:23.391919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:35:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30592)
[2023-10-04 20:35:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30594)
[2023-10-04 20:35:25 +0000] [684] [ERROR] Worker (pid:30592) was sent code 134!
[2023-10-04 20:35:25 +0000] [30768] [INFO] Booting worker with pid: 30768
[2023-10-04 20:35:25 +0000] [684] [ERROR] Worker (pid:30594) was sent code 134!
[2023-10-04 20:35:25 +0000] [30769] [INFO] Booting worker with pid: 30769
2023-10-04 20:35:31.574685: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:35:31.582303: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:35:32.392983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:35:32.400332: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:35:33.241681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:33.244900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:33.245542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:33.248500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:33.251029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:33.253200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:35.325003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:35.325040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:35.329597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:35.329827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:35.334263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:35.334476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:37.994887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:37.997651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:37.999735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:38.002397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18992 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:35:38.097893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:38.099886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:38.101562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:35:38.103404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:35:38.994056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:38.995216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:38.996235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:38.997252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:38.998289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:38.999383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.000402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.001432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.002490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.003556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.004574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.005616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.006710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.007738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.008764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.009788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.010857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.011876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.012889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.013901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.015777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.016807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.017828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:35:39.018894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:36:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30768)
[2023-10-04 20:36:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30769)
[2023-10-04 20:36:12 +0000] [684] [ERROR] Worker (pid:30768) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:36:12 +0000] [30823] [INFO] Booting worker with pid: 30823
[2023-10-04 20:36:12 +0000] [684] [ERROR] Worker (pid:30769) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:36:12 +0000] [30824] [INFO] Booting worker with pid: 30824
2023-10-04 20:36:35.671333: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:36:35.671333: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:36:37.599880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:36:37.599880: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:36:40.695289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:40.695289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:40.709602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:40.709633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:40.713928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:40.714182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:36:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30823)
[2023-10-04 20:36:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30824)
[2023-10-04 20:36:42 +0000] [684] [ERROR] Worker (pid:30823) was sent code 134!
[2023-10-04 20:36:42 +0000] [30884] [INFO] Booting worker with pid: 30884
[2023-10-04 20:36:43 +0000] [684] [ERROR] Worker (pid:30824) was sent code 134!
[2023-10-04 20:36:43 +0000] [30885] [INFO] Booting worker with pid: 30885
2023-10-04 20:36:48.960868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:36:48.960868: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:36:49.772749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:36:49.775244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:36:50.620045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:50.622287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:50.623295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:50.626779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:50.626874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:50.631390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:52.666047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:52.666047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:52.670634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:52.670886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:52.675244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:52.675501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.271392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.273706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.275883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.277851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:36:55.286067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.288968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.291292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:36:55.293018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:36:56.223114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.224460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.225465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.226461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.227481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.228491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.229479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.230473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.231483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.232471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.233438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.234426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.235462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.236433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.237412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.238398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.239423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.240401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.241377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.242401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.243435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.244429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.245449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:36:56.246461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:37:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30885)
[2023-10-04 20:37:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30884)
[2023-10-04 20:37:41 +0000] [684] [ERROR] Worker (pid:30885) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:37:41 +0000] [30937] [INFO] Booting worker with pid: 30937
[2023-10-04 20:37:42 +0000] [684] [ERROR] Worker (pid:30884) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:37:42 +0000] [30938] [INFO] Booting worker with pid: 30938
2023-10-04 20:38:04.723555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:38:04.723555: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:38:06.534850: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:38:06.534849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:38:09.776907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:09.776907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:09.792093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:09.792093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:09.796503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:09.796751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:38:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30937)
[2023-10-04 20:38:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30938)
[2023-10-04 20:38:12 +0000] [684] [ERROR] Worker (pid:30938) was sent code 134!
[2023-10-04 20:38:12 +0000] [30998] [INFO] Booting worker with pid: 30998
[2023-10-04 20:38:12 +0000] [684] [ERROR] Worker (pid:30937) was sent code 134!
[2023-10-04 20:38:12 +0000] [30999] [INFO] Booting worker with pid: 30999
2023-10-04 20:38:18.526759: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:38:18.622061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:38:19.348984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:38:19.439759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:38:20.199179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:20.202327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:20.204543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:20.275396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:20.278807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:20.281031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:21.975074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:21.975074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:21.979707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:21.979934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:21.984330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:21.984547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.923524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.926870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.929456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.930342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.933526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:38:24.934322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.936472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:38:24.938508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:38:25.867836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.869119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.870115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.871200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.872205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.873207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.874217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.875253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.876265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.877264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.878289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.879322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.880329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.881320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.882401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.883494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.884503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.885495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.886534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.887588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.888592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.889598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.890648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:38:25.891659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:39:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30998)
[2023-10-04 20:40:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:30999)
[2023-10-04 20:40:25 +0000] [684] [ERROR] Worker (pid:30998) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:40:25 +0000] [31085] [INFO] Booting worker with pid: 31085
[2023-10-04 20:40:25 +0000] [684] [ERROR] Worker (pid:30999) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:40:25 +0000] [31086] [INFO] Booting worker with pid: 31086
2023-10-04 20:40:49.066390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:40:49.066389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:40:50.884547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:40:50.884547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:40:53.968652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:40:53.968651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:40:53.986765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:40:53.986791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:40:53.991186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:40:53.991431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:40:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31085)
[2023-10-04 20:40:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31086)
[2023-10-04 20:40:55 +0000] [684] [ERROR] Worker (pid:31085) was sent code 134!
[2023-10-04 20:40:55 +0000] [31156] [INFO] Booting worker with pid: 31156
[2023-10-04 20:40:56 +0000] [684] [ERROR] Worker (pid:31086) was sent code 134!
[2023-10-04 20:40:56 +0000] [31157] [INFO] Booting worker with pid: 31157
2023-10-04 20:41:01.910315: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:41:02.007244: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:41:02.720934: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:41:02.827387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:41:03.565135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:03.568622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:03.570833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:03.679137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:03.682539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:03.684756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:05.784234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:05.784234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:05.788677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:05.788922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:05.793153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:05.793411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.521504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.524260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.526737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.529131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19022 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:41:08.668402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.670263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.671869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:41:08.673484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:41:09.586183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.588040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.589067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.590080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.591461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.593299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.594343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.595874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.597113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.598824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.600535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.602278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.604489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.606391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.608414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.610385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.612233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.613548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.614838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.616961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.618342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.619641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.620922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.622133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:41:09.623377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:41:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31156)
[2023-10-04 20:43:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31157)
[2023-10-04 20:45:38 +0000] [684] [ERROR] Worker (pid:31156) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:45:38 +0000] [31236] [INFO] Booting worker with pid: 31236
[2023-10-04 20:45:38 +0000] [684] [ERROR] Worker (pid:31157) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:45:38 +0000] [31245] [INFO] Booting worker with pid: 31245
2023-10-04 20:46:02.272440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:46:02.272440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:46:04.193081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:46:04.193085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:46:07.190787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:07.190787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:07.205390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:07.205402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:07.209724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:07.209981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:46:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31236)
[2023-10-04 20:46:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31245)
[2023-10-04 20:46:08 +0000] [684] [ERROR] Worker (pid:31245) was sent code 134!
[2023-10-04 20:46:08 +0000] [31386] [INFO] Booting worker with pid: 31386
[2023-10-04 20:46:08 +0000] [684] [ERROR] Worker (pid:31236) was sent code 134!
[2023-10-04 20:46:08 +0000] [31387] [INFO] Booting worker with pid: 31387
2023-10-04 20:46:14.776273: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:46:14.877532: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:46:15.597168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:46:15.715558: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:46:16.443146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:16.446521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:16.448773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:16.576217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:16.579485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:16.581555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:18.912466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:18.912466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:18.916905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:18.917154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:18.921340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:18.921590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.539869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.542165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.544007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.545761: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:46:21.757180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.759173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.760940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:46:21.762673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:46:22.672731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.673887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.674990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.676355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.677392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.678438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.679575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.680609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.681635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.682721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.683745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.684772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.685814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.686934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.688023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.689153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.690183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.691270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.692326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.693369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.694416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.695488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.696549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.697571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:46:22.698640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:46:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31386)
[2023-10-04 20:48:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31387)
[2023-10-04 20:51:06 +0000] [684] [ERROR] Worker (pid:31386) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:51:06 +0000] [31475] [INFO] Booting worker with pid: 31475
[2023-10-04 20:51:06 +0000] [684] [ERROR] Worker (pid:31387) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:51:06 +0000] [31476] [INFO] Booting worker with pid: 31476
2023-10-04 20:51:30.125586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:51:30.125584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:51:31.844675: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:51:31.844679: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:51:34.798924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:34.798924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:34.817151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:34.817152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:34.821479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:34.821729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:51:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31475)
[2023-10-04 20:51:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31476)
[2023-10-04 20:51:36 +0000] [684] [ERROR] Worker (pid:31475) was sent code 134!
[2023-10-04 20:51:36 +0000] [31621] [INFO] Booting worker with pid: 31621
[2023-10-04 20:51:36 +0000] [684] [ERROR] Worker (pid:31476) was sent code 134!
[2023-10-04 20:51:36 +0000] [31622] [INFO] Booting worker with pid: 31622
2023-10-04 20:51:42.787786: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:51:42.830286: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:51:43.605903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:51:43.649811: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:51:44.456200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:44.459663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:44.461760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:44.509372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:44.512815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:44.515070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:46.617175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:46.617193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:46.621607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:46.621854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:46.626003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:46.626258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.399416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.401782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.404006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.405990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:51:49.421685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.423821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.425582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:51:49.427417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:51:50.343224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.344506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.345494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.346484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.347486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.348466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.349433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.350408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.351474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.352494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.353504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.354540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.355616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.356656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.357712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.358797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.359828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.360835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.361854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.362935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.363964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.364989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.365987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:51:50.367105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:52:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31621)
[2023-10-04 20:54:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31622)
[2023-10-04 20:56:30 +0000] [684] [ERROR] Worker (pid:31621) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:56:30 +0000] [31682] [INFO] Booting worker with pid: 31682
[2023-10-04 20:56:30 +0000] [684] [ERROR] Worker (pid:31622) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 20:56:30 +0000] [31683] [INFO] Booting worker with pid: 31683
2023-10-04 20:56:54.001447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:56:54.001447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:56:55.763668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:56:55.763668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:56:58.821062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:56:58.821062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:56:58.835998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:56:58.835995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:56:58.840392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:56:58.840660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 20:57:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31682)
[2023-10-04 20:57:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31683)
[2023-10-04 20:57:00 +0000] [684] [ERROR] Worker (pid:31682) was sent code 134!
[2023-10-04 20:57:00 +0000] [31823] [INFO] Booting worker with pid: 31823
[2023-10-04 20:57:00 +0000] [684] [ERROR] Worker (pid:31683) was sent code 134!
[2023-10-04 20:57:00 +0000] [31824] [INFO] Booting worker with pid: 31824
2023-10-04 20:57:06.853389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:57:06.874671: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 20:57:07.666796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:57:07.685359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 20:57:08.517848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:08.521069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:08.522893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:08.534091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:08.537367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:08.539501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:10.690388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:10.690424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:10.694828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:10.695077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:10.699248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:10.699496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.250091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.252355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.254508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.256367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:57:13.335794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.338533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.341049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 20:57:13.342926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 20:57:14.352876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.354501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.355613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.357101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.358826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.360909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.362615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.364046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.366040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.368067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.369626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.371241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.372741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.374258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.375790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.379833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.381363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.383377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.385391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.386914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.388010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.391457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.392538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 20:57:14.393607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 20:57:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31823)
[2023-10-04 20:59:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31824)
[2023-10-04 21:00:49 +0000] [684] [ERROR] Worker (pid:31823) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:00:49 +0000] [31911] [INFO] Booting worker with pid: 31911
[2023-10-04 21:00:49 +0000] [684] [ERROR] Worker (pid:31824) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:00:49 +0000] [31914] [INFO] Booting worker with pid: 31914
2023-10-04 21:01:13.171731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:01:13.171731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:01:15.085185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:01:15.085185: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:01:18.266357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:18.266357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:18.283532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:18.283532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:18.288079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:18.288336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:01:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31911)
[2023-10-04 21:01:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:31914)
[2023-10-04 21:01:20 +0000] [684] [ERROR] Worker (pid:31911) was sent code 134!
[2023-10-04 21:01:20 +0000] [32013] [INFO] Booting worker with pid: 32013
[2023-10-04 21:01:20 +0000] [684] [ERROR] Worker (pid:31914) was sent code 134!
[2023-10-04 21:01:20 +0000] [32014] [INFO] Booting worker with pid: 32014
2023-10-04 21:01:26.165677: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:01:26.186556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:01:26.983246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:01:27.017101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:01:27.825037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:27.828403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:27.830456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:27.872761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:27.875914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:27.878097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:30.256922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:30.256922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:30.261447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:30.261722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:30.266029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:30.266278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.272894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.275201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.276911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.278661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:01:33.316651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.319541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.322218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:01:33.324795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:01:34.291456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.292749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.293779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.294849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.295876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.296935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.298144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.299447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.300803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.301871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.303587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.305236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.306972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.308601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.310065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.311281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.312434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.313583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.314773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.315861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.316931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.318009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.319133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:01:34.320225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:02:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32013)
[2023-10-04 21:04:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32014)
[2023-10-04 21:06:03 +0000] [684] [ERROR] Worker (pid:32013) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:06:03 +0000] [32071] [INFO] Booting worker with pid: 32071
[2023-10-04 21:06:04 +0000] [684] [ERROR] Worker (pid:32014) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:06:04 +0000] [32072] [INFO] Booting worker with pid: 32072
2023-10-04 21:06:27.133019: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:06:27.133019: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:06:28.891566: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:06:28.891572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:06:31.792580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:31.792580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:31.809255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:31.809273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:31.813719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:31.813965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:06:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32071)
[2023-10-04 21:06:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32072)
[2023-10-04 21:06:34 +0000] [684] [ERROR] Worker (pid:32072) was sent code 134!
[2023-10-04 21:06:34 +0000] [32144] [INFO] Booting worker with pid: 32144
[2023-10-04 21:06:34 +0000] [684] [ERROR] Worker (pid:32071) was sent code 134!
[2023-10-04 21:06:34 +0000] [32145] [INFO] Booting worker with pid: 32145
2023-10-04 21:06:40.425026: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:06:40.502726: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:06:41.237342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:06:41.326064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:06:42.084365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:42.087693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:42.089675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:42.178876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:42.182075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:42.184295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:43.879684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:43.879689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:43.884087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:43.884342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:43.888527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:43.888775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.726923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.729780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.732427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.735034: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:06:46.790352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.792413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.794269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:06:46.795916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:06:47.704588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.706050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.707243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.708447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.710027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.711582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.713039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.714180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.715666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.717028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.718407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.719489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.720594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.721915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.723118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.724409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.725922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.727380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.728598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.729883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.731536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.733157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.734806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:06:47.736402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:07:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32145)
[2023-10-04 21:09:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32144)
[2023-10-04 21:11:12 +0000] [684] [ERROR] Worker (pid:32145) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:11:13 +0000] [32198] [INFO] Booting worker with pid: 32198
[2023-10-04 21:11:13 +0000] [684] [ERROR] Worker (pid:32144) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:11:13 +0000] [32202] [INFO] Booting worker with pid: 32202
2023-10-04 21:11:36.836640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:11:36.836640: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:11:38.615445: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:11:38.615444: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:11:41.582726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:41.582726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:41.597670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:41.597670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:41.602068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:41.602303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:11:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32198)
[2023-10-04 21:11:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32202)
[2023-10-04 21:11:43 +0000] [684] [ERROR] Worker (pid:32198) was sent code 134!
[2023-10-04 21:11:43 +0000] [32273] [INFO] Booting worker with pid: 32273
[2023-10-04 21:11:44 +0000] [684] [ERROR] Worker (pid:32202) was sent code 134!
[2023-10-04 21:11:44 +0000] [32274] [INFO] Booting worker with pid: 32274
2023-10-04 21:11:49.954709: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:11:49.976467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:11:50.770556: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:11:50.790871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:11:51.622357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:51.625709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:51.627786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:51.631894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:51.634925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:51.636866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:53.631721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:53.631898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:53.636174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:53.636429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:53.640632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:53.640889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.322683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.325535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.328087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.330561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:11:56.516516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.518663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.520304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:11:56.521906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:11:57.421432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.422789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.423872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.425672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.427214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.428668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.429748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.431158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.432621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.434068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.436016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.437090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.438535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.440373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.441836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.443313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.444737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.446169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.447715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.449142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.450633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.452076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.453147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:11:57.454210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:12:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32273)
[2023-10-04 21:14:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32274)
[2023-10-04 21:16:52 +0000] [684] [ERROR] Worker (pid:32273) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:16:52 +0000] [32327] [INFO] Booting worker with pid: 32327
[2023-10-04 21:16:52 +0000] [684] [ERROR] Worker (pid:32274) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:16:52 +0000] [32328] [INFO] Booting worker with pid: 32328
2023-10-04 21:17:16.439716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:17:16.439716: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:17:18.276957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:17:18.276957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:17:21.393014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:21.393014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:21.407464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:21.407481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:21.411789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:21.412044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:17:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32327)
[2023-10-04 21:17:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32328)
[2023-10-04 21:17:22 +0000] [684] [ERROR] Worker (pid:32327) was sent code 134!
[2023-10-04 21:17:22 +0000] [32507] [INFO] Booting worker with pid: 32507
[2023-10-04 21:17:22 +0000] [684] [ERROR] Worker (pid:32328) was sent code 134!
[2023-10-04 21:17:22 +0000] [32508] [INFO] Booting worker with pid: 32508
2023-10-04 21:17:29.015410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:17:29.037602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:17:29.825166: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:17:29.847725: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:17:30.674021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:30.677268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:30.679536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:30.694840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:30.698066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:30.700031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:33.185370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:33.185370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:33.189866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:33.190066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:33.194465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:33.194713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.091330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.091342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.096471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.096724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.101393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.101673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:17:36.106257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:17:36.106516: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:17:37.040427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.041433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.042736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.043933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.045197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.046372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.047458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.048586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.049732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.050882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.052191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.053470: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.054775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.055900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.057189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.058332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.059441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.060571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.061606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.062679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.064303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.065360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.066449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:17:37.067494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:18:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32507)
[2023-10-04 21:19:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32508)
[2023-10-04 21:22:03 +0000] [684] [ERROR] Worker (pid:32507) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:22:03 +0000] [32566] [INFO] Booting worker with pid: 32566
[2023-10-04 21:22:03 +0000] [684] [ERROR] Worker (pid:32508) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:22:03 +0000] [32568] [INFO] Booting worker with pid: 32568
2023-10-04 21:22:27.583436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:22:27.583436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:22:29.429701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:22:29.429701: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:22:32.393567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:32.393567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:32.410513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:32.410513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:32.414913: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:32.415156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:22:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32566)
[2023-10-04 21:22:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32568)
[2023-10-04 21:22:34 +0000] [684] [ERROR] Worker (pid:32566) was sent code 134!
[2023-10-04 21:22:34 +0000] [32709] [INFO] Booting worker with pid: 32709
[2023-10-04 21:22:34 +0000] [684] [ERROR] Worker (pid:32568) was sent code 134!
[2023-10-04 21:22:34 +0000] [32710] [INFO] Booting worker with pid: 32710
2023-10-04 21:22:40.272703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:22:40.306117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:22:41.083281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:22:41.117119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:22:41.925796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:41.929023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:41.931285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:41.957209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:41.960373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:41.962428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:44.294052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:44.294052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:44.298548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:44.298756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:44.303047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:44.303247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.212424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.215216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.217348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.219438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:22:47.244974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.246985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.248844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:22:47.250802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:22:48.168003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.169446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.170507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.171571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.172618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.173658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.174735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.175776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.176805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.177826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.178904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.179927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.180949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.181953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.183050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.184087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.185101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.186107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.187178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.188188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.189199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.190238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.191314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:22:48.192318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:23:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32709)
[2023-10-04 21:25:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32710)
[2023-10-04 21:28:03 +0000] [684] [ERROR] Worker (pid:32709) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:28:03 +0000] [32769] [INFO] Booting worker with pid: 32769
[2023-10-04 21:28:04 +0000] [684] [ERROR] Worker (pid:32710) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:28:04 +0000] [32772] [INFO] Booting worker with pid: 32772
2023-10-04 21:28:28.156858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:28:28.156858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:28:29.976194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:28:29.976194: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:28:32.953017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:32.953017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:32.970286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:32.970286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:32.974623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:32.974857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:28:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32769)
[2023-10-04 21:28:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32772)
[2023-10-04 21:28:34 +0000] [684] [ERROR] Worker (pid:32769) was sent code 134!
[2023-10-04 21:28:34 +0000] [32912] [INFO] Booting worker with pid: 32912
[2023-10-04 21:28:34 +0000] [684] [ERROR] Worker (pid:32772) was sent code 134!
[2023-10-04 21:28:34 +0000] [32913] [INFO] Booting worker with pid: 32913
2023-10-04 21:28:40.705241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:28:40.769971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:28:41.521696: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:28:41.582170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:28:42.370281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:42.373555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:42.375638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:42.422243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:42.425379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:42.427446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:44.710528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:44.710544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:44.714998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:44.715249: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:44.719479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:44.719736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.376878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.379282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.381183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.383015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:28:47.400912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.402944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.404768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:28:47.406388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:28:48.325123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.326911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.328693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.330432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.332196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.333905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.335567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.336811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.337997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.339196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.340352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.341519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.342760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.343957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.345130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.346335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.347964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.349125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.350755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.352351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.354487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.356229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.357465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:28:48.358765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:29:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32912)
[2023-10-04 21:30:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32913)
[2023-10-04 21:32:13 +0000] [684] [ERROR] Worker (pid:32912) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:32:13 +0000] [32969] [INFO] Booting worker with pid: 32969
[2023-10-04 21:32:13 +0000] [684] [ERROR] Worker (pid:32913) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:32:13 +0000] [32970] [INFO] Booting worker with pid: 32970
2023-10-04 21:32:37.276184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:32:37.276184: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:32:39.143784: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:32:39.143788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:32:42.163034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:42.163034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:42.178739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:42.178739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:42.183166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:42.183429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:32:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32969)
[2023-10-04 21:32:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:32970)
[2023-10-04 21:32:44 +0000] [684] [ERROR] Worker (pid:32970) was sent code 134!
[2023-10-04 21:32:44 +0000] [33041] [INFO] Booting worker with pid: 33041
[2023-10-04 21:32:44 +0000] [684] [ERROR] Worker (pid:32969) was sent code 134!
[2023-10-04 21:32:44 +0000] [33042] [INFO] Booting worker with pid: 33042
2023-10-04 21:32:50.126443: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:32:50.164308: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:32:50.943344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:32:50.977081: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:32:51.790756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:51.793916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:51.796122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:51.823390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:51.826384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:51.828390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:54.017777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:54.017777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:54.022227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:54.022482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:54.026690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:54.026935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.681094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.683830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.686313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.688806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:32:56.929384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.931396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.933038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:32:56.934641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:32:57.873043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.874364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.875486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.876593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.878284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.880658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.882044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.883712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.885161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.886379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.888082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.890540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.892183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.893817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.895825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.897260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.898880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.900472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.902064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.904062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.905650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.907682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.909306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.910889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:32:57.912506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:33:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33041)
[2023-10-04 21:35:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33042)
[2023-10-04 21:37:26 +0000] [684] [ERROR] Worker (pid:33041) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:37:26 +0000] [33096] [INFO] Booting worker with pid: 33096
[2023-10-04 21:37:26 +0000] [684] [ERROR] Worker (pid:33042) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:37:26 +0000] [33098] [INFO] Booting worker with pid: 33098
2023-10-04 21:37:50.523534: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:37:50.523533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:37:52.321899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:37:52.321899: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:37:55.402358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:37:55.402358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:37:55.418736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:37:55.418736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:37:55.423170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:37:55.423414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:37:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33096)
[2023-10-04 21:37:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33098)
[2023-10-04 21:37:57 +0000] [684] [ERROR] Worker (pid:33098) was sent code 134!
[2023-10-04 21:37:57 +0000] [33239] [INFO] Booting worker with pid: 33239
[2023-10-04 21:37:57 +0000] [684] [ERROR] Worker (pid:33096) was sent code 134!
[2023-10-04 21:37:57 +0000] [33240] [INFO] Booting worker with pid: 33240
2023-10-04 21:38:02.946361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:38:03.066703: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:38:03.770437: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:38:03.884862: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:38:04.621507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:04.624846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:04.627186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:04.743977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:04.747026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:04.749022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:07.049969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:07.050147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:07.054638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:07.054811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:07.059161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:07.059375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.923468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.926156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.927285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.930058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.932261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.934918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:38:09.935907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:38:09.937844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:38:10.866063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.867400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.868369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.869337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.870313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.871324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.872302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.873305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.874304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.875362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.876381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.877408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.878455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.879497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.880516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.881524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.882550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.883585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.884600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.885606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.886701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.887795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.888826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:38:10.889853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:38:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33239)
[2023-10-04 21:40:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33240)
[2023-10-04 21:43:06 +0000] [684] [ERROR] Worker (pid:33239) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:43:07 +0000] [33322] [INFO] Booting worker with pid: 33322
[2023-10-04 21:43:07 +0000] [684] [ERROR] Worker (pid:33240) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:43:07 +0000] [33334] [INFO] Booting worker with pid: 33334
2023-10-04 21:43:31.084366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:43:31.084366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:43:32.831094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:43:32.831089: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:43:35.745885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:35.745885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:35.761851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:35.761851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:35.766422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:35.766708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:43:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33322)
[2023-10-04 21:43:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33334)
[2023-10-04 21:43:37 +0000] [684] [ERROR] Worker (pid:33322) was sent code 134!
[2023-10-04 21:43:37 +0000] [33475] [INFO] Booting worker with pid: 33475
[2023-10-04 21:43:37 +0000] [684] [ERROR] Worker (pid:33334) was sent code 134!
[2023-10-04 21:43:37 +0000] [33476] [INFO] Booting worker with pid: 33476
2023-10-04 21:43:43.804069: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:43:43.846241: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:43:44.619077: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:43:44.667220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:43:45.463881: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:45.467289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:45.469399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:45.519383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:45.522953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:45.525049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:47.688889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:47.688939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:47.693290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:47.693543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:47.697744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:47.697997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.348775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.351064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.353070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.355045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:43:50.447216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.449239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.451212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:43:50.453055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:43:51.396401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.398170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.399603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.400905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.402179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.403688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.405262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.406787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.408717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.410909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.412514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.414561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.416542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.417583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.419594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.421616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.423601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.425096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.426151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.427695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.429196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.430765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.431991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:43:51.433067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:44:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33475)
[2023-10-04 21:46:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33476)
[2023-10-04 21:48:44 +0000] [684] [ERROR] Worker (pid:33475) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:48:45 +0000] [33535] [INFO] Booting worker with pid: 33535
[2023-10-04 21:48:45 +0000] [684] [ERROR] Worker (pid:33476) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:48:45 +0000] [33536] [INFO] Booting worker with pid: 33536
2023-10-04 21:49:08.865041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:49:08.865041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:49:10.665015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:49:10.665015: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:49:13.685013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:13.685013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:13.700435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:13.700435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:13.704788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:13.705045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:49:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33535)
[2023-10-04 21:49:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33536)
[2023-10-04 21:49:15 +0000] [684] [ERROR] Worker (pid:33535) was sent code 134!
[2023-10-04 21:49:15 +0000] [33679] [INFO] Booting worker with pid: 33679
[2023-10-04 21:49:15 +0000] [684] [ERROR] Worker (pid:33536) was sent code 134!
[2023-10-04 21:49:15 +0000] [33680] [INFO] Booting worker with pid: 33680
2023-10-04 21:49:21.735696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:49:21.751822: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:49:22.561968: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:49:22.566372: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:49:23.404529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:23.405006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:23.409794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:23.410143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:23.413896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:23.414583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:25.606572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:25.606619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:25.610986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:25.611237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:25.615397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:25.615644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.276543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.279320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.281678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.284154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:49:28.363469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.365408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.367256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:49:28.369121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:49:29.263508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.264627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.265664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.266775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.267823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.268864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.269904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.270982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.272000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.273022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.274054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.275100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.276119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.277129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.278131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.279187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.280193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.281209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.282230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.283294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.284327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.285340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.286361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:49:29.287427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:50:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33679)
[2023-10-04 21:52:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33680)
[2023-10-04 21:52:21 +0000] [684] [ERROR] Worker (pid:33680) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:52:21 +0000] [33788] [INFO] Booting worker with pid: 33788
[2023-10-04 21:52:21 +0000] [684] [ERROR] Worker (pid:33679) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:52:21 +0000] [33789] [INFO] Booting worker with pid: 33789
2023-10-04 21:52:43.301276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:52:43.301276: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:52:45.155913: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:52:45.155908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:52:48.033167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:48.033167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:48.045882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:48.045882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:48.050307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:48.050546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:50.977513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:50.977539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:50.981980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:50.982234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:50.986478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:50.986731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 21:52:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33788)
[2023-10-04 21:52:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33789)
[2023-10-04 21:52:51 +0000] [684] [ERROR] Worker (pid:33788) was sent code 134!
[2023-10-04 21:52:51 +0000] [33850] [INFO] Booting worker with pid: 33850
[2023-10-04 21:52:51 +0000] [684] [ERROR] Worker (pid:33789) was sent code 134!
[2023-10-04 21:52:51 +0000] [33851] [INFO] Booting worker with pid: 33851
2023-10-04 21:52:57.518819: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:52:57.579236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:52:58.351670: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:52:58.395952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:52:59.205441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:59.208700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:59.210971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:59.241505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:59.244603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:52:59.246558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:00.774540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:00.776633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:00.778904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:00.806761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:00.809120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:00.811698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.668467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.670791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.672969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.674990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:53:03.682174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.684883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.687447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:53:03.689217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:53:04.635735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.637020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.638048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.639110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.640135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.641168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.642186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.643234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.644253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.645275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.646307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.647355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.648387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.649414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.650460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.651493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.652506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.653534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.654578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.655643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.656673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.657672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.658724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:53:04.659731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:53:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33850)
[2023-10-04 21:55:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33851)
[2023-10-04 21:57:35 +0000] [684] [ERROR] Worker (pid:33850) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:57:35 +0000] [33934] [INFO] Booting worker with pid: 33934
[2023-10-04 21:57:36 +0000] [684] [ERROR] Worker (pid:33851) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 21:57:36 +0000] [33936] [INFO] Booting worker with pid: 33936
2023-10-04 21:58:03.872275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:58:03.872275: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:58:05.608012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:58:05.608012: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
[2023-10-04 21:58:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33934)
[2023-10-04 21:58:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:33936)
[2023-10-04 21:58:06 +0000] [684] [ERROR] Worker (pid:33936) was sent code 134!
[2023-10-04 21:58:06 +0000] [34817] [INFO] Booting worker with pid: 34817
[2023-10-04 21:58:06 +0000] [684] [ERROR] Worker (pid:33934) was sent code 134!
[2023-10-04 21:58:06 +0000] [34818] [INFO] Booting worker with pid: 34818
2023-10-04 21:58:12.535694: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:58:12.615400: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 21:58:13.355044: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:58:13.441750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 21:58:15.911851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:15.911851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:15.925910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:15.925909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:15.930317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:15.930547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:19.031145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:19.031179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:19.035690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:19.035949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:19.040167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:19.040431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.224044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.226499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.228495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.230246: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:58:22.338970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.341023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.342848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 21:58:22.344710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 21:58:23.247066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.248346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.250089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.251617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.252636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.254174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.255618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.257005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.258021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.259283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.260675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.261694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.263040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.264076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.265472: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.266890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.267903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.269092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.270726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.271747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.272957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.274144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.275206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 21:58:23.276216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 21:59:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:34817)
[2023-10-04 22:01:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:34818)
[2023-10-04 22:03:20 +0000] [684] [ERROR] Worker (pid:34817) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:03:20 +0000] [34912] [INFO] Booting worker with pid: 34912
[2023-10-04 22:03:20 +0000] [684] [ERROR] Worker (pid:34818) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:03:20 +0000] [34914] [INFO] Booting worker with pid: 34914
2023-10-04 22:03:44.837586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:03:44.837586: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:03:46.787256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:03:46.787256: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:03:49.833436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:49.833436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:49.849487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:49.849487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:49.853879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:49.854126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:03:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:34912)
[2023-10-04 22:03:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:34914)
[2023-10-04 22:03:51 +0000] [684] [ERROR] Worker (pid:34912) was sent code 134!
[2023-10-04 22:03:51 +0000] [35057] [INFO] Booting worker with pid: 35057
[2023-10-04 22:03:51 +0000] [684] [ERROR] Worker (pid:34914) was sent code 134!
[2023-10-04 22:03:51 +0000] [35058] [INFO] Booting worker with pid: 35058
2023-10-04 22:03:57.372921: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:03:57.441253: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:03:58.194339: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:03:58.256573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:03:59.047980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:59.051448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:59.053731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:59.108358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:59.111836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:03:59.113990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:01.429082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:01.429082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:01.433503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:01.433776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:01.437951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:01.438193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.171755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.174151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.176734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.179173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:04:04.180033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.182056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.183902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:04:04.185498: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:04:05.107585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.108854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.109847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.110893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.111883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.112862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.113887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.114958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.115974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.116992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.118037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.119103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.120137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.121174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.122197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.123260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.124274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.125288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.126485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.127534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.128563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.129603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.130660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:04:05.131682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:04:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35057)
[2023-10-04 22:07:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35058)
[2023-10-04 22:09:21 +0000] [684] [ERROR] Worker (pid:35058) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:09:21 +0000] [35148] [INFO] Booting worker with pid: 35148
[2023-10-04 22:09:21 +0000] [684] [ERROR] Worker (pid:35057) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:09:21 +0000] [35149] [INFO] Booting worker with pid: 35149
2023-10-04 22:09:45.191734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:09:45.191734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:09:47.355910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:09:47.355916: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:09:50.658709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:50.658709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:50.673856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:50.673856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:50.678283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:50.678544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:09:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35148)
[2023-10-04 22:09:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35149)
[2023-10-04 22:09:52 +0000] [684] [ERROR] Worker (pid:35149) was sent code 134!
[2023-10-04 22:09:52 +0000] [35291] [INFO] Booting worker with pid: 35291
[2023-10-04 22:09:52 +0000] [684] [ERROR] Worker (pid:35148) was sent code 134!
[2023-10-04 22:09:52 +0000] [35292] [INFO] Booting worker with pid: 35292
2023-10-04 22:09:58.050105: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:09:58.050104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:09:58.863470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:09:58.863482: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:09:59.704924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:59.706086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:59.709504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:59.710399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:59.712931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:09:59.714940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:02.288285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:02.288285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:02.292534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:02.292777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:02.296737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:02.296978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.220186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.223097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.225650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.227998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:10:05.240855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.242961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.244782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:10:05.246526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:10:06.148620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.150259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.151652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.152790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.153854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.155043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.156333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.157714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.159347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.160557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.161646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.162717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.164078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.165196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.166388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.168334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.169445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.170937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.172570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.174210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.175702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.177590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.178686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:10:06.179732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:10:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35291)
[2023-10-04 22:13:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35292)
[2023-10-04 22:13:43 +0000] [684] [ERROR] Worker (pid:35292) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:13:43 +0000] [35409] [INFO] Booting worker with pid: 35409
[2023-10-04 22:13:43 +0000] [684] [ERROR] Worker (pid:35291) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:13:43 +0000] [35410] [INFO] Booting worker with pid: 35410
2023-10-04 22:14:05.917002: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:14:05.917001: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:14:07.701314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:14:07.701314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:14:10.670844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:10.670844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:10.685635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:10.685648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:10.689976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:10.690237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:13.748994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:13.748994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:14:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35409)
[2023-10-04 22:14:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35410)
[2023-10-04 22:14:14 +0000] [684] [ERROR] Worker (pid:35409) was sent code 134!
[2023-10-04 22:14:14 +0000] [35489] [INFO] Booting worker with pid: 35489
[2023-10-04 22:14:14 +0000] [684] [ERROR] Worker (pid:35410) was sent code 134!
[2023-10-04 22:14:14 +0000] [35494] [INFO] Booting worker with pid: 35494
2023-10-04 22:14:20.016931: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:14:20.087376: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:14:20.840340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:14:20.910982: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:14:21.685950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:21.689231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:21.691459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:21.758960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:21.762056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:21.764190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:23.230517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:23.232730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:23.234991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:23.306887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:23.310408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:23.312844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.851678: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.854166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.856045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.857774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:14:25.881266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.883403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.885366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:14:25.887515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:14:26.807641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.808973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.809945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.810973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.812014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.813043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.814071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.815132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.816152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.817160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.818177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.819229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.820241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.821256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.822290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.823397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.824457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.825555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.826643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.827671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.828758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.829815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.830952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:14:26.832008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:14:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35489)
[2023-10-04 22:17:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35494)
[2023-10-04 22:19:02 +0000] [684] [ERROR] Worker (pid:35489) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:19:02 +0000] [35555] [INFO] Booting worker with pid: 35555
[2023-10-04 22:19:03 +0000] [684] [ERROR] Worker (pid:35494) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:19:03 +0000] [35558] [INFO] Booting worker with pid: 35558
2023-10-04 22:19:26.789838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:19:26.789838: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:19:28.637800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:19:28.637801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:19:31.661730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:31.661730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:31.679616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:31.679616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:31.684056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:31.684294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:19:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35555)
[2023-10-04 22:19:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35558)
[2023-10-04 22:19:33 +0000] [684] [ERROR] Worker (pid:35558) was sent code 134!
[2023-10-04 22:19:33 +0000] [35630] [INFO] Booting worker with pid: 35630
[2023-10-04 22:19:33 +0000] [684] [ERROR] Worker (pid:35555) was sent code 134!
[2023-10-04 22:19:33 +0000] [35631] [INFO] Booting worker with pid: 35631
2023-10-04 22:19:39.446343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:19:39.486343: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:19:40.260304: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:19:40.302665: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:19:41.103711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:41.106721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:41.108718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:41.151187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:41.154278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:41.156182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:43.517193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:43.517193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:43.521688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:43.521904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:43.526062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:43.526347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.436667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.439480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.442010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.444458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:19:46.511583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.513364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.515247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:19:46.516967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:19:47.401284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.402388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.403443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.404436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.405433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.406438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.407460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.408463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.409478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.410510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.411554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.412570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.413592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.414641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.415662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.416675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.417714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.418776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.419846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.420942: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.422013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.423527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.424574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:19:47.425741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:20:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35630)
[2023-10-04 22:20:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35631)
[2023-10-04 22:21:01 +0000] [684] [ERROR] Worker (pid:35630) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:21:01 +0000] [35683] [INFO] Booting worker with pid: 35683
[2023-10-04 22:21:01 +0000] [684] [ERROR] Worker (pid:35631) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:21:01 +0000] [35686] [INFO] Booting worker with pid: 35686
2023-10-04 22:21:25.138455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:21:25.138455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:21:26.993099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:21:26.993095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:21:30.198877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:30.198877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:30.217098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:30.217096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:30.221430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:30.221689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:21:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35683)
[2023-10-04 22:21:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35686)
[2023-10-04 22:21:32 +0000] [684] [ERROR] Worker (pid:35683) was sent code 134!
[2023-10-04 22:21:32 +0000] [35755] [INFO] Booting worker with pid: 35755
[2023-10-04 22:21:32 +0000] [684] [ERROR] Worker (pid:35686) was sent code 134!
[2023-10-04 22:21:32 +0000] [35756] [INFO] Booting worker with pid: 35756
2023-10-04 22:21:38.114018: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:21:38.145545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:21:38.924065: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:21:38.967690: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:21:39.761478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:39.764655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:39.766565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:39.816763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:39.820267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:39.822354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:42.132422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:42.132422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:42.136829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:42.137095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:42.141369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:42.141593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.165673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.168496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.170943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.172871: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:21:45.209540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.211596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.213360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:21:45.215231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:21:46.121596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.123348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.124917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.126479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.128076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.129672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.131271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.132844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.134442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.136037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.137598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.139203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.140816: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.142393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.144004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.145580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.147184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.148748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.150317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.151927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.153491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.155093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.156642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:21:46.158183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:22:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35755)
[2023-10-04 22:24:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35756)
[2023-10-04 22:26:30 +0000] [684] [ERROR] Worker (pid:35756) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:26:30 +0000] [35812] [INFO] Booting worker with pid: 35812
[2023-10-04 22:26:30 +0000] [684] [ERROR] Worker (pid:35755) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:26:30 +0000] [35813] [INFO] Booting worker with pid: 35813
2023-10-04 22:26:54.782831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:26:54.782831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:26:56.621136: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:26:56.621132: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:26:59.718068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:26:59.718068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:26:59.736068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:26:59.736068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:26:59.740535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:26:59.740796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:27:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35812)
[2023-10-04 22:27:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35813)
[2023-10-04 22:27:01 +0000] [684] [ERROR] Worker (pid:35813) was sent code 134!
[2023-10-04 22:27:01 +0000] [35958] [INFO] Booting worker with pid: 35958
[2023-10-04 22:27:01 +0000] [684] [ERROR] Worker (pid:35812) was sent code 134!
[2023-10-04 22:27:01 +0000] [35959] [INFO] Booting worker with pid: 35959
2023-10-04 22:27:07.142852: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:27:07.143752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:27:07.952148: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:27:07.953439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:27:08.796309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:08.799594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:08.801648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:08.802030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:08.807395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:08.809489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:11.329116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:11.329162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:11.333565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:11.333817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:11.338050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:11.338304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.369921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.372969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.374862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.376613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:27:14.376672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.378442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.380602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:27:14.382431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:27:15.338458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.339796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.340786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.341750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.342768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.343741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.344700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.345726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.346823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.347856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.348870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.349887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.350982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.352018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.353073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.354096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.355169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.356194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.357218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.358243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.359376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.360479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.361587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:27:15.362712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:27:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35958)
[2023-10-04 22:29:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:35959)
[2023-10-04 22:30:59 +0000] [684] [ERROR] Worker (pid:35958) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:30:59 +0000] [36044] [INFO] Booting worker with pid: 36044
[2023-10-04 22:30:59 +0000] [684] [ERROR] Worker (pid:35959) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:30:59 +0000] [36048] [INFO] Booting worker with pid: 36048
2023-10-04 22:31:23.166712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:31:23.166712: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:31:25.031858: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:31:25.031859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:31:28.049670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:28.049670: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:28.067482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:28.067482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:28.071960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:28.072191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:31:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36044)
[2023-10-04 22:31:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36048)
[2023-10-04 22:31:30 +0000] [684] [ERROR] Worker (pid:36048) was sent code 134!
[2023-10-04 22:31:30 +0000] [36150] [INFO] Booting worker with pid: 36150
[2023-10-04 22:31:30 +0000] [684] [ERROR] Worker (pid:36044) was sent code 134!
[2023-10-04 22:31:30 +0000] [36151] [INFO] Booting worker with pid: 36151
2023-10-04 22:31:36.222548: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:31:36.270063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:31:37.042466: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:31:37.088131: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:31:37.888806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:37.892140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:37.894235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:37.942897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:37.946343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:37.948523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:39.988558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:39.988558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:39.992996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:39.993252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:39.997547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:39.997771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.739384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.741825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.743648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.745324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:31:42.873875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.875759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.877318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:31:42.878961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:31:43.848951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.850062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.851567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.853352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.855324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.856832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.858692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.860151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.861614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.863483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.864892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.866371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.868227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.869838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.871847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.873291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.875332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.876846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.878359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.879775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.881152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.882569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.883985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:31:43.885579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:32:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36150)
[2023-10-04 22:34:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36151)
[2023-10-04 22:35:46 +0000] [684] [ERROR] Worker (pid:36150) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:35:46 +0000] [36226] [INFO] Booting worker with pid: 36226
[2023-10-04 22:35:46 +0000] [684] [ERROR] Worker (pid:36151) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:35:46 +0000] [36227] [INFO] Booting worker with pid: 36227
2023-10-04 22:36:09.089840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:36:09.089840: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:36:10.953341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:36:10.953341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:36:13.872873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:13.872874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:13.887596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:13.887596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:13.891999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:13.892254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:16.734815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:16.734834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:16.739244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:16.739491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:16.743794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:16.744045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:36:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36226)
[2023-10-04 22:36:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36227)
[2023-10-04 22:36:17 +0000] [684] [ERROR] Worker (pid:36226) was sent code 134!
[2023-10-04 22:36:17 +0000] [36318] [INFO] Booting worker with pid: 36318
[2023-10-04 22:36:17 +0000] [684] [ERROR] Worker (pid:36227) was sent code 134!
[2023-10-04 22:36:17 +0000] [36319] [INFO] Booting worker with pid: 36319
2023-10-04 22:36:23.513185: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:36:23.513185: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:36:24.315884: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:36:24.317480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:36:25.150053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:25.151110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:25.154611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:25.155581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:25.157811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:25.159799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:26.685441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:26.687896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:26.690165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:26.690986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:26.696818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:26.699256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.247974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.250157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.251180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.253657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.255762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.258388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:36:29.259681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:36:29.261864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:36:30.184569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.185869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.186923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.187905: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.188912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.189912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.190984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.191986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.192991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.193984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.195057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.196068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.197082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.198108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.199193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.200218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.201247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.202295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.203378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.204405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.205419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.206438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.207489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:36:30.208490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:37:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36318)
[2023-10-04 22:39:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36319)
[2023-10-04 22:42:09 +0000] [684] [ERROR] Worker (pid:36318) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:42:10 +0000] [36375] [INFO] Booting worker with pid: 36375
[2023-10-04 22:42:10 +0000] [684] [ERROR] Worker (pid:36319) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:42:10 +0000] [36377] [INFO] Booting worker with pid: 36377
2023-10-04 22:42:34.062818: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:42:34.062818: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:42:35.826985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:42:35.826984: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:42:38.684127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:38.684126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:38.698358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:38.698367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:38.702699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:38.702963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:42:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36375)
[2023-10-04 22:42:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36377)
[2023-10-04 22:42:40 +0000] [684] [ERROR] Worker (pid:36377) was sent code 134!
[2023-10-04 22:42:40 +0000] [36520] [INFO] Booting worker with pid: 36520
[2023-10-04 22:42:40 +0000] [684] [ERROR] Worker (pid:36375) was sent code 134!
[2023-10-04 22:42:40 +0000] [36521] [INFO] Booting worker with pid: 36521
2023-10-04 22:42:46.896156: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:42:46.912116: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:42:47.723941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:42:47.726769: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:42:48.571362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:48.573604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:48.574316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:48.577975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:48.578058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:48.582455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:50.626551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:50.626551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:50.630988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:50.631237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:50.635391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:50.635638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.211321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.214271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.216768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.218672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:42:53.232494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.235133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.237649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:42:53.240138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:42:54.151974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.153033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.154146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.155236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.156242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.157253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.158285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.159342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.160359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.161379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.162413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.163460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.164500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.165517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.166565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.167598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.168609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.169623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.170677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.171697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.173200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.174230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.175278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:42:54.176295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:43:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36520)
[2023-10-04 22:43:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36521)
[2023-10-04 22:43:19 +0000] [684] [ERROR] Worker (pid:36520) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:43:19 +0000] [36593] [INFO] Booting worker with pid: 36593
[2023-10-04 22:43:19 +0000] [684] [ERROR] Worker (pid:36521) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:43:19 +0000] [36594] [INFO] Booting worker with pid: 36594
2023-10-04 22:43:35.400929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:43:35.400929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:43:37.266810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:43:37.266810: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:43:40.204138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:40.204138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:40.219504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:40.219504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:40.223910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:40.224134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:43.134454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:43.134454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:43.138856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:43.139107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:43.143253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:43.143510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.715427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.718761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.721313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.723806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:43:45.752112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.754070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.756165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:43:45.757759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:43:46.682423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.684090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.685540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.687195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.688705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.690341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.691963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.693289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.694947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.696568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.698195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.699838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.701175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.702535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.704188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.705810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.707461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.709109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.710800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.712443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.714087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.715756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.717291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:43:46.718495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:44:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36593)
[2023-10-04 22:46:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36594)
[2023-10-04 22:47:10 +0000] [684] [ERROR] Worker (pid:36593) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:47:10 +0000] [36660] [INFO] Booting worker with pid: 36660
[2023-10-04 22:47:11 +0000] [684] [ERROR] Worker (pid:36594) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:47:11 +0000] [36663] [INFO] Booting worker with pid: 36663
2023-10-04 22:47:34.435698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:47:34.435698: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:47:36.180207: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:47:36.180212: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:47:39.183977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:39.183977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:39.200987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:39.200987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:39.205398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:39.205636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:47:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36660)
[2023-10-04 22:47:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36663)
[2023-10-04 22:47:41 +0000] [684] [ERROR] Worker (pid:36660) was sent code 134!
[2023-10-04 22:47:41 +0000] [36732] [INFO] Booting worker with pid: 36732
[2023-10-04 22:47:41 +0000] [684] [ERROR] Worker (pid:36663) was sent code 134!
[2023-10-04 22:47:41 +0000] [36733] [INFO] Booting worker with pid: 36733
2023-10-04 22:47:47.649585: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:47:47.661029: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:47:48.460738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:47:48.474910: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:47:49.304379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:49.307663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:49.309916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:49.312463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:49.315609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:49.317805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:51.362986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:51.362986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:51.367437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:51.367655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:51.371897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:51.372126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.964425: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.967252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.969486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.971711: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:47:53.987845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.989981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.991854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:47:53.993684: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:47:54.896907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.898308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.899424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.900432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.901440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.902467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.903515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.904529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.905555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.906575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.907602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.908593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.909620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.910659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.911758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.912803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.913812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.914858: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.915851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.916850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.917863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.918927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.919930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:47:54.920937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:48:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36732)
[2023-10-04 22:50:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36733)
[2023-10-04 22:52:33 +0000] [684] [ERROR] Worker (pid:36732) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:52:33 +0000] [36788] [INFO] Booting worker with pid: 36788
[2023-10-04 22:52:33 +0000] [684] [ERROR] Worker (pid:36733) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:52:33 +0000] [36790] [INFO] Booting worker with pid: 36790
2023-10-04 22:52:57.390380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:52:57.390379: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:52:59.314480: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:52:59.314481: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:53:02.620835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:02.620835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:02.640317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:02.640317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:02.644725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:02.644988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:53:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36788)
[2023-10-04 22:53:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36790)
[2023-10-04 22:53:03 +0000] [684] [ERROR] Worker (pid:36790) was sent code 134!
[2023-10-04 22:53:03 +0000] [36930] [INFO] Booting worker with pid: 36930
[2023-10-04 22:53:03 +0000] [684] [ERROR] Worker (pid:36788) was sent code 134!
[2023-10-04 22:53:03 +0000] [36931] [INFO] Booting worker with pid: 36931
2023-10-04 22:53:09.807969: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:53:09.816541: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:53:10.618732: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:53:10.621909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:53:11.455601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:11.458857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:11.460791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:11.462376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:11.465991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:11.468074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:14.001872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:14.001885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:14.006291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:14.006535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:14.010866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:14.011128: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.506518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.509663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.512574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.515347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19026 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:53:16.712048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.713978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.715872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:53:16.717426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:53:17.737754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.739207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.740451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.742462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.744018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.745077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.747316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.748873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.750687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.752588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.754218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.755930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.757997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.759556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.760620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.762151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.764118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.765878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.767738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.768887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.770320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.771396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.772854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.774010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:53:17.775105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:53:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36930)
[2023-10-04 22:55:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:36931)
[2023-10-04 22:57:39 +0000] [684] [ERROR] Worker (pid:36930) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:57:39 +0000] [37020] [INFO] Booting worker with pid: 37020
[2023-10-04 22:57:39 +0000] [684] [ERROR] Worker (pid:36931) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 22:57:39 +0000] [37023] [INFO] Booting worker with pid: 37023
2023-10-04 22:58:03.595857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:58:03.595857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:58:05.363579: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:58:05.363578: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:58:08.311166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:08.311166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:08.327428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:08.327428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:08.331740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:08.331992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 22:58:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37020)
[2023-10-04 22:58:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37023)
[2023-10-04 22:58:10 +0000] [684] [ERROR] Worker (pid:37023) was sent code 134!
[2023-10-04 22:58:10 +0000] [37165] [INFO] Booting worker with pid: 37165
[2023-10-04 22:58:10 +0000] [684] [ERROR] Worker (pid:37020) was sent code 134!
[2023-10-04 22:58:10 +0000] [37166] [INFO] Booting worker with pid: 37166
2023-10-04 22:58:16.178704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:58:16.216947: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 22:58:16.994348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:58:17.032244: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 22:58:17.841015: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:17.844369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:17.846455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:17.885334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:17.888627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:17.890715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:20.118387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:20.118387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:20.122825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:20.123088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:20.127292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:20.127548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.888424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.891024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.893244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.895441: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:58:22.904977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.906971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.908780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 22:58:22.910683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 22:58:23.832862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.834150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.835192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.836181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.837169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.838152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.839194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.840184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.841175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.842377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.843526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.845055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.846375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.847430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.848593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.849675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.851003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.852261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.853569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.855166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.856476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.857820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.858927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 22:58:23.860333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 22:58:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37165)
[2023-10-04 23:00:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37166)
[2023-10-04 23:03:06 +0000] [684] [ERROR] Worker (pid:37165) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:03:06 +0000] [37255] [INFO] Booting worker with pid: 37255
[2023-10-04 23:03:06 +0000] [684] [ERROR] Worker (pid:37166) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:03:06 +0000] [37257] [INFO] Booting worker with pid: 37257
2023-10-04 23:03:30.587990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:03:30.587990: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:03:32.436554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:03:32.436559: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:03:35.555893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:35.555893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:35.571535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:35.571546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:35.575889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:35.576136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:03:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37255)
[2023-10-04 23:03:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37257)
[2023-10-04 23:03:37 +0000] [684] [ERROR] Worker (pid:37257) was sent code 134!
[2023-10-04 23:03:37 +0000] [37397] [INFO] Booting worker with pid: 37397
[2023-10-04 23:03:37 +0000] [684] [ERROR] Worker (pid:37255) was sent code 134!
[2023-10-04 23:03:37 +0000] [37398] [INFO] Booting worker with pid: 37398
2023-10-04 23:03:43.164277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:03:43.205521: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:03:43.977355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:03:44.013688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:03:44.828791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:44.832116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:44.834194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:44.858523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:44.861640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:44.863514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:47.224161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:47.224169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:47.228802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:47.229057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:47.233379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:47.233639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.940596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.943209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.945439: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.947359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:03:49.958226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.960279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.962131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:03:49.963989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:03:50.884449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.885752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.886804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.887802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.888787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.890068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.891549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.893126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.894174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.895367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.896462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.897521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.899115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.900359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.901541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.902749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.903887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.904945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.905995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.907680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.908955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.910002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.911112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:03:50.912193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:04:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37397)
[2023-10-04 23:06:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37398)
[2023-10-04 23:08:02 +0000] [684] [ERROR] Worker (pid:37397) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:08:02 +0000] [37459] [INFO] Booting worker with pid: 37459
[2023-10-04 23:08:02 +0000] [684] [ERROR] Worker (pid:37398) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:08:02 +0000] [37461] [INFO] Booting worker with pid: 37461
2023-10-04 23:08:25.550510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:08:25.550510: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:08:27.364677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:08:27.364677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:08:30.180576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:30.180576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:30.193855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:30.193855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:30.198192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:30.198444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:08:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37459)
[2023-10-04 23:08:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37461)
[2023-10-04 23:08:32 +0000] [684] [ERROR] Worker (pid:37459) was sent code 134!
[2023-10-04 23:08:32 +0000] [37531] [INFO] Booting worker with pid: 37531
[2023-10-04 23:08:32 +0000] [684] [ERROR] Worker (pid:37461) was sent code 134!
[2023-10-04 23:08:32 +0000] [37532] [INFO] Booting worker with pid: 37532
2023-10-04 23:08:38.678478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:08:38.732212: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:08:39.495890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:08:39.541780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:08:40.340087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:40.343395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:40.345548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:40.382526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:40.385526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:40.387715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:42.188646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:42.188646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:42.193203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:42.193415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:42.197695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:42.197918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.952930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.955480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.957534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.959665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:08:44.968640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.970760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.972645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:08:44.974340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:08:45.898878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.900559: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.901592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.902686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.903689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.904677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.905668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.906705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.907700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.908696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.909962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.911222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.912891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.914545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.916205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.917826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.919152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.920214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.921354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.922427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.923580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.924663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.925733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:08:45.926845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:09:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37531)
[2023-10-04 23:09:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37532)
[2023-10-04 23:09:51 +0000] [684] [ERROR] Worker (pid:37531) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:09:51 +0000] [37585] [INFO] Booting worker with pid: 37585
[2023-10-04 23:09:51 +0000] [684] [ERROR] Worker (pid:37532) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:09:51 +0000] [37586] [INFO] Booting worker with pid: 37586
2023-10-04 23:10:14.929221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:10:14.929221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:10:16.754498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:10:16.754499: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:10:19.650255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:19.650256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:19.666297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:19.666297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:19.670729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:19.670986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:10:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37585)
[2023-10-04 23:10:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37586)
[2023-10-04 23:10:22 +0000] [684] [ERROR] Worker (pid:37585) was sent code 134!
[2023-10-04 23:10:22 +0000] [37685] [INFO] Booting worker with pid: 37685
[2023-10-04 23:10:22 +0000] [684] [ERROR] Worker (pid:37586) was sent code 134!
[2023-10-04 23:10:22 +0000] [37686] [INFO] Booting worker with pid: 37686
2023-10-04 23:10:28.002472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:10:28.075790: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:10:28.813694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:10:28.892125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:10:29.662545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:29.665774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:29.667842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:29.736588: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:29.739723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:29.741950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:31.611601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:31.611716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:31.616111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:31.616356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:31.620608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:31.620842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.329577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.331993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.334079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.336096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:10:34.356660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.358535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.360289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:10:34.361898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:10:35.266700: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.268002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.269026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.270054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.271120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.272243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.273615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.275026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.276656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.277870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.279251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.280601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.282284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.283709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.285042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.286708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.287930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.288992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.290390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.292117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.293742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.295387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.297026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:10:35.298709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:11:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37685)
[2023-10-04 23:13:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37686)
[2023-10-04 23:14:58 +0000] [684] [ERROR] Worker (pid:37685) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:14:58 +0000] [37743] [INFO] Booting worker with pid: 37743
[2023-10-04 23:14:58 +0000] [684] [ERROR] Worker (pid:37686) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:14:58 +0000] [37744] [INFO] Booting worker with pid: 37744
2023-10-04 23:15:22.414547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:15:22.414547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:15:24.250859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:15:24.250860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:15:27.432394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:27.432394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:27.450866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:27.450866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:27.455194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:27.455449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:15:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37743)
[2023-10-04 23:15:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37744)
[2023-10-04 23:15:28 +0000] [684] [ERROR] Worker (pid:37744) was sent code 134!
[2023-10-04 23:15:28 +0000] [37919] [INFO] Booting worker with pid: 37919
[2023-10-04 23:15:28 +0000] [684] [ERROR] Worker (pid:37743) was sent code 134!
[2023-10-04 23:15:28 +0000] [37920] [INFO] Booting worker with pid: 37920
2023-10-04 23:15:34.837209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:15:34.897666: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:15:35.649275: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:15:35.717396: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:15:36.501716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:36.505014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:36.507373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:36.572420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:36.575578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:36.577819: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:39.026014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:39.026048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:39.030720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:39.030904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:39.035212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:39.035466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.794197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.796212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.798152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.800215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:15:41.840951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.842743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.844482: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:15:41.846076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:15:42.738513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.739777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.740830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.741989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.743520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.744978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.746174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.747304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.748623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.750101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.751721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.753046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.754340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.755680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.757076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.758437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.759861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.760911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.761976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.763410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.764573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.765726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.766814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:15:42.767886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:16:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37919)
[2023-10-04 23:18:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37920)
[2023-10-04 23:20:34 +0000] [684] [ERROR] Worker (pid:37919) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:20:34 +0000] [37981] [INFO] Booting worker with pid: 37981
[2023-10-04 23:20:35 +0000] [684] [ERROR] Worker (pid:37920) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:20:35 +0000] [37985] [INFO] Booting worker with pid: 37985
2023-10-04 23:20:58.889097: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:20:58.889097: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:21:00.676259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:21:00.676259: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:21:03.687568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:03.687568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:03.702813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:03.702841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:03.707061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:03.707317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:21:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37981)
[2023-10-04 23:21:05 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:37985)
[2023-10-04 23:21:05 +0000] [684] [ERROR] Worker (pid:37985) was sent code 134!
[2023-10-04 23:21:05 +0000] [38131] [INFO] Booting worker with pid: 38131
[2023-10-04 23:21:05 +0000] [684] [ERROR] Worker (pid:37981) was sent code 134!
[2023-10-04 23:21:05 +0000] [38132] [INFO] Booting worker with pid: 38132
2023-10-04 23:21:11.564109: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:21:11.596231: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:21:12.376295: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:21:12.400551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:21:13.219013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:13.222123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:13.224186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:13.244240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:13.247211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:13.249161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:15.474870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:15.474870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:15.479404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:15.479657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:15.483834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:15.484081: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.154204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.156488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.158302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.160200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:21:18.204672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.206582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.208461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:21:18.210184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:21:19.099608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.100894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.101872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.102899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.103861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.104827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.105800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.106843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.107872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.108886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.109902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.110952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.111974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.112998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.114031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.115121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.116132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.117136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.118138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.119186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.120178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.121185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.122190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:21:19.123246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:21:51 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38131)
[2023-10-04 23:23:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38132)
[2023-10-04 23:25:44 +0000] [684] [ERROR] Worker (pid:38131) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:25:44 +0000] [38220] [INFO] Booting worker with pid: 38220
[2023-10-04 23:25:44 +0000] [684] [ERROR] Worker (pid:38132) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:25:44 +0000] [38221] [INFO] Booting worker with pid: 38221
2023-10-04 23:26:08.240117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:26:08.240117: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:26:10.012814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:26:10.012812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:26:12.859058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:12.859058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:12.875754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:12.875754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:12.880076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:12.880327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:26:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38220)
[2023-10-04 23:26:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38221)
[2023-10-04 23:26:14 +0000] [684] [ERROR] Worker (pid:38221) was sent code 134!
[2023-10-04 23:26:14 +0000] [38361] [INFO] Booting worker with pid: 38361
[2023-10-04 23:26:14 +0000] [684] [ERROR] Worker (pid:38220) was sent code 134!
[2023-10-04 23:26:14 +0000] [38362] [INFO] Booting worker with pid: 38362
2023-10-04 23:26:20.894728: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:26:21.711000: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:26:21.917076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:26:22.558078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:22.561392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:22.563645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:22.723935: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:26:23.556820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:23.560180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:23.562329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:24.640710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:24.642892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:24.644871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:25.110857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:25.113408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:25.115580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.525575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.527933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.530143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.532119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:26:27.586352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.588305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.590050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:26:27.591787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:26:28.487013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.488041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.489092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.490101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.491226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.492276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.493313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.494356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.495415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.496431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.497450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.498473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.499547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.500563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.501597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.502677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.503702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.504722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.505751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.507037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.508824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.509847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.510909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:26:28.511927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:26:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38361)
[2023-10-04 23:28:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38362)
[2023-10-04 23:30:56 +0000] [684] [ERROR] Worker (pid:38361) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:30:56 +0000] [38450] [INFO] Booting worker with pid: 38450
[2023-10-04 23:30:56 +0000] [684] [ERROR] Worker (pid:38362) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:30:56 +0000] [38455] [INFO] Booting worker with pid: 38455
2023-10-04 23:31:20.051889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:31:20.051889: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:31:21.869091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:31:21.869088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:31:24.914751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:24.914751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:24.930540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:24.930539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:24.935144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:24.935368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:31:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38450)
[2023-10-04 23:31:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38455)
[2023-10-04 23:31:26 +0000] [684] [ERROR] Worker (pid:38455) was sent code 134!
[2023-10-04 23:31:26 +0000] [38627] [INFO] Booting worker with pid: 38627
[2023-10-04 23:31:26 +0000] [684] [ERROR] Worker (pid:38450) was sent code 134!
[2023-10-04 23:31:26 +0000] [38628] [INFO] Booting worker with pid: 38628
2023-10-04 23:31:32.699467: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:31:32.755436: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:31:33.522541: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:31:33.571283: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:31:34.371403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:34.374718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:34.376829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:34.407557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:34.410697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:34.412611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:36.788843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:36.788989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:36.793331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:36.793582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:36.797804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:36.798061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.519661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.522371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.524833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.527347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:31:39.536188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.538036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.539764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:31:39.541585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:31:40.454904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.456262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.457338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.458392: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.459443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.460479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.461596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.462735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.463814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.464859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.465926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.467023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.468068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.469178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.470344: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.471463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.472562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.473629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.474691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.475788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.476843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.477900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.479049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:31:40.480121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:32:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38627)
[2023-10-04 23:34:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38628)
[2023-10-04 23:36:03 +0000] [684] [ERROR] Worker (pid:38627) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:36:03 +0000] [38683] [INFO] Booting worker with pid: 38683
[2023-10-04 23:36:03 +0000] [684] [ERROR] Worker (pid:38628) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:36:03 +0000] [38684] [INFO] Booting worker with pid: 38684
2023-10-04 23:36:27.711339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:36:27.711339: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:36:29.437005: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:36:29.437001: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:36:32.257206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:32.257206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:32.271530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:32.271530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:32.276033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:32.276294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:36:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38683)
[2023-10-04 23:36:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38684)
[2023-10-04 23:36:33 +0000] [684] [ERROR] Worker (pid:38684) was sent code 134!
[2023-10-04 23:36:33 +0000] [38829] [INFO] Booting worker with pid: 38829
[2023-10-04 23:36:33 +0000] [684] [ERROR] Worker (pid:38683) was sent code 134!
[2023-10-04 23:36:33 +0000] [38830] [INFO] Booting worker with pid: 38830
2023-10-04 23:36:39.716553: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:36:39.749351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:36:40.531991: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:36:40.569370: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:36:41.378236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:41.381476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:41.383580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:41.416122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:41.419459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:41.421368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:43.614366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:43.614374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:43.618951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:43.619207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:43.623577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:43.623830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.355379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.355379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.360506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.360786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.365494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.365772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:36:46.370538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:36:46.370810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:36:47.485318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.486642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.487600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.488547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.489488: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.490447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.491552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.492784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.494035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.495210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.496441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.498008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.499259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.500386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.501729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.503132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.504231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.505375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.506548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.508121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.509174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.510238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.511360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:36:47.512418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:37:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38829)
[2023-10-04 23:38:44 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38830)
[2023-10-04 23:38:56 +0000] [684] [ERROR] Worker (pid:38830) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:38:56 +0000] [38888] [INFO] Booting worker with pid: 38888
[2023-10-04 23:38:56 +0000] [684] [ERROR] Worker (pid:38829) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:38:56 +0000] [38889] [INFO] Booting worker with pid: 38889
2023-10-04 23:39:20.016214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:39:20.016214: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:39:21.887801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:39:21.887803: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:39:24.973417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:24.973417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:24.990921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:24.990921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:24.995401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:24.995660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:39:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38888)
[2023-10-04 23:39:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38889)
[2023-10-04 23:39:27 +0000] [684] [ERROR] Worker (pid:38888) was sent code 134!
[2023-10-04 23:39:27 +0000] [38988] [INFO] Booting worker with pid: 38988
[2023-10-04 23:39:27 +0000] [684] [ERROR] Worker (pid:38889) was sent code 134!
[2023-10-04 23:39:27 +0000] [38989] [INFO] Booting worker with pid: 38989
2023-10-04 23:39:33.140512: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:39:33.377693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:39:33.958585: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:39:34.205985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:39:34.808205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:34.811531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:34.813829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:35.066557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:35.069807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:35.072014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:36.680460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:36.680460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:36.685101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:36.685325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:36.689629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:36.689891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.310379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.313381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.315560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.318119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:39:39.381634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.384069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.386648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:39:39.389155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:39:40.502164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.503362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.504414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.505457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.507274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.508810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.510427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.512427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.513966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.515970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.518038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.519698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.521180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.522780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.524722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.526730: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.528398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.529995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.531557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.533529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.535088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.536739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.537769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:39:40.538849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:40:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38989)
[2023-10-04 23:42:22 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:38988)
[2023-10-04 23:44:20 +0000] [684] [ERROR] Worker (pid:38989) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:44:20 +0000] [39046] [INFO] Booting worker with pid: 39046
[2023-10-04 23:44:20 +0000] [684] [ERROR] Worker (pid:38988) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:44:20 +0000] [39050] [INFO] Booting worker with pid: 39050
2023-10-04 23:44:44.269641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:44:44.269641: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:44:46.029197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:44:46.029196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:44:48.887026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:48.887026: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:48.901600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:48.901600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:48.905940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:48.906217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:44:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39046)
[2023-10-04 23:44:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39050)
[2023-10-04 23:44:51 +0000] [684] [ERROR] Worker (pid:39050) was sent code 134!
[2023-10-04 23:44:51 +0000] [39194] [INFO] Booting worker with pid: 39194
[2023-10-04 23:44:51 +0000] [684] [ERROR] Worker (pid:39046) was sent code 134!
[2023-10-04 23:44:51 +0000] [39195] [INFO] Booting worker with pid: 39195
2023-10-04 23:44:57.027026: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:44:57.135858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:44:57.840755: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:44:57.947100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:44:58.699650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:58.703006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:58.705223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:58.789964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:58.793777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:44:58.796215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:00.523671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:00.523672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:00.528179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:00.528413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:00.532638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:00.532896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.174173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.176796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.179049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.181235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:45:03.227530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.229519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.231383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:45:03.233265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:45:04.324373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.326060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.327526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.328699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.330098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.331787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.333092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.334327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.335506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.337020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.338759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.340469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.342202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.343644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.345357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.347051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.348360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.349566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.350806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.351935: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.353084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.354257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.355447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:45:04.356615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:45:41 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39194)
[2023-10-04 23:47:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39195)
[2023-10-04 23:50:10 +0000] [684] [ERROR] Worker (pid:39194) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:50:10 +0000] [39274] [INFO] Booting worker with pid: 39274
[2023-10-04 23:50:10 +0000] [684] [ERROR] Worker (pid:39195) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:50:10 +0000] [39284] [INFO] Booting worker with pid: 39284
2023-10-04 23:50:34.589053: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:50:34.589053: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:50:36.339812: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:50:36.339814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:50:39.164033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:39.164033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:39.179335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:39.179335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:39.183943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:39.184160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:50:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39274)
[2023-10-04 23:50:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39284)
[2023-10-04 23:50:41 +0000] [684] [ERROR] Worker (pid:39284) was sent code 134!
[2023-10-04 23:50:41 +0000] [39430] [INFO] Booting worker with pid: 39430
[2023-10-04 23:50:41 +0000] [684] [ERROR] Worker (pid:39274) was sent code 134!
[2023-10-04 23:50:41 +0000] [39431] [INFO] Booting worker with pid: 39431
2023-10-04 23:50:47.078623: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:50:47.099798: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:50:47.892833: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:50:47.921957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:50:48.740631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:48.744046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:48.746284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:48.782813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:48.786042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:48.788152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:50.759514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:50.759679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:50.764196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:50.764837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:50.769374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:50.769764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.433258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.436040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.437900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.439675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:50:53.536433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.538218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.539848: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:50:53.541449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:50:54.639319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.640568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.641591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.642651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.643671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.645049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.646420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.647789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.649166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.650184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.651918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.653287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.654325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.655374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.657196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.658613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.659639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.661021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.662759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.664136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.665154: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.666897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.667930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:50:54.668989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:51:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39430)
[2023-10-04 23:53:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39431)
[2023-10-04 23:55:15 +0000] [684] [ERROR] Worker (pid:39430) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:55:15 +0000] [39490] [INFO] Booting worker with pid: 39490
[2023-10-04 23:55:15 +0000] [684] [ERROR] Worker (pid:39431) was sent SIGKILL! Perhaps out of memory?
[2023-10-04 23:55:15 +0000] [39493] [INFO] Booting worker with pid: 39493
2023-10-04 23:55:38.959193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:55:38.959193: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:55:40.680776: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:55:40.680780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:55:43.424770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:43.424770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:43.439246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:43.439267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:43.443587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:43.443840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-04 23:55:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39490)
[2023-10-04 23:55:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39493)
[2023-10-04 23:55:45 +0000] [684] [ERROR] Worker (pid:39490) was sent code 134!
[2023-10-04 23:55:45 +0000] [39633] [INFO] Booting worker with pid: 39633
[2023-10-04 23:55:45 +0000] [684] [ERROR] Worker (pid:39493) was sent code 134!
[2023-10-04 23:55:45 +0000] [39634] [INFO] Booting worker with pid: 39634
2023-10-04 23:55:51.849251: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:55:51.930837: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-04 23:55:52.670423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:55:52.753105: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-04 23:55:53.512468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:53.515904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:53.517898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:53.604621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:53.607715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:53.609696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:55.153631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:55.153631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:55.158016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:55.158271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:55.162429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:55.162667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.675669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.677775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.680055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.681690: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:55:57.753327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.755467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.757118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-04 23:55:57.758952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-04 23:55:58.885723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.887315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.889209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.890831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.892887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.894971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.897310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.899326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.901377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.902969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.904886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.906416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.908524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.910487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.912528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.914556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.916984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.919497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.921084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.922983: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.924480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.926190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.927352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-04 23:55:58.928444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-04 23:56:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39633)
[2023-10-04 23:58:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39634)
[2023-10-05 00:00:38 +0000] [684] [ERROR] Worker (pid:39633) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:00:38 +0000] [39693] [INFO] Booting worker with pid: 39693
[2023-10-05 00:00:39 +0000] [684] [ERROR] Worker (pid:39634) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:00:39 +0000] [39697] [INFO] Booting worker with pid: 39697
2023-10-05 00:01:03.326825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:01:03.326825: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:01:05.094905: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:01:05.094903: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:01:08.040811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:08.040811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:08.057168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:08.057168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:08.061706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:08.061964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:01:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39693)
[2023-10-05 00:01:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39697)
[2023-10-05 00:01:09 +0000] [684] [ERROR] Worker (pid:39697) was sent code 134!
[2023-10-05 00:01:09 +0000] [39960] [INFO] Booting worker with pid: 39960
[2023-10-05 00:01:09 +0000] [684] [ERROR] Worker (pid:39693) was sent code 134!
[2023-10-05 00:01:09 +0000] [39961] [INFO] Booting worker with pid: 39961
2023-10-05 00:01:15.647533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:01:15.647533: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:01:16.454546: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:01:16.474646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:01:17.291276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:17.294575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:17.296772: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:17.317830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:17.320833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:17.322925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:19.597285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:19.597284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:19.601686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:19.601940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:19.606134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:19.606387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.320295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.322632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.324822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.326937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:01:22.350006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.352090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.354087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:01:22.355915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:01:23.465595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.466750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.467738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.468729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.469967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.471152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.472266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.473387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.474473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.475562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.476595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.477742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.478899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.479952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.481973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.483146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.484656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.486136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.487706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.489211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.491309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.493387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.494491: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:01:23.495545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:02:03 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39960)
[2023-10-05 00:03:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:39961)
[2023-10-05 00:05:37 +0000] [684] [ERROR] Worker (pid:39960) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:05:37 +0000] [40049] [INFO] Booting worker with pid: 40049
[2023-10-05 00:05:37 +0000] [684] [ERROR] Worker (pid:39961) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:05:37 +0000] [40051] [INFO] Booting worker with pid: 40051
2023-10-05 00:06:00.391351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:06:00.391351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:06:02.201312: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:06:02.201314: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:06:05.086775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:05.086775: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:05.104253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:05.104253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:05.108681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:05.108911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:06:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40049)
[2023-10-05 00:06:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40051)
[2023-10-05 00:06:07 +0000] [684] [ERROR] Worker (pid:40051) was sent code 134!
[2023-10-05 00:06:07 +0000] [40121] [INFO] Booting worker with pid: 40121
[2023-10-05 00:06:07 +0000] [684] [ERROR] Worker (pid:40049) was sent code 134!
[2023-10-05 00:06:07 +0000] [40122] [INFO] Booting worker with pid: 40122
2023-10-05 00:06:13.663124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:06:13.669531: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:06:14.473450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:06:14.486020: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:06:15.328508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:15.331721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:15.333694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:15.340978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:15.344260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:15.346340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:16.977544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:16.977681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:16.981985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:16.982229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:16.986396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:16.986662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.721462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.724020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.726173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.728382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:06:19.773215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.775294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.777044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:06:19.778765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:06:20.875575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.876671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.877710: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.878790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.879807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.880820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.881842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.882956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.883987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.885010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.886017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.887109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.888147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.889187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.890215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.891300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.893109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.894692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.895728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.897824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.898897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.899911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.901030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:06:20.902222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:06:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40122)
[2023-10-05 00:08:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40121)
[2023-10-05 00:10:39 +0000] [684] [ERROR] Worker (pid:40122) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:10:39 +0000] [40203] [INFO] Booting worker with pid: 40203
[2023-10-05 00:10:39 +0000] [684] [ERROR] Worker (pid:40121) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:10:39 +0000] [40209] [INFO] Booting worker with pid: 40209
2023-10-05 00:11:02.960050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:11:02.960050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:11:04.704191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:11:04.704191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:11:07.629211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:07.629211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:07.643735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:07.643750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:07.648058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:07.648312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:11:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40203)
[2023-10-05 00:11:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40209)
[2023-10-05 00:11:10 +0000] [684] [ERROR] Worker (pid:40209) was sent code 134!
[2023-10-05 00:11:10 +0000] [40295] [INFO] Booting worker with pid: 40295
[2023-10-05 00:11:10 +0000] [684] [ERROR] Worker (pid:40203) was sent code 134!
[2023-10-05 00:11:10 +0000] [40301] [INFO] Booting worker with pid: 40301
2023-10-05 00:11:16.364274: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:11:16.364274: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:11:17.172336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:11:17.185548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:11:18.008840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:18.011976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:18.013971: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:18.028898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:18.032184: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:18.034351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:19.631737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:19.631761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:19.636755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:19.637021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:19.641667: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:19.642251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.198887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.201068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.202927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.205231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:11:22.207160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.208981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.210637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:11:22.212140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:11:23.323840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.325115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.326095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.327137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.328105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.329086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.330279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.331475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.332585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.333669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.334806: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.335918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.337393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.338954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.340023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.341186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.342518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.344047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.345656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.347028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.348639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.350261: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.351893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:11:23.353360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:11:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40295)
[2023-10-05 00:13:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40301)
[2023-10-05 00:16:17 +0000] [684] [ERROR] Worker (pid:40295) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:16:17 +0000] [40365] [INFO] Booting worker with pid: 40365
[2023-10-05 00:16:17 +0000] [684] [ERROR] Worker (pid:40301) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:16:17 +0000] [40368] [INFO] Booting worker with pid: 40368
2023-10-05 00:16:41.589084: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:16:41.589081: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:16:43.336535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:16:43.336535: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:16:46.172213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:46.172213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:46.187190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:46.187190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:46.191583: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:46.191835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:16:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40365)
[2023-10-05 00:16:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40368)
[2023-10-05 00:16:48 +0000] [684] [ERROR] Worker (pid:40365) was sent code 134!
[2023-10-05 00:16:48 +0000] [40511] [INFO] Booting worker with pid: 40511
[2023-10-05 00:16:48 +0000] [684] [ERROR] Worker (pid:40368) was sent code 134!
[2023-10-05 00:16:48 +0000] [40512] [INFO] Booting worker with pid: 40512
2023-10-05 00:16:54.240952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:16:54.288391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:16:55.061557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:16:55.104373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:16:55.915895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:55.919345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:55.921371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:55.952129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:55.955300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:55.957186: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:57.831420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:57.831604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:57.835948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:57.836159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:57.840370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:16:57.840619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.537313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.540008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.542284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.545065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19016 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:17:00.618224: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.620089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.621776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:17:00.623556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:17:01.729386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.730514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.731576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.732595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.733644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.735314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.736370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.737818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.739223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.740978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.742391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.743476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.744875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.746231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.747645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.749423: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.750450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.752155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.753199: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.754217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.756263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.757387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.758524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.759587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:17:01.760603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:17:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40511)
[2023-10-05 00:19:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40512)
[2023-10-05 00:21:43 +0000] [684] [ERROR] Worker (pid:40511) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:21:43 +0000] [40597] [INFO] Booting worker with pid: 40597
[2023-10-05 00:21:43 +0000] [684] [ERROR] Worker (pid:40512) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:21:43 +0000] [40600] [INFO] Booting worker with pid: 40600
2023-10-05 00:22:07.756390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:22:07.756390: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:22:09.588091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:22:09.588091: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:22:12.567432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:12.567432: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:12.581785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:12.581785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:12.586146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:12.586407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:22:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40597)
[2023-10-05 00:22:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40600)
[2023-10-05 00:22:14 +0000] [684] [ERROR] Worker (pid:40600) was sent code 134!
[2023-10-05 00:22:14 +0000] [40752] [INFO] Booting worker with pid: 40752
[2023-10-05 00:22:14 +0000] [684] [ERROR] Worker (pid:40597) was sent code 134!
[2023-10-05 00:22:14 +0000] [40753] [INFO] Booting worker with pid: 40753
2023-10-05 00:22:20.245963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:22:20.277521: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:22:21.072715: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:22:21.084590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:22:21.917973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:21.921131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:21.923285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:21.928087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:21.931427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:21.933536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:24.161918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:24.161918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:24.166329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:24.166611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:24.170951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:24.171205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.909279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.912140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.914693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.917167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:22:26.966669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.969419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.971996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:22:26.974485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:22:28.091574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.092838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.094007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.095156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.096257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.097349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.098436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.099606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.100722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.102064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.103222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.104289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.106167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.107850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.109434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.111099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.112647: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.113950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.115102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.116222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.117852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.119889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.121000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:22:28.122112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:22:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40753)
[2023-10-05 00:24:37 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40752)
[2023-10-05 00:24:47 +0000] [684] [ERROR] Worker (pid:40753) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:24:47 +0000] [40842] [INFO] Booting worker with pid: 40842
[2023-10-05 00:24:48 +0000] [684] [ERROR] Worker (pid:40752) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:24:48 +0000] [40843] [INFO] Booting worker with pid: 40843
2023-10-05 00:25:10.485897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:25:10.485897: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:25:12.254749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:25:12.254749: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:25:15.089140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:15.089140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:15.101788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:15.101787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:15.106145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:15.106405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:17.798737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:17.798737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:17.803165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:17.803418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:17.807691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:17.807927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:25:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40842)
[2023-10-05 00:25:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40843)
[2023-10-05 00:25:18 +0000] [684] [ERROR] Worker (pid:40843) was sent code 134!
[2023-10-05 00:25:18 +0000] [40954] [INFO] Booting worker with pid: 40954
[2023-10-05 00:25:18 +0000] [684] [ERROR] Worker (pid:40842) was sent code 134!
[2023-10-05 00:25:18 +0000] [40955] [INFO] Booting worker with pid: 40955
2023-10-05 00:25:24.482483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:25:24.482483: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:25:25.291849: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:25:25.297943: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:25:26.135225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:26.138404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:26.140393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:26.147422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:26.150682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:26.152768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:27.488099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:27.490460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:27.492210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:27.507718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:27.516450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:27.518697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.093542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.096290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.098751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.101131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:25:30.143679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.145880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.148051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:25:30.149848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:25:31.254886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.255964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.256981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.257986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.259051: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.260078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.261105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.262144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.263306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.264321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.265338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.266388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.267462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.268501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.269523: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.270556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.271589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.272614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.273629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.274680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.275697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.276712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.277718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:25:31.278776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:26:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40954)
[2023-10-05 00:28:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:40955)
[2023-10-05 00:30:28 +0000] [684] [ERROR] Worker (pid:40954) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:30:28 +0000] [41014] [INFO] Booting worker with pid: 41014
[2023-10-05 00:30:28 +0000] [684] [ERROR] Worker (pid:40955) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:30:28 +0000] [41017] [INFO] Booting worker with pid: 41017
2023-10-05 00:30:52.445102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:30:52.445102: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:30:54.184828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:30:54.184828: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:30:57.088133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:30:57.088133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:30:57.102040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:30:57.102044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:30:57.106448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:30:57.106709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:30:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41014)
[2023-10-05 00:30:58 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41017)
[2023-10-05 00:30:59 +0000] [684] [ERROR] Worker (pid:41017) was sent code 134!
[2023-10-05 00:30:59 +0000] [41168] [INFO] Booting worker with pid: 41168
[2023-10-05 00:30:59 +0000] [684] [ERROR] Worker (pid:41014) was sent code 134!
[2023-10-05 00:30:59 +0000] [41169] [INFO] Booting worker with pid: 41169
2023-10-05 00:31:05.326916: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:31:05.397704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:31:06.139342: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:31:06.228109: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:31:06.984801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:06.988108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:06.990207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:07.091275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:07.094660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:07.096733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:08.964695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:08.964866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:08.969225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:08.969438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:08.973611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:08.973860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.625377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.628270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.630777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.633250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:31:11.658486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.660416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.661954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:31:11.663729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:31:12.775740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.777090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.778082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.779126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.780127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.781119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.782096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.783138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.784149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.785140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.786160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.787242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.788336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.789347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.790390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.791454: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.792478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.793502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.794520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.795558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.796663: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.797691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.798742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:31:12.799735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:31:50 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41168)
[2023-10-05 00:33:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41169)
[2023-10-05 00:35:49 +0000] [684] [ERROR] Worker (pid:41168) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:35:49 +0000] [41249] [INFO] Booting worker with pid: 41249
[2023-10-05 00:35:50 +0000] [684] [ERROR] Worker (pid:41169) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:35:50 +0000] [41258] [INFO] Booting worker with pid: 41258
2023-10-05 00:36:13.761077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:36:13.761077: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:36:15.599246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:36:15.599246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:36:18.595012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:18.595012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:18.609849: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:18.609851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:18.614191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:18.614446: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:36:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41249)
[2023-10-05 00:36:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41258)
[2023-10-05 00:36:20 +0000] [684] [ERROR] Worker (pid:41258) was sent code 134!
[2023-10-05 00:36:20 +0000] [41429] [INFO] Booting worker with pid: 41429
[2023-10-05 00:36:20 +0000] [684] [ERROR] Worker (pid:41249) was sent code 134!
[2023-10-05 00:36:20 +0000] [41430] [INFO] Booting worker with pid: 41430
2023-10-05 00:36:26.523794: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:36:26.642391: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:36:27.353525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:36:27.464382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:36:28.197167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:28.200485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:28.202551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:28.312879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:28.316001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:28.317965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:30.203032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:30.203097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:30.207459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:30.207721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:30.211944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:30.212197: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.791861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.794238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.796307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.798035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:36:32.838536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.841281: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.843870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:36:32.846405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:36:33.961788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.963110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.964077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.965039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.966017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.967038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.968040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.969610: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.971063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.972282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.973426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.975022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.976173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.977513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.979144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.980660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.981998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.983265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.984405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.985543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.986789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.988363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.989574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:36:33.990751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:37:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41429)
[2023-10-05 00:39:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41430)
[2023-10-05 00:40:59 +0000] [684] [ERROR] Worker (pid:41430) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:40:59 +0000] [41493] [INFO] Booting worker with pid: 41493
[2023-10-05 00:40:59 +0000] [684] [ERROR] Worker (pid:41429) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:40:59 +0000] [41494] [INFO] Booting worker with pid: 41494
2023-10-05 00:41:23.178410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:41:23.178410: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:41:24.905097: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:41:24.905099: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:41:27.836214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:27.836214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:27.850531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:27.850552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:27.854888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:27.855137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:41:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41493)
[2023-10-05 00:41:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41494)
[2023-10-05 00:41:30 +0000] [684] [ERROR] Worker (pid:41494) was sent code 134!
[2023-10-05 00:41:30 +0000] [41666] [INFO] Booting worker with pid: 41666
[2023-10-05 00:41:30 +0000] [684] [ERROR] Worker (pid:41493) was sent code 134!
[2023-10-05 00:41:30 +0000] [41667] [INFO] Booting worker with pid: 41667
2023-10-05 00:41:36.090683: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:41:36.148219: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:41:36.903197: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:41:36.956303: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:41:37.740927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:37.744185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:37.746168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:37.790630: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:37.793861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:37.795930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:39.738077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:39.738104: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:39.742531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:39.742794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:39.747121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:39.747379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.313313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.316006: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.318209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.320318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:41:42.397092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.400289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.402911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:41:42.405522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:41:43.363217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.364354: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.366092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.367217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.368313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.369921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.371288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.372871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.373959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.375520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.377121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.378205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.379810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.381141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.382245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.383619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.384719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.386324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.387495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.388837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.390005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.391383: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.392723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:41:43.393807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:42:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41666)
[2023-10-05 00:44:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41667)
[2023-10-05 00:46:12 +0000] [684] [ERROR] Worker (pid:41666) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:46:12 +0000] [41724] [INFO] Booting worker with pid: 41724
[2023-10-05 00:46:12 +0000] [684] [ERROR] Worker (pid:41667) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:46:12 +0000] [41725] [INFO] Booting worker with pid: 41725
2023-10-05 00:46:36.694082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:46:36.694082: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:46:38.409774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:46:38.409774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:46:41.245353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:41.245353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:41.258777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:41.258777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:41.263063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:41.263321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:46:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41724)
[2023-10-05 00:46:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41725)
[2023-10-05 00:46:43 +0000] [684] [ERROR] Worker (pid:41725) was sent code 134!
[2023-10-05 00:46:43 +0000] [41865] [INFO] Booting worker with pid: 41865
[2023-10-05 00:46:43 +0000] [684] [ERROR] Worker (pid:41724) was sent code 134!
[2023-10-05 00:46:43 +0000] [41866] [INFO] Booting worker with pid: 41866
2023-10-05 00:46:49.225894: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:46:49.256529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:46:50.041754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:46:50.079918: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:46:50.892391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:50.895585: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:50.897632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:50.928044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:50.931372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:50.933530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:53.006073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:53.006110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:53.010496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:53.010750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:53.014960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:53.015209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.658367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.660692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.663387: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.665976: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:46:55.729002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.730961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.732603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:46:55.734201: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:46:56.623279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.624323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.625303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.626277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.627299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.628327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.629337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.630361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.631409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.632418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.633444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.634466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.635528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.636540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.637566: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.638629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.639658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.640675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.641697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.642768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.643795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.645039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.646129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:46:56.647225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:47:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41866)
[2023-10-05 00:49:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41865)
[2023-10-05 00:51:38 +0000] [684] [ERROR] Worker (pid:41866) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:51:38 +0000] [41925] [INFO] Booting worker with pid: 41925
[2023-10-05 00:51:39 +0000] [684] [ERROR] Worker (pid:41865) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:51:39 +0000] [41928] [INFO] Booting worker with pid: 41928
2023-10-05 00:52:02.789236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:52:02.789236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:52:04.525349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:52:04.525347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:52:07.342324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:07.342324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:07.357414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:07.357414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:07.361758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:07.362016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:52:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41925)
[2023-10-05 00:52:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:41928)
[2023-10-05 00:52:09 +0000] [684] [ERROR] Worker (pid:41925) was sent code 134!
[2023-10-05 00:52:09 +0000] [42091] [INFO] Booting worker with pid: 42091
[2023-10-05 00:52:09 +0000] [684] [ERROR] Worker (pid:41928) was sent code 134!
[2023-10-05 00:52:09 +0000] [42092] [INFO] Booting worker with pid: 42092
2023-10-05 00:52:15.875700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:52:15.902877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:52:16.698211: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:52:16.716110: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:52:17.548688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:17.551841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:17.553943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:17.556712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:17.559755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:17.561846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:19.414370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:19.414370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:19.418847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:19.419071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:19.423326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:19.423536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.018053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.020900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.022769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.024920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:52:22.104065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.106303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.108112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:52:22.109929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:52:23.042272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.044835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.047378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.048521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.050680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.052760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.054228: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.055780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.057389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.059322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.060669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.062214: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.063826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.065785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.067929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.069475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.071622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.073231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.075210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.077240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.078797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.080355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.081456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:52:23.082563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:53:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42091)
[2023-10-05 00:55:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42092)
[2023-10-05 00:56:56 +0000] [684] [ERROR] Worker (pid:42091) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:56:56 +0000] [42158] [INFO] Booting worker with pid: 42158
[2023-10-05 00:56:56 +0000] [684] [ERROR] Worker (pid:42092) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:56:56 +0000] [42159] [INFO] Booting worker with pid: 42159
2023-10-05 00:57:20.632864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:57:20.632864: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:57:22.590258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:57:22.590258: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:57:25.515718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:25.515718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:25.530704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:25.530703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:25.534980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:25.535232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 00:57:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42158)
[2023-10-05 00:57:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42159)
[2023-10-05 00:57:27 +0000] [684] [ERROR] Worker (pid:42159) was sent code 134!
[2023-10-05 00:57:27 +0000] [42332] [INFO] Booting worker with pid: 42332
[2023-10-05 00:57:27 +0000] [684] [ERROR] Worker (pid:42158) was sent code 134!
[2023-10-05 00:57:27 +0000] [42333] [INFO] Booting worker with pid: 42333
2023-10-05 00:57:33.304209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:57:33.354245: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 00:57:34.123747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:57:34.181470: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 00:57:34.970094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:34.973359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:34.975479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:35.028124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:35.031377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:35.033285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:37.262130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:37.262240: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:37.266549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:37.266799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:37.271012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:37.271263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.875276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.878152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.880650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.883171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:57:39.964490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.966731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.968553: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 00:57:39.970156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 00:57:40.911461: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.912976: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.914897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.916343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.917788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.919800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.922056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.923551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.924890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.926064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.927695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.929183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.930753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.932740: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.934259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.935825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.936853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.938327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.939799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.941258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.943145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.944665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.945741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 00:57:40.946840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 00:58:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42332)
[2023-10-05 00:58:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42333)
[2023-10-05 00:59:55 +0000] [684] [ERROR] Worker (pid:42332) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:59:55 +0000] [42388] [INFO] Booting worker with pid: 42388
[2023-10-05 00:59:56 +0000] [684] [ERROR] Worker (pid:42333) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 00:59:56 +0000] [42389] [INFO] Booting worker with pid: 42389
2023-10-05 01:00:19.186859: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:00:19.186859: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:00:20.928949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:00:20.928948: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:00:23.803427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:23.803427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:23.817295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:23.817295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:23.821907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:23.822136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:00:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42388)
[2023-10-05 01:00:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42389)
[2023-10-05 01:00:26 +0000] [684] [ERROR] Worker (pid:42389) was sent code 134!
[2023-10-05 01:00:26 +0000] [42491] [INFO] Booting worker with pid: 42491
[2023-10-05 01:00:26 +0000] [684] [ERROR] Worker (pid:42388) was sent code 134!
[2023-10-05 01:00:26 +0000] [42492] [INFO] Booting worker with pid: 42492
2023-10-05 01:00:32.365736: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:00:32.421398: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:00:33.188213: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:00:33.245180: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:00:34.035082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:34.038237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:34.040455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:34.096102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:34.099196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:34.101318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:35.997322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:35.997376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:36.002019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:36.002237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:36.006696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:36.006953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.954703: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.957821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.959930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.961778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:00:38.975418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.977370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.979209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:00:38.980994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:00:39.898779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.900065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.901045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.902067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.903141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.904180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.905191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.906212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.907302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.908303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.909310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.910343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.911474: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.912502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.913540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.914565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.915602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.916599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.917603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.918648: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.919645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.920646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.921680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:00:39.922731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:01:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42491)
[2023-10-05 01:03:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42492)
[2023-10-05 01:05:25 +0000] [684] [ERROR] Worker (pid:42491) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:05:25 +0000] [42547] [INFO] Booting worker with pid: 42547
[2023-10-05 01:05:25 +0000] [684] [ERROR] Worker (pid:42492) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:05:25 +0000] [42549] [INFO] Booting worker with pid: 42549
2023-10-05 01:05:49.298939: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:05:49.298940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:05:51.065442: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:05:51.065447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:05:53.969568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:05:53.969568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:05:53.985592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:05:53.985609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:05:53.989928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:05:53.990180: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:05:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42547)
[2023-10-05 01:05:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42549)
[2023-10-05 01:05:55 +0000] [684] [ERROR] Worker (pid:42547) was sent code 134!
[2023-10-05 01:05:55 +0000] [42693] [INFO] Booting worker with pid: 42693
[2023-10-05 01:05:56 +0000] [684] [ERROR] Worker (pid:42549) was sent code 134!
[2023-10-05 01:05:56 +0000] [42694] [INFO] Booting worker with pid: 42694
2023-10-05 01:06:01.938706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:06:01.938706: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:06:02.748227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:06:02.756945: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:06:03.587377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:03.590568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:03.592637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:03.604680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:03.607786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:03.609788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:05.812074: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:05.812250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:05.816540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:05.816752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:05.820952: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:05.821173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.010709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.012813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.014616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.016319: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:06:09.107552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.109419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.111153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:06:09.112730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:06:10.066049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.067211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.069098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.070501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.072029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.073727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.075876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.078158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.080044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.081516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.083008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.084892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.086408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.088251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.089783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.091289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.092777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.094684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.096140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.097668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.099211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.101101: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.102181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:06:10.103475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:06:43 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42693)
[2023-10-05 01:08:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42694)
[2023-10-05 01:10:38 +0000] [684] [ERROR] Worker (pid:42693) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:10:38 +0000] [42782] [INFO] Booting worker with pid: 42782
[2023-10-05 01:10:38 +0000] [684] [ERROR] Worker (pid:42694) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:10:39 +0000] [42785] [INFO] Booting worker with pid: 42785
2023-10-05 01:11:02.997139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:11:02.997139: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:11:04.841129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:11:04.841129: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:11:08.012863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:08.012863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:08.029989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:08.029989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:08.034345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:08.034614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:11:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42782)
[2023-10-05 01:11:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42785)
[2023-10-05 01:11:09 +0000] [684] [ERROR] Worker (pid:42785) was sent code 134!
[2023-10-05 01:11:09 +0000] [42934] [INFO] Booting worker with pid: 42934
[2023-10-05 01:11:09 +0000] [684] [ERROR] Worker (pid:42782) was sent code 134!
[2023-10-05 01:11:09 +0000] [42935] [INFO] Booting worker with pid: 42935
2023-10-05 01:11:15.390628: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:11:15.452129: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:11:16.203957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:11:16.270938: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:11:17.044483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:17.047747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:17.049896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:17.124713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:17.127941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:17.129840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:19.506885: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:19.506896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:19.511422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:19.511654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:19.515880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:19.516124: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.461236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.463883: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.464150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.469087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.469356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.474008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:11:22.474243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:11:22.476083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:11:23.401299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.402639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.403623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.404636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.406005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.407616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.409009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.410352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.411509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.412909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.414129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.415348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.416530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.417638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.418756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.419844: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.421395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.422980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.424492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.425970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.427092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.428620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.429699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:11:23.430862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:12:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42934)
[2023-10-05 01:14:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:42935)
[2023-10-05 01:16:30 +0000] [684] [ERROR] Worker (pid:42934) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:16:30 +0000] [43030] [INFO] Booting worker with pid: 43030
[2023-10-05 01:16:30 +0000] [684] [ERROR] Worker (pid:42935) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:16:30 +0000] [43054] [INFO] Booting worker with pid: 43054
2023-10-05 01:16:54.542307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:16:54.542307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:16:56.356917: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:16:56.356921: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:16:59.581601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:16:59.581601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:16:59.598578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:16:59.598654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:16:59.603058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:16:59.603296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:17:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43030)
[2023-10-05 01:17:00 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43054)
[2023-10-05 01:17:01 +0000] [684] [ERROR] Worker (pid:43030) was sent code 134!
[2023-10-05 01:17:01 +0000] [43227] [INFO] Booting worker with pid: 43227
[2023-10-05 01:17:01 +0000] [684] [ERROR] Worker (pid:43054) was sent code 134!
[2023-10-05 01:17:01 +0000] [43228] [INFO] Booting worker with pid: 43228
2023-10-05 01:17:07.181381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:17:07.247608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:17:07.998429: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:17:08.060175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:17:08.838235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:08.841487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:08.843603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:08.905872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:08.909306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:08.911450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:11.446088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:11.446190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:11.450538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:11.450791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:11.455014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:11.455256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.302908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.306033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.308964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.311892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19002 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:17:14.497320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.500122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.502719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:17:14.505268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:17:15.452083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.454417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.455846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.457183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.458875: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.460704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.462172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.463530: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.465211: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.466234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.467582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.469328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.470807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.472175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.473825: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.475192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.476528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.477859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.479244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.480888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.482217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.483596: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.484694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.485738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:17:15.486791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:17:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43227)
[2023-10-05 01:19:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43228)
[2023-10-05 01:19:50 +0000] [684] [ERROR] Worker (pid:43228) was sent code 134!
[2023-10-05 01:19:50 +0000] [43338] [INFO] Booting worker with pid: 43338
[2023-10-05 01:19:50 +0000] [684] [ERROR] Worker (pid:43227) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:19:50 +0000] [43339] [INFO] Booting worker with pid: 43339
2023-10-05 01:20:12.717930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:20:12.717930: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:20:14.532119: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:20:14.532118: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:20:17.467693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:17.467693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:17.481810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:17.481827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:17.486230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:17.486443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:20.453611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:20.453635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:20.458029: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:20.458282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:20.462466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:20.462721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:20:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43338)
[2023-10-05 01:20:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43339)
[2023-10-05 01:20:21 +0000] [684] [ERROR] Worker (pid:43338) was sent code 134!
[2023-10-05 01:20:21 +0000] [43431] [INFO] Booting worker with pid: 43431
[2023-10-05 01:20:21 +0000] [684] [ERROR] Worker (pid:43339) was sent code 134!
[2023-10-05 01:20:21 +0000] [43432] [INFO] Booting worker with pid: 43432
2023-10-05 01:20:27.027414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:20:27.123608: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:20:27.841270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:20:27.949233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:20:28.689761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:28.692931: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:28.694996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:28.800886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:28.804106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:28.806158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:30.242216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:30.244584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:30.246496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:30.337428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:30.341337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:30.344292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.836460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.838857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.841342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.843736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:20:32.859835: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.861866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.863629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:20:32.865268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:20:33.769342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.770413: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.771476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.772587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.774361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.776254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.778034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.779626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.781225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.782380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.783804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.785181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.786532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.787760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.788974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.790120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.791332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.792496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.793713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.794871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.796045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.797182: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.798308: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:20:33.799492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:21:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43431)
[2023-10-05 01:22:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43432)
[2023-10-05 01:25:03 +0000] [684] [ERROR] Worker (pid:43431) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:25:03 +0000] [43485] [INFO] Booting worker with pid: 43485
[2023-10-05 01:25:04 +0000] [684] [ERROR] Worker (pid:43432) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:25:04 +0000] [43487] [INFO] Booting worker with pid: 43487
2023-10-05 01:25:28.101517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:25:28.101517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:25:29.963460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:25:29.963460: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:25:33.181922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:33.181922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:33.200200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:33.200217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:33.204539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:33.204795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:25:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43485)
[2023-10-05 01:25:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43487)
[2023-10-05 01:25:34 +0000] [684] [ERROR] Worker (pid:43487) was sent code 134!
[2023-10-05 01:25:34 +0000] [43632] [INFO] Booting worker with pid: 43632
[2023-10-05 01:25:34 +0000] [684] [ERROR] Worker (pid:43485) was sent code 134!
[2023-10-05 01:25:34 +0000] [43633] [INFO] Booting worker with pid: 43633
2023-10-05 01:25:40.549513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:25:40.549513: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:25:41.361067: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:25:41.363375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:25:42.196123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:42.196795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:42.201111: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:42.201675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:42.205000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:42.206135: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:44.823940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:44.823940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:44.828414: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:44.828656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:44.832901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:44.833160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.522555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.524855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.526671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.528536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:25:47.690359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.692504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.694312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:25:47.696091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:25:48.627551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.628675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.629712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.631057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.632390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.633866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.635315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.637044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.638502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.640169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.641602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.642705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.644158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.645608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.647114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.648702: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.650263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.651693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.652748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.654147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.655966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.657441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.658504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:25:48.659574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:26:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43632)
[2023-10-05 01:27:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43633)
[2023-10-05 01:27:54 +0000] [684] [ERROR] Worker (pid:43632) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:27:54 +0000] [43691] [INFO] Booting worker with pid: 43691
[2023-10-05 01:27:54 +0000] [684] [ERROR] Worker (pid:43633) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:27:54 +0000] [43692] [INFO] Booting worker with pid: 43692
2023-10-05 01:28:18.021357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:28:18.021357: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:28:19.838929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:28:19.838929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:28:22.848961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:22.848961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:22.867138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:22.867142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:22.871627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:22.871890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:28:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43691)
[2023-10-05 01:28:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43692)
[2023-10-05 01:28:25 +0000] [684] [ERROR] Worker (pid:43692) was sent code 134!
[2023-10-05 01:28:25 +0000] [43791] [INFO] Booting worker with pid: 43791
[2023-10-05 01:28:25 +0000] [684] [ERROR] Worker (pid:43691) was sent code 134!
[2023-10-05 01:28:25 +0000] [43792] [INFO] Booting worker with pid: 43792
2023-10-05 01:28:31.103438: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:28:31.153418: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:28:31.908361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:28:31.969668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:28:32.747299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:32.750494: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:32.752637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:32.820321: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:32.823377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:32.825283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:34.836738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:34.836738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:34.841158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:34.841401: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:34.845586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:34.845832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.714634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.717143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.719467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.721938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:28:37.778763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.781535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.784183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:28:37.786691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:28:38.718964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.720086: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.721573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.723577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.725615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.727669: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.729554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.731586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.733060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.734988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.736460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.738368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.740287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.741769: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.743697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.745193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.747136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.748616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.750087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.752079: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.753961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.755469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.756657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:28:38.758095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:29:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43791)
[2023-10-05 01:30:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43792)
[2023-10-05 01:33:12 +0000] [684] [ERROR] Worker (pid:43791) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:33:12 +0000] [43848] [INFO] Booting worker with pid: 43848
[2023-10-05 01:33:12 +0000] [684] [ERROR] Worker (pid:43792) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:33:12 +0000] [43850] [INFO] Booting worker with pid: 43850
2023-10-05 01:33:36.460779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:33:36.460779: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:33:38.264336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:33:38.264338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:33:41.265457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:41.265457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:41.281502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:41.281502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:41.285846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:41.286095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:33:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43848)
[2023-10-05 01:33:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43850)
[2023-10-05 01:33:43 +0000] [684] [ERROR] Worker (pid:43850) was sent code 134!
[2023-10-05 01:33:43 +0000] [43992] [INFO] Booting worker with pid: 43992
[2023-10-05 01:33:43 +0000] [684] [ERROR] Worker (pid:43848) was sent code 134!
[2023-10-05 01:33:43 +0000] [43993] [INFO] Booting worker with pid: 43993
2023-10-05 01:33:48.953491: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:33:49.051399: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:33:49.761398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:33:49.869053: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:33:50.595241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:50.598476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:50.600332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:50.719145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:50.722277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:50.724127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:53.010624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:53.010685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:53.015292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:53.015514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:53.019856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:53.020114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.558077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.560967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.563560: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.566061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:33:55.790302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.792329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.793964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:33:55.795637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:33:56.714964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.716298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.717418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.718880: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.720286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.721362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.722800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.724207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.725601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.727385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.728782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.729838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.731608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.732993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.734447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.735901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.737320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.738811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.740375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.741855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.743961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.745755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.747138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.748406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:33:56.749665: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:34:33 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43992)
[2023-10-05 01:36:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:43993)
[2023-10-05 01:38:09 +0000] [684] [ERROR] Worker (pid:43992) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:38:09 +0000] [44049] [INFO] Booting worker with pid: 44049
[2023-10-05 01:38:09 +0000] [684] [ERROR] Worker (pid:43993) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:38:09 +0000] [44052] [INFO] Booting worker with pid: 44052
2023-10-05 01:38:33.025149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:38:33.025149: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:38:34.874380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:38:34.874379: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:38:37.865895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:37.865895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:37.882200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:37.882218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:37.886694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:37.886956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:38:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44049)
[2023-10-05 01:38:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44052)
[2023-10-05 01:38:40 +0000] [684] [ERROR] Worker (pid:44049) was sent code 134!
[2023-10-05 01:38:40 +0000] [44122] [INFO] Booting worker with pid: 44122
[2023-10-05 01:38:40 +0000] [684] [ERROR] Worker (pid:44052) was sent code 134!
[2023-10-05 01:38:40 +0000] [44123] [INFO] Booting worker with pid: 44123
2023-10-05 01:38:46.272128: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:38:46.275934: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:38:47.085758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:38:47.099237: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:38:47.922921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:47.926175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:47.928336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:47.938934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:47.941824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:47.943827: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:49.897861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:49.897861: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:49.902259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:49.902503: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:49.906733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:49.906972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.616715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.619576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.622038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.624539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:38:52.662800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.664803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.666705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:38:52.668458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:38:53.589984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.591303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.592292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.593283: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.594277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.595311: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.596330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.597345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.598372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.599431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.600448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.601457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.602496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.603534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.604556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.605625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.606712: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.607719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.608725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.609741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.610801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.611810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.612814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:38:53.613802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:39:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44122)
[2023-10-05 01:41:09 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44123)
[2023-10-05 01:43:00 +0000] [684] [ERROR] Worker (pid:44122) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:43:00 +0000] [44178] [INFO] Booting worker with pid: 44178
[2023-10-05 01:43:01 +0000] [684] [ERROR] Worker (pid:44123) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:43:01 +0000] [44181] [INFO] Booting worker with pid: 44181
2023-10-05 01:43:24.549625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:43:24.549625: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:43:26.419168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:43:26.419168: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:43:29.493412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:29.493412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:29.510108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:29.510109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:29.514626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:29.514872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:43:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44178)
[2023-10-05 01:43:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44181)
[2023-10-05 01:43:31 +0000] [684] [ERROR] Worker (pid:44178) was sent code 134!
[2023-10-05 01:43:31 +0000] [44250] [INFO] Booting worker with pid: 44250
[2023-10-05 01:43:31 +0000] [684] [ERROR] Worker (pid:44181) was sent code 134!
[2023-10-05 01:43:31 +0000] [44251] [INFO] Booting worker with pid: 44251
2023-10-05 01:43:37.698092: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:43:37.775199: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:43:38.505970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:43:38.589549: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:43:39.347879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:39.351041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:39.353127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:39.440975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:39.444404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:39.446575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:41.395600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:41.395736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:41.400023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:41.400271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:41.404467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:41.404720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:43.983625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:43.986477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:43.989066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:43.991572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:43:44.014659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:44.017327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:44.019641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:43:44.021471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:43:44.944914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.946923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.948271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.949431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.951112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.952860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.954644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.956430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.958250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.960013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.961376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.962696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.963899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.965031: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.966208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.967458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.968650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.969813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.971032: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.972270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.973852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.975733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.977085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:43:44.978270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:44:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44250)
[2023-10-05 01:46:06 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44251)
[2023-10-05 01:47:56 +0000] [684] [ERROR] Worker (pid:44250) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:47:56 +0000] [44305] [INFO] Booting worker with pid: 44305
[2023-10-05 01:47:56 +0000] [684] [ERROR] Worker (pid:44251) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:47:56 +0000] [44306] [INFO] Booting worker with pid: 44306
2023-10-05 01:48:19.911048: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:48:19.911048: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:48:21.702100: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:48:21.702103: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:48:24.662507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:24.662507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:24.677562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:24.677562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:24.682188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:24.682412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:48:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44305)
[2023-10-05 01:48:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44306)
[2023-10-05 01:48:27 +0000] [684] [ERROR] Worker (pid:44305) was sent code 134!
[2023-10-05 01:48:27 +0000] [44408] [INFO] Booting worker with pid: 44408
[2023-10-05 01:48:27 +0000] [684] [ERROR] Worker (pid:44306) was sent code 134!
[2023-10-05 01:48:27 +0000] [44409] [INFO] Booting worker with pid: 44409
2023-10-05 01:48:33.174263: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:48:33.226731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:48:33.984048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:48:34.040562: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:48:34.817348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:34.820504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:34.822770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:34.880457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:34.883745: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:34.885782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:36.758212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:36.758252: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:36.762715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:36.762970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:36.767168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:36.767415: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.444639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.447298: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.449506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.451793: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:48:39.454655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.457190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.459716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:48:39.462196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:48:40.383950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.385230: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.386293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.387389: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.388684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.389887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.391318: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.392973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.394645: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.396315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.397958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.399632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.401307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.402970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.404333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.405715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.407013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.408161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.409295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.410869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.412011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.413091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.414174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:48:40.415309: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:49:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44409)
[2023-10-05 01:50:02 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44408)
[2023-10-05 01:51:48 +0000] [684] [ERROR] Worker (pid:44409) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:51:48 +0000] [44465] [INFO] Booting worker with pid: 44465
[2023-10-05 01:51:49 +0000] [684] [ERROR] Worker (pid:44408) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:51:49 +0000] [44469] [INFO] Booting worker with pid: 44469
2023-10-05 01:52:12.329393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:52:12.329393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:52:14.264425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:52:14.264423: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:52:17.525502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:17.525502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:17.543094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:17.543113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:17.547370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:17.547639: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:52:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44465)
[2023-10-05 01:52:19 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44469)
[2023-10-05 01:52:19 +0000] [684] [ERROR] Worker (pid:44465) was sent code 134!
[2023-10-05 01:52:19 +0000] [44583] [INFO] Booting worker with pid: 44583
[2023-10-05 01:52:20 +0000] [684] [ERROR] Worker (pid:44469) was sent code 134!
[2023-10-05 01:52:20 +0000] [44584] [INFO] Booting worker with pid: 44584
2023-10-05 01:52:25.891437: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:52:25.961091: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:52:26.702390: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:52:26.772038: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:52:27.540660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:27.543959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:27.545919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:27.631770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:27.635030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:27.637087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:29.524628: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:29.524756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:29.529033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:29.529279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:29.533444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:29.533687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.225065: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.227528: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.229453: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.231338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:52:32.250002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.252201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.254069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:52:32.255928: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:52:33.175323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.176615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.177584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.178550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.179846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.181080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.182337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.183450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.184840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.186465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.188108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.189728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.191340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.192968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.194584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.196143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.197350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.198514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.199698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.200776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.201814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.202933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.204037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:52:33.205085: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:53:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44583)
[2023-10-05 01:54:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44584)
[2023-10-05 01:56:46 +0000] [684] [ERROR] Worker (pid:44583) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:56:46 +0000] [44641] [INFO] Booting worker with pid: 44641
[2023-10-05 01:56:46 +0000] [684] [ERROR] Worker (pid:44584) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:56:46 +0000] [44642] [INFO] Booting worker with pid: 44642
2023-10-05 01:57:09.265282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:57:09.265282: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:57:11.109957: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:57:11.109955: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:57:13.997160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:13.997160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:14.010879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:14.010878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:14.015189: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:14.015440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 01:57:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44641)
[2023-10-05 01:57:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44642)
[2023-10-05 01:57:16 +0000] [684] [ERROR] Worker (pid:44642) was sent code 134!
[2023-10-05 01:57:16 +0000] [44741] [INFO] Booting worker with pid: 44741
[2023-10-05 01:57:16 +0000] [684] [ERROR] Worker (pid:44641) was sent code 134!
[2023-10-05 01:57:16 +0000] [44742] [INFO] Booting worker with pid: 44742
2023-10-05 01:57:22.556767: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:57:23.066914: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 01:57:23.394573: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:57:23.869643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 01:57:24.242341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:24.245715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:24.247968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:24.718082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:24.721213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:24.723356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:26.090399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:26.092696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:26.094801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:26.282145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:26.284347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:26.286416: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.823696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.826456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.828571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.830937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:57:28.883744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.885773: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.887540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 01:57:28.889150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 01:57:29.788969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.790046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.791163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.792198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.793231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.794288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.795351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.796386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.797422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.798466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.799521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.801398: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.802927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.804002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.805052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.806940: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.808428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.810365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.811843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.813390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.814934: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.816458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.817490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 01:57:29.818561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 01:57:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44741)
[2023-10-05 01:58:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44742)
[2023-10-05 01:59:39 +0000] [684] [ERROR] Worker (pid:44742) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:59:39 +0000] [44796] [INFO] Booting worker with pid: 44796
[2023-10-05 01:59:39 +0000] [684] [ERROR] Worker (pid:44741) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 01:59:39 +0000] [44797] [INFO] Booting worker with pid: 44797
2023-10-05 02:00:03.300896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:00:03.300896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:00:05.048808: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:00:05.048809: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:00:07.939169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:07.939169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:07.954053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:07.954054: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:07.958435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:07.958698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:00:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44796)
[2023-10-05 02:00:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44797)
[2023-10-05 02:00:10 +0000] [684] [ERROR] Worker (pid:44796) was sent code 134!
[2023-10-05 02:00:10 +0000] [44871] [INFO] Booting worker with pid: 44871
[2023-10-05 02:00:10 +0000] [684] [ERROR] Worker (pid:44797) was sent code 134!
[2023-10-05 02:00:10 +0000] [44872] [INFO] Booting worker with pid: 44872
2023-10-05 02:00:16.292346: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:00:16.388124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:00:17.103101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:00:17.211697: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:00:17.950960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:17.954436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:17.956487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:18.061918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:18.065095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:18.067198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:19.924418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:19.924429: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:19.928878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:19.929203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:19.933068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:19.933749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.505735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.508485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.510776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.512720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:00:22.552737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.554794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.556586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:00:22.558243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:00:23.512338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.513487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.514614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.515840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.516936: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.518554: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.519705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.520789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.521874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.523008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.524084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.525435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.526635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.528231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.529577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.531185: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.532511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.533590: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.535380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.536709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.538052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.539172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.540260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:00:23.541338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:00:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44871)
[2023-10-05 02:02:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44872)
[2023-10-05 02:04:50 +0000] [684] [ERROR] Worker (pid:44871) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:04:50 +0000] [44955] [INFO] Booting worker with pid: 44955
[2023-10-05 02:04:50 +0000] [684] [ERROR] Worker (pid:44872) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:04:50 +0000] [44958] [INFO] Booting worker with pid: 44958
2023-10-05 02:05:14.591539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:05:14.591539: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:05:16.533023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:05:16.533024: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:05:19.681110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:19.681110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:19.698275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:19.698275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:19.702807: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:19.703075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:05:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44955)
[2023-10-05 02:05:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:44958)
[2023-10-05 02:05:21 +0000] [684] [ERROR] Worker (pid:44958) was sent code 134!
[2023-10-05 02:05:21 +0000] [45129] [INFO] Booting worker with pid: 45129
[2023-10-05 02:05:21 +0000] [684] [ERROR] Worker (pid:44955) was sent code 134!
[2023-10-05 02:05:21 +0000] [45130] [INFO] Booting worker with pid: 45130
2023-10-05 02:05:27.307464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:05:27.330857: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:05:28.131360: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:05:28.135387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:05:28.986361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:28.987480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:28.990930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:28.991791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:28.994333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:28.996371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:31.365336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:31.365336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:31.369909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:31.370166: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:31.374325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:31.374573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.079574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.082814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.083487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.087832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.088576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.092730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:05:34.093282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:05:34.095740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:05:35.021322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.022672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.023662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.024635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.025608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.026627: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.027708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.028902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.030371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.031520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.032709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.034299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.035505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.036830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.037924: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.039289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.040651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.041826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.043052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.044232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.045552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.046644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.047690: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:05:35.048785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:06:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45129)
[2023-10-05 02:07:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45130)
[2023-10-05 02:10:00 +0000] [684] [ERROR] Worker (pid:45129) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:10:00 +0000] [45187] [INFO] Booting worker with pid: 45187
[2023-10-05 02:10:01 +0000] [684] [ERROR] Worker (pid:45130) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:10:01 +0000] [45190] [INFO] Booting worker with pid: 45190
2023-10-05 02:10:24.818691: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:10:24.818833: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:10:26.576320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:10:26.576330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:10:29.453187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:29.453187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:29.466516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:29.466515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:29.470814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:29.471068: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:10:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45187)
[2023-10-05 02:10:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45190)
[2023-10-05 02:10:31 +0000] [684] [ERROR] Worker (pid:45187) was sent code 134!
[2023-10-05 02:10:31 +0000] [45335] [INFO] Booting worker with pid: 45335
[2023-10-05 02:10:31 +0000] [684] [ERROR] Worker (pid:45190) was sent code 134!
[2023-10-05 02:10:31 +0000] [45336] [INFO] Booting worker with pid: 45336
2023-10-05 02:10:37.580066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:10:37.654935: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:10:38.401254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:10:38.479462: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:10:39.242490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:39.245809: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:39.247957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:39.319000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:39.322127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:39.324133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:41.360447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:41.360447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:41.364814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:41.365063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:41.369239: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:41.369486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.194381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.197366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.199147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.200877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:10:44.310515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.312511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.314320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:10:44.316202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:10:45.268201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.269355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.270417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.271847: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.273369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.274758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.276113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.277889: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.279269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.280979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.282349: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.283767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.285208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.286638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.288041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.289873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.291319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.292726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.294146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.295555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.296960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.298330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.299384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:10:45.300693: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:11:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45335)
[2023-10-05 02:12:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45336)
[2023-10-05 02:13:49 +0000] [684] [ERROR] Worker (pid:45335) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:13:49 +0000] [45391] [INFO] Booting worker with pid: 45391
[2023-10-05 02:13:49 +0000] [684] [ERROR] Worker (pid:45336) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:13:49 +0000] [45392] [INFO] Booting worker with pid: 45392
2023-10-05 02:14:13.146787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:14:13.146787: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:14:14.897402: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:14:14.897398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:14:17.716200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:17.716200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:17.731008: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:17.731009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:17.735492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:17.735754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:14:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45391)
[2023-10-05 02:14:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45392)
[2023-10-05 02:14:20 +0000] [684] [ERROR] Worker (pid:45392) was sent code 134!
[2023-10-05 02:14:20 +0000] [45492] [INFO] Booting worker with pid: 45492
[2023-10-05 02:14:20 +0000] [684] [ERROR] Worker (pid:45391) was sent code 134!
[2023-10-05 02:14:20 +0000] [45493] [INFO] Booting worker with pid: 45493
2023-10-05 02:14:26.296478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:14:26.368146: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:14:27.120514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:14:27.182662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:14:27.972076: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:27.975288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:27.977534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:28.024390: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:28.027535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:28.029584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:29.844262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:29.844266: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:29.848748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:29.848966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:29.853202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:29.853435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.528671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.531499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.533405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.535138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:14:32.574845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.576857: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.578643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:14:32.580234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:14:33.494729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.497109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.498611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.499787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.501274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.502818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.504351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.506381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.508277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.510183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.512099: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.513573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.515525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.517009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.519040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.521030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.522972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.524471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.526320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.527856: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.529732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.531277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.532362: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:14:33.533396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:15:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45492)
[2023-10-05 02:16:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45493)
[2023-10-05 02:19:21 +0000] [684] [ERROR] Worker (pid:45492) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:19:21 +0000] [45547] [INFO] Booting worker with pid: 45547
[2023-10-05 02:19:22 +0000] [684] [ERROR] Worker (pid:45493) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:19:22 +0000] [45550] [INFO] Booting worker with pid: 45550
2023-10-05 02:19:45.809430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:19:45.809430: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:19:47.479349: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:19:47.479348: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:19:50.277041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:19:50.277041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:19:50.291618: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:19:50.291664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:19:50.295865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:19:50.296125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:19:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45547)
[2023-10-05 02:19:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45550)
[2023-10-05 02:19:52 +0000] [684] [ERROR] Worker (pid:45547) was sent code 134!
[2023-10-05 02:19:52 +0000] [45692] [INFO] Booting worker with pid: 45692
[2023-10-05 02:19:52 +0000] [684] [ERROR] Worker (pid:45550) was sent code 134!
[2023-10-05 02:19:52 +0000] [45693] [INFO] Booting worker with pid: 45693
2023-10-05 02:19:58.525714: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:19:58.561977: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:19:59.311439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:19:59.347519: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:20:00.123562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:00.126845: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:00.128798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:00.154024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:00.157097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:00.159153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:02.032869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:02.032892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:02.037192: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:02.037920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:02.041737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:02.042430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.797571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.799382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.801204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.803135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:20:04.842977: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.844898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.846683: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:20:04.848314: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:20:05.708599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.709873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.710948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.711974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.712990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.714019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.715388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.717035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.718748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.720360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.721993: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.723658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.724921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.726095: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.727295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.728435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.729506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.730649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.731739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.732821: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.734428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.736082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.737210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:20:05.738326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:20:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45693)
[2023-10-05 02:22:30 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45692)
[2023-10-05 02:24:14 +0000] [684] [ERROR] Worker (pid:45693) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:24:14 +0000] [45753] [INFO] Booting worker with pid: 45753
[2023-10-05 02:24:15 +0000] [684] [ERROR] Worker (pid:45692) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:24:15 +0000] [45774] [INFO] Booting worker with pid: 45774
2023-10-05 02:24:37.944114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:24:37.944114: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:24:39.753922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:24:39.753922: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:24:43.205520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:43.205520: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:43.280928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:43.280964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:43.285251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:43.285506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:24:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45753)
[2023-10-05 02:24:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45774)
[2023-10-05 02:24:45 +0000] [684] [ERROR] Worker (pid:45753) was sent code 134!
[2023-10-05 02:24:45 +0000] [45855] [INFO] Booting worker with pid: 45855
[2023-10-05 02:24:45 +0000] [684] [ERROR] Worker (pid:45774) was sent code 134!
[2023-10-05 02:24:45 +0000] [45856] [INFO] Booting worker with pid: 45856
2023-10-05 02:24:51.311992: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:24:51.324523: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:24:52.092419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:24:52.100007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:24:52.899524: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:52.902763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:52.904467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:52.904680: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:52.909572: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:52.911617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:55.022050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:55.022050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:55.026484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:55.026735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:55.030941: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:55.031193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.795407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.795411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.800570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.800854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.805573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.805859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:24:57.810536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:24:57.810805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:24:58.698658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.699953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.700978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.702017: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.703290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.704516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.705852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.706985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.708256: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.709894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.711550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.712899: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.714011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.715167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.716302: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.717329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.718369: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.719438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.720498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.721813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.723268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.724360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.725422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:24:58.726466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:25:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45856)
[2023-10-05 02:27:35 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45855)
[2023-10-05 02:28:53 +0000] [684] [ERROR] Worker (pid:45856) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:28:53 +0000] [45907] [INFO] Booting worker with pid: 45907
[2023-10-05 02:28:54 +0000] [684] [ERROR] Worker (pid:45855) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:28:54 +0000] [45910] [INFO] Booting worker with pid: 45910
2023-10-05 02:29:17.503205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:29:17.503205: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:29:19.331257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:29:19.331257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:29:22.470789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:22.470789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:22.485870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:22.485874: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:22.490151: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:22.490403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:29:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45907)
[2023-10-05 02:29:24 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:45910)
[2023-10-05 02:29:24 +0000] [684] [ERROR] Worker (pid:45907) was sent code 134!
[2023-10-05 02:29:24 +0000] [46009] [INFO] Booting worker with pid: 46009
[2023-10-05 02:29:25 +0000] [684] [ERROR] Worker (pid:45910) was sent code 134!
[2023-10-05 02:29:25 +0000] [46010] [INFO] Booting worker with pid: 46010
2023-10-05 02:29:30.811444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:29:30.839472: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:29:31.594554: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:29:31.618356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:29:32.402994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:32.406155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:32.408022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:32.424718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:32.427818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:32.430027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:34.316483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:34.316484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:34.320929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:34.321178: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:34.325374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:34.325622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:36.998653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:37.001469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:37.004002: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:37.006500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:29:37.035535: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:37.037550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:37.039411: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:29:37.041237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:29:37.909722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.911062: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.912025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.912991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.914000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.915052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.916061: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.917080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.918088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.919144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.920134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.921131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.922160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.923232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.924232: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.925242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.926275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.927317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.928329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.929335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.930334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.931373: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.932385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:29:37.933399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:30:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46009)
[2023-10-05 02:32:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46010)
[2023-10-05 02:34:25 +0000] [684] [ERROR] Worker (pid:46009) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:34:25 +0000] [46068] [INFO] Booting worker with pid: 46068
[2023-10-05 02:34:25 +0000] [684] [ERROR] Worker (pid:46010) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:34:25 +0000] [46071] [INFO] Booting worker with pid: 46071
2023-10-05 02:34:48.458724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:34:48.458724: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:34:50.139448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:34:50.139447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:34:53.023119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:34:53.023119: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:34:53.037434: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:34:53.037489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:34:53.041711: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:34:53.041960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:34:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46068)
[2023-10-05 02:34:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46071)
[2023-10-05 02:34:55 +0000] [684] [ERROR] Worker (pid:46068) was sent code 134!
[2023-10-05 02:34:55 +0000] [46211] [INFO] Booting worker with pid: 46211
[2023-10-05 02:34:55 +0000] [684] [ERROR] Worker (pid:46071) was sent code 134!
[2023-10-05 02:34:55 +0000] [46212] [INFO] Booting worker with pid: 46212
2023-10-05 02:35:01.335896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:35:01.406373: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:35:02.125322: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:35:02.183952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:35:02.938367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:02.941490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:02.943456: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:02.987604: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:02.990715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:02.992650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:04.674792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:04.674873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:04.679215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:04.679460: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:04.683673: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:04.683927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.666129: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.668780: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.670748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.672833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:35:07.676299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.678231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.679944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:35:07.681676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:35:08.565882: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.567219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.568193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.569195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.570170: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.571176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.572132: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.573107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.574133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.575188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.576207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.577208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.578251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.579305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.580326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.581335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.582346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.583406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.584406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.585404: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.586410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.587435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.588430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:35:08.589420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:35:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46211)
[2023-10-05 02:37:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46212)
[2023-10-05 02:38:51 +0000] [684] [ERROR] Worker (pid:46211) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:38:51 +0000] [46296] [INFO] Booting worker with pid: 46296
[2023-10-05 02:38:51 +0000] [684] [ERROR] Worker (pid:46212) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:38:51 +0000] [46300] [INFO] Booting worker with pid: 46300
2023-10-05 02:39:15.407380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:39:15.407380: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:39:17.177239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:39:17.177239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:39:20.107388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:20.107388: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:20.121356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:20.121384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:20.125634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:20.125891: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:39:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46296)
[2023-10-05 02:39:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46300)
[2023-10-05 02:39:22 +0000] [684] [ERROR] Worker (pid:46296) was sent code 134!
[2023-10-05 02:39:22 +0000] [46400] [INFO] Booting worker with pid: 46400
[2023-10-05 02:39:22 +0000] [684] [ERROR] Worker (pid:46300) was sent code 134!
[2023-10-05 02:39:22 +0000] [46401] [INFO] Booting worker with pid: 46401
2023-10-05 02:39:28.223529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:39:28.223529: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:39:29.000267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:39:29.013525: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:39:29.803562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:29.806783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:29.808796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:29.825721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:29.828850: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:29.830907: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:31.850072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:31.850133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:31.854509: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:31.854746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:31.858923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:31.859171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.577067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.579674: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.582236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.584277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:39:34.591284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.593786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.596237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:39:34.598493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:39:35.490512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.491830: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.492801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.493758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.494771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.495725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.496736: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.497927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.499146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.500676: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.502130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.503799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.505219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.506833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.508016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.509168: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.510340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.511489: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.512531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.513582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.514657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.515701: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.516754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:39:35.517808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:40:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46400)
[2023-10-05 02:40:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46401)
[2023-10-05 02:40:24 +0000] [684] [ERROR] Worker (pid:46400) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:40:24 +0000] [46454] [INFO] Booting worker with pid: 46454
[2023-10-05 02:40:24 +0000] [684] [ERROR] Worker (pid:46401) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:40:24 +0000] [46457] [INFO] Booting worker with pid: 46457
2023-10-05 02:40:47.948962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:40:47.948962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:40:49.674641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:40:49.674652: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:40:52.584278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:40:52.584278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:40:52.598609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:40:52.598655: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:40:52.602937: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:40:52.603191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:40:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46454)
[2023-10-05 02:40:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46457)
[2023-10-05 02:40:55 +0000] [684] [ERROR] Worker (pid:46454) was sent code 134!
[2023-10-05 02:40:55 +0000] [46527] [INFO] Booting worker with pid: 46527
[2023-10-05 02:40:55 +0000] [684] [ERROR] Worker (pid:46457) was sent code 134!
[2023-10-05 02:40:55 +0000] [46528] [INFO] Booting worker with pid: 46528
2023-10-05 02:41:01.122427: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:41:01.199881: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:41:01.909340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:41:01.995881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:41:02.713070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:02.716306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:02.718234: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:02.818056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:02.821165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:02.823296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:04.455616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:04.455617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:04.460188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:04.460447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:04.464666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:04.464920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.162361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.165188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.167748: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.170261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:41:07.229649: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.232122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.233884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:41:07.235527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:41:08.098040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.099181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.100222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.101276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.102305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.103364: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.104395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.105410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.106431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.107477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.108982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.109999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.111070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.112765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.114574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.115968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.117327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.119050: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.120452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.121798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.123188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.124886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.125908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:41:08.126958: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:41:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46527)
[2023-10-05 02:41:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46528)
[2023-10-05 02:41:37 +0000] [684] [ERROR] Worker (pid:46527) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:41:37 +0000] [46601] [INFO] Booting worker with pid: 46601
[2023-10-05 02:41:37 +0000] [684] [ERROR] Worker (pid:46528) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:41:37 +0000] [46602] [INFO] Booting worker with pid: 46602
2023-10-05 02:41:59.337194: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:41:59.337194: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:42:01.067927: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:42:01.067929: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:42:04.023469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:04.023469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:04.037632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:04.037632: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:04.041909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:04.042162: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:06.902963: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:06.903016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:06.907532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:06.907797: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:06.912139: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:06.912397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:42:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46601)
[2023-10-05 02:42:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46602)
[2023-10-05 02:42:07 +0000] [684] [ERROR] Worker (pid:46602) was sent code 134!
[2023-10-05 02:42:07 +0000] [46663] [INFO] Booting worker with pid: 46663
[2023-10-05 02:42:07 +0000] [684] [ERROR] Worker (pid:46601) was sent code 134!
[2023-10-05 02:42:07 +0000] [46664] [INFO] Booting worker with pid: 46664
2023-10-05 02:42:13.739475: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:42:13.757244: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:42:14.519306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:42:14.529297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:42:15.324979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:15.326455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:15.328407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:15.330343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:15.331705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:15.334469: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:16.803204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:16.803399: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:16.807699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:16.807944: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:16.812127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:16.812380: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.133592: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.135741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.137455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.139198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:42:19.230073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.232114: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.233884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:42:19.235737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:42:20.121161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.122723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.124225: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.125741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.127299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.128839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.130366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.131915: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.133436: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.134990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.136834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.138366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.139949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.141462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.143016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.144851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.146677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.149058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.150614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.152397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.153917: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.155904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.157220: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:42:20.158427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:42:49 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46664)
[2023-10-05 02:44:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46663)
[2023-10-05 02:46:42 +0000] [684] [ERROR] Worker (pid:46664) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:46:42 +0000] [46749] [INFO] Booting worker with pid: 46749
[2023-10-05 02:46:42 +0000] [684] [ERROR] Worker (pid:46663) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:46:42 +0000] [46750] [INFO] Booting worker with pid: 46750
2023-10-05 02:47:05.767664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:47:05.767664: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:47:07.496593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:47:07.496593: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:47:10.381419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:10.381419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:10.399455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:10.399508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:10.403771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:10.404018: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:47:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46749)
[2023-10-05 02:47:12 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46750)
[2023-10-05 02:47:13 +0000] [684] [ERROR] Worker (pid:46749) was sent code 134!
[2023-10-05 02:47:13 +0000] [46843] [INFO] Booting worker with pid: 46843
[2023-10-05 02:47:13 +0000] [684] [ERROR] Worker (pid:46750) was sent code 134!
[2023-10-05 02:47:13 +0000] [46844] [INFO] Booting worker with pid: 46844
2023-10-05 02:47:18.946178: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:47:19.058005: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:47:19.731025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:47:19.843994: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:47:20.540824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:20.544181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:20.546314: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:20.654735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:20.657725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:20.659629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:22.123306: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:22.125403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:22.127397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:22.128985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:22.133765: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:22.136335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.856510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.858567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.860078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.861485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:47:24.920282: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.922188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.924020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:47:24.925776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:47:25.787753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.788805: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.789779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.790810: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.791784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.792755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.793739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.794833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.795903: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.796920: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.797930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.798995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.800025: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.801030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.802045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.803094: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.804103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.805108: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.806117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.807183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.808193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.809552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.810574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:47:25.811623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:47:57 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46844)
[2023-10-05 02:50:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46843)
[2023-10-05 02:52:07 +0000] [684] [ERROR] Worker (pid:46844) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:52:07 +0000] [46908] [INFO] Booting worker with pid: 46908
[2023-10-05 02:52:07 +0000] [684] [ERROR] Worker (pid:46843) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:52:07 +0000] [46909] [INFO] Booting worker with pid: 46909
2023-10-05 02:52:31.481729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:52:31.481729: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:52:33.182446: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:52:33.182453: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:52:36.493004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:36.493004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:36.508080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:36.508102: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:36.512375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:36.512631: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:52:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46908)
[2023-10-05 02:52:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:46909)
[2023-10-05 02:52:38 +0000] [684] [ERROR] Worker (pid:46908) was sent code 134!
[2023-10-05 02:52:38 +0000] [47053] [INFO] Booting worker with pid: 47053
[2023-10-05 02:52:38 +0000] [684] [ERROR] Worker (pid:46909) was sent code 134!
[2023-10-05 02:52:38 +0000] [47054] [INFO] Booting worker with pid: 47054
2023-10-05 02:52:44.248620: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:52:44.263371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:52:45.024600: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:52:45.049328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:52:45.831689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:45.834887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:45.836854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:45.862841: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:45.866088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:45.868336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:47.951044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:47.951092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:47.955570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:47.955823: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:47.960038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:47.960297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.647521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.647521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.652743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.653028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.657761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.658040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:52:50.662764: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:52:50.663029: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:52:51.557376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.558716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.559691: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.560672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.561633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.562636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.563601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.564770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.565909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.567167: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.568624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.569843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.571045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.572120: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.573463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.574813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.576233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.577834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.579367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.580982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.582608: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.584118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.585275: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:52:51.586428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:53:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47053)
[2023-10-05 02:55:08 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47054)
[2023-10-05 02:57:17 +0000] [684] [ERROR] Worker (pid:47053) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:57:17 +0000] [47112] [INFO] Booting worker with pid: 47112
[2023-10-05 02:57:18 +0000] [684] [ERROR] Worker (pid:47054) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:57:18 +0000] [47115] [INFO] Booting worker with pid: 47115
2023-10-05 02:57:41.796371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:57:41.796371: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:57:43.452457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:57:43.452463: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:57:46.249468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:46.249468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:46.264287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:46.264342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:46.268565: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:46.268822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 02:57:48 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47112)
[2023-10-05 02:57:48 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47115)
[2023-10-05 02:57:48 +0000] [684] [ERROR] Worker (pid:47112) was sent code 134!
[2023-10-05 02:57:48 +0000] [47256] [INFO] Booting worker with pid: 47256
[2023-10-05 02:57:48 +0000] [684] [ERROR] Worker (pid:47115) was sent code 134!
[2023-10-05 02:57:48 +0000] [47257] [INFO] Booting worker with pid: 47257
2023-10-05 02:57:54.583968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:57:54.630320: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 02:57:55.377092: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:57:55.418378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 02:57:56.192987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:56.196312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:56.198188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:56.228863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:56.232007: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:56.233970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:57.967709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:57.967925: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:57.972105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:57.972355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:57.976537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:57:57.976790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.520498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.523329: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.525366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.527625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:58:00.638974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.641624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.643869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 02:58:00.645694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 02:58:01.561181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.562808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.564393: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.566642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.568187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.569811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.571505: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.573449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.575004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.576901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.578337: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.579904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.581350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.583173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.584613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.586043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.587525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.589003: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.590945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.592376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.593796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.595272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.596330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 02:58:01.597377: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 02:58:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47256)
[2023-10-05 02:59:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47257)
[2023-10-05 02:59:55 +0000] [684] [ERROR] Worker (pid:47256) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:59:55 +0000] [47315] [INFO] Booting worker with pid: 47315
[2023-10-05 02:59:56 +0000] [684] [ERROR] Worker (pid:47257) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 02:59:56 +0000] [47328] [INFO] Booting worker with pid: 47328
2023-10-05 03:00:19.519174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:00:19.519174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:00:21.179497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:00:21.179497: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:00:24.001873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:24.001873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:24.016301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:24.016353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:24.020532: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:24.020785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:00:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47315)
[2023-10-05 03:00:26 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47328)
[2023-10-05 03:00:26 +0000] [684] [ERROR] Worker (pid:47315) was sent code 134!
[2023-10-05 03:00:26 +0000] [47446] [INFO] Booting worker with pid: 47446
[2023-10-05 03:00:26 +0000] [684] [ERROR] Worker (pid:47328) was sent code 134!
[2023-10-05 03:00:26 +0000] [47447] [INFO] Booting worker with pid: 47447
2023-10-05 03:00:32.462433: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:00:32.518104: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:00:33.239023: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:00:33.307477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:00:34.043123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:34.046267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:34.048247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:34.125802: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:34.128876: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:34.131105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:35.731601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:35.731732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:35.735992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:35.736241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:35.740495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:35.740756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.423391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.426156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.428517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.430989: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:00:38.468097: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.469996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.471981: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:00:38.473585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:00:39.356140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.357526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.358758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.360156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.361644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.363039: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.364537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.365814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.366953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.368317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.369957: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.371640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.373315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.374990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.376620: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.378262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.379801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.381319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.382839: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.384207: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.385384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.387060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.388187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:00:39.389267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:01:15 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47446)
[2023-10-05 03:02:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47447)
[2023-10-05 03:05:17 +0000] [684] [ERROR] Worker (pid:47446) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:05:17 +0000] [47501] [INFO] Booting worker with pid: 47501
[2023-10-05 03:05:17 +0000] [684] [ERROR] Worker (pid:47447) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:05:17 +0000] [47504] [INFO] Booting worker with pid: 47504
2023-10-05 03:05:41.120366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:05:41.120366: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:05:42.831095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:05:42.831093: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:05:45.756939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:45.756939: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:45.772296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:45.772331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:45.776570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:45.776826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:05:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47501)
[2023-10-05 03:05:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47504)
[2023-10-05 03:05:47 +0000] [684] [ERROR] Worker (pid:47501) was sent code 134!
[2023-10-05 03:05:47 +0000] [47644] [INFO] Booting worker with pid: 47644
[2023-10-05 03:05:48 +0000] [684] [ERROR] Worker (pid:47504) was sent code 134!
[2023-10-05 03:05:48 +0000] [47645] [INFO] Booting worker with pid: 47645
2023-10-05 03:05:53.744954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:05:53.819066: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:05:54.542705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:05:54.607231: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:05:55.355866: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:55.359028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:55.360928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:55.430285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:55.433457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:55.435511: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:57.294356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:57.294356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:57.298869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:57.299126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:57.303431: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:57.303694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.851384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.851382: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.856365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.856679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.861278: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.861589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:05:59.866152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:05:59.866414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:06:00.766995: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.768269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.769235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.770219: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.771222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.772195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.773165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.774126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.775140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.776098: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.777056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.778066: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.779153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.780164: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.781177: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.782198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.783243: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.784238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.785227: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.786245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.787289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.788304: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.789315: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:06:00.790334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:06:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47644)
[2023-10-05 03:08:39 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47645)
[2023-10-05 03:11:08 +0000] [684] [ERROR] Worker (pid:47644) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:11:08 +0000] [47708] [INFO] Booting worker with pid: 47708
[2023-10-05 03:11:08 +0000] [684] [ERROR] Worker (pid:47645) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:11:08 +0000] [47712] [INFO] Booting worker with pid: 47712
2023-10-05 03:11:32.218524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:11:32.218524: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:11:33.924014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:11:33.924021: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:11:36.809506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:36.809506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:36.826464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:36.826487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:36.830950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:36.831196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:11:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47708)
[2023-10-05 03:11:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47712)
[2023-10-05 03:11:38 +0000] [684] [ERROR] Worker (pid:47712) was sent code 134!
[2023-10-05 03:11:38 +0000] [47891] [INFO] Booting worker with pid: 47891
[2023-10-05 03:11:38 +0000] [684] [ERROR] Worker (pid:47708) was sent code 134!
[2023-10-05 03:11:38 +0000] [47892] [INFO] Booting worker with pid: 47892
2023-10-05 03:11:44.925414: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:11:44.966115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:11:45.711649: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:11:45.757785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:11:46.518336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:46.521466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:46.523594: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:46.576582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:46.579872: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:46.581768: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:48.529264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:48.529264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:48.533641: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:48.533898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:48.538063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:48.538319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.299556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.302326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.304257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.306108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:11:51.331238: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.333361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.335149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:11:51.336756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:11:52.207047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.208486: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.209859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.211056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.212223: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.213402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.214612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.215784: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.216966: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.218145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.219366: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.220545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.221742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.222965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.224147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.225327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.227063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.228659: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.230258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.232296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.233909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.235960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.237146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:11:52.238338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:12:32 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47891)
[2023-10-05 03:14:17 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47892)
[2023-10-05 03:15:50 +0000] [684] [ERROR] Worker (pid:47891) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:15:50 +0000] [47948] [INFO] Booting worker with pid: 47948
[2023-10-05 03:15:50 +0000] [684] [ERROR] Worker (pid:47892) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:15:50 +0000] [47950] [INFO] Booting worker with pid: 47950
2023-10-05 03:16:13.665498: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:16:13.665495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:16:15.330336: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:16:15.330341: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:16:18.092545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:18.092545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:18.109082: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:18.109121: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:18.113516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:18.113781: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:16:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47948)
[2023-10-05 03:16:20 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:47950)
[2023-10-05 03:16:21 +0000] [684] [ERROR] Worker (pid:47950) was sent code 134!
[2023-10-05 03:16:21 +0000] [48051] [INFO] Booting worker with pid: 48051
[2023-10-05 03:16:21 +0000] [684] [ERROR] Worker (pid:47948) was sent code 134!
[2023-10-05 03:16:21 +0000] [48052] [INFO] Booting worker with pid: 48052
2023-10-05 03:16:26.921286: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:16:26.997047: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:16:27.707939: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:16:27.787653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:16:28.520661: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:28.523803: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:28.525567: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:28.604418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:28.607666: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:28.609575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:30.081571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:30.083910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:30.085798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:30.099052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:30.110932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:30.112794: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.634136: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.637194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.639591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.642000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:16:32.696365: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.698293: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.700257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:16:32.702069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:16:33.559072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.560146: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.561141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.562152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.563203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.564195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.565204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.566215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.567236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.568231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.569231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.570242: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.571296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.572292: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.573284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.574295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.575326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.576325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.577320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.578324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.579394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.580384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.581379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:16:33.582402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:17:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48051)
[2023-10-05 03:18:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48052)
[2023-10-05 03:18:43 +0000] [684] [ERROR] Worker (pid:48051) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:18:43 +0000] [48107] [INFO] Booting worker with pid: 48107
[2023-10-05 03:18:43 +0000] [684] [ERROR] Worker (pid:48052) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:18:43 +0000] [48110] [INFO] Booting worker with pid: 48110
2023-10-05 03:19:07.084028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:19:07.084028: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:19:08.817337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:19:08.817337: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:19:11.626201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:11.626201: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:11.639890: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:11.639932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:11.644196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:11.644445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:19:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48107)
[2023-10-05 03:19:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48110)
[2023-10-05 03:19:14 +0000] [684] [ERROR] Worker (pid:48110) was sent code 134!
[2023-10-05 03:19:14 +0000] [48183] [INFO] Booting worker with pid: 48183
[2023-10-05 03:19:14 +0000] [684] [ERROR] Worker (pid:48107) was sent code 134!
[2023-10-05 03:19:14 +0000] [48184] [INFO] Booting worker with pid: 48184
2023-10-05 03:19:20.034485: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:19:20.052131: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:19:20.819030: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:19:20.826050: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:19:21.624328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:21.626247: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:21.627783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:21.630069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:21.630763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:21.634525: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:23.352378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:23.352510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:23.356774: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:23.357021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:23.361206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:23.361450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.112555: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.115043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.117574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.120068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:19:26.144277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.146955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.148790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:19:26.150665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:19:27.018568: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.019869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.020840: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.021800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.022811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.023967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.025070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.026148: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.027273: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.028343: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.029422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.030815: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.031985: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.033157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.034335: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.035611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.036979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.038326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.039984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.041400: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.043016: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.044341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.045463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:19:27.046613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:19:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48183)
[2023-10-05 03:22:07 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48184)
[2023-10-05 03:25:10 +0000] [684] [ERROR] Worker (pid:48183) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:25:10 +0000] [48272] [INFO] Booting worker with pid: 48272
[2023-10-05 03:25:10 +0000] [684] [ERROR] Worker (pid:48184) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:25:10 +0000] [48274] [INFO] Booting worker with pid: 48274
2023-10-05 03:25:34.767922: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:25:34.767927: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:25:37.095669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:25:37.095673: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:25:39.992744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:39.992744: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:40.008156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:40.008187: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:40.012448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:40.012698: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:25:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48272)
[2023-10-05 03:25:40 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48274)
[2023-10-05 03:25:41 +0000] [684] [ERROR] Worker (pid:48274) was sent code 134!
[2023-10-05 03:25:41 +0000] [48433] [INFO] Booting worker with pid: 48433
[2023-10-05 03:25:41 +0000] [684] [ERROR] Worker (pid:48272) was sent code 134!
[2023-10-05 03:25:41 +0000] [48434] [INFO] Booting worker with pid: 48434
2023-10-05 03:25:46.961968: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:25:47.103401: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:25:47.746167: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:25:47.888805: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:25:48.557327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:48.560791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:48.562908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:48.702579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:48.705679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:48.707723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:50.910686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:50.910686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:50.915064: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:50.915320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:50.919498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:50.919749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.580376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.582686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.584465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.586626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:25:53.669792: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.671685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.673458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:25:53.675285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:25:54.527909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.529658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.530760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.532152: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.533562: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.534980: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.536330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.538479: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.539855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.540870: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.541878: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.542933: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.544756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.546169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.547637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.549023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.550515: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.551950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.553317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.554722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.556430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.557790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.558846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:25:54.559873: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:26:25 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48434)
[2023-10-05 03:28:14 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48433)
[2023-10-05 03:29:16 +0000] [684] [ERROR] Worker (pid:48434) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:29:16 +0000] [48488] [INFO] Booting worker with pid: 48488
[2023-10-05 03:29:17 +0000] [684] [ERROR] Worker (pid:48433) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:29:17 +0000] [48489] [INFO] Booting worker with pid: 48489
2023-10-05 03:29:39.733133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:29:39.733133: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:29:41.452887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:29:41.452887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:29:44.428385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:44.428385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:44.440367: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:44.440378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:44.444650: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:44.444908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:29:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48488)
[2023-10-05 03:29:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48489)
[2023-10-05 03:29:47 +0000] [684] [ERROR] Worker (pid:48489) was sent code 134!
[2023-10-05 03:29:47 +0000] [48562] [INFO] Booting worker with pid: 48562
[2023-10-05 03:29:47 +0000] [684] [ERROR] Worker (pid:48488) was sent code 134!
[2023-10-05 03:29:47 +0000] [48563] [INFO] Booting worker with pid: 48563
2023-10-05 03:29:53.332778: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:29:53.348893: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:29:54.122239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:29:54.132323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:29:54.930360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:54.933546: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:54.935589: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:54.945747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:54.948817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:54.950910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:56.419034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:56.419034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:56.423476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:56.423731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:56.427954: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:56.428206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.680750: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.683921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.686834: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.689591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19006 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:29:59.860767: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.862788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.864420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:29:59.865983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:30:00.830004: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.831516: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.833150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.834427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.836090: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.837879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.839394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.841106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.843471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.844984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.846490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.847706: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.849634: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.851372: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.852965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.854908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.856538: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.858300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.859742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.861355: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.862943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.864424: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.865579: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.866833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:30:00.868402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:30:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48562)
[2023-10-05 03:33:13 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48563)
[2023-10-05 03:35:06 +0000] [684] [ERROR] Worker (pid:48562) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:35:06 +0000] [48650] [INFO] Booting worker with pid: 48650
[2023-10-05 03:35:06 +0000] [684] [ERROR] Worker (pid:48563) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:35:06 +0000] [48651] [INFO] Booting worker with pid: 48651
2023-10-05 03:35:30.291929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:35:30.291929: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:35:32.087179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:35:32.087179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:35:35.054965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:35.054965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:35.071779: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:35.071818: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:35.076053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:35.076301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:35:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48650)
[2023-10-05 03:35:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48651)
[2023-10-05 03:35:36 +0000] [684] [ERROR] Worker (pid:48650) was sent code 134!
[2023-10-05 03:35:36 +0000] [48793] [INFO] Booting worker with pid: 48793
[2023-10-05 03:35:36 +0000] [684] [ERROR] Worker (pid:48651) was sent code 134!
[2023-10-05 03:35:36 +0000] [48794] [INFO] Booting worker with pid: 48794
2023-10-05 03:35:42.631858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:35:42.733522: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:35:43.424464: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:35:43.523316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:35:44.240753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:44.244011: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:44.246093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:44.332212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:44.335324: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:44.337202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:46.533544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:46.533616: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:46.538133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:46.538386: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:46.542713: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:46.542972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.275347: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.277510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.280067: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.282522: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:35:49.294106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.296195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.298073: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:35:49.299754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:35:50.177756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.179088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.180024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.180943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.181862: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.182838: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.183798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.185034: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.186351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.187975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.189549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.190991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.192181: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.193204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.194557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.196140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.197430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.199071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.200681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.202284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.203537: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.204699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.205869: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:35:50.207022: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:36:21 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48793)
[2023-10-05 03:38:01 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48794)
[2023-10-05 03:38:15 +0000] [684] [ERROR] Worker (pid:48793) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:38:15 +0000] [48848] [INFO] Booting worker with pid: 48848
[2023-10-05 03:38:15 +0000] [684] [ERROR] Worker (pid:48794) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:38:15 +0000] [48850] [INFO] Booting worker with pid: 48850
2023-10-05 03:38:39.033966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:38:39.033966: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:38:40.803495: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:38:40.803500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:38:43.649131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:43.649131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:43.665150: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:43.665191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:43.669442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:43.669692: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:38:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48848)
[2023-10-05 03:38:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48850)
[2023-10-05 03:38:46 +0000] [684] [ERROR] Worker (pid:48848) was sent code 134!
[2023-10-05 03:38:46 +0000] [48919] [INFO] Booting worker with pid: 48919
[2023-10-05 03:38:46 +0000] [684] [ERROR] Worker (pid:48850) was sent code 134!
[2023-10-05 03:38:46 +0000] [48920] [INFO] Booting worker with pid: 48920
2023-10-05 03:38:51.993801: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:38:52.072174: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:38:52.786281: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:38:52.861404: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:38:53.593714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:53.597024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:53.599269: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:53.672543: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:53.675638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:53.677612: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:55.457893: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:55.458037: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:55.462326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:55.462571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:55.466754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:55.467010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.231651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.231658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.236652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.236947: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.241561: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.241846: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:38:58.246372: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:38:58.246656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:38:59.137363: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.138833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.140345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.141629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.143179: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.144296: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.145949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.147578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.148716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.149886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.151174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.152334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.153688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.155115: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.156359: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.157716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.159402: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.160738: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.162378: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.164036: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.165682: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.167351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.169021: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:38:59.170551: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:39:34 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48919)
[2023-10-05 03:41:16 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48920)
[2023-10-05 03:43:40 +0000] [684] [ERROR] Worker (pid:48919) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:43:40 +0000] [48975] [INFO] Booting worker with pid: 48975
[2023-10-05 03:43:41 +0000] [684] [ERROR] Worker (pid:48920) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:43:41 +0000] [48985] [INFO] Booting worker with pid: 48985
2023-10-05 03:44:04.809413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:44:04.809413: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:44:06.561557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:44:06.561557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:44:09.397778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:09.397778: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:09.413161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:09.413206: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:09.417437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:09.417689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:44:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48975)
[2023-10-05 03:44:11 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:48985)
[2023-10-05 03:44:11 +0000] [684] [ERROR] Worker (pid:48985) was sent code 134!
[2023-10-05 03:44:11 +0000] [49149] [INFO] Booting worker with pid: 49149
[2023-10-05 03:44:11 +0000] [684] [ERROR] Worker (pid:48975) was sent code 134!
[2023-10-05 03:44:11 +0000] [49150] [INFO] Booting worker with pid: 49150
2023-10-05 03:44:17.262684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:44:17.354797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:44:18.051138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:44:18.150658: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:44:18.865005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:18.868280: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:18.870277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:18.959514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:18.962652: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:18.964385: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:20.834930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:20.834970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:20.839375: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:20.839622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:20.843813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:20.844060: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.685699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.687762: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.689455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.691240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:44:23.711348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.713246: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.715159: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:44:23.716804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:44:24.591726: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.593019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.593984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.594991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.595950: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.597235: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.598420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.599625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.600671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.602100: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.603754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.605332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.607009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.608587: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.610212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.611602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.612753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.613911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.615053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.616092: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.617140: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.618195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.619267: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:44:24.620319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:44:56 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49149)
[2023-10-05 03:46:59 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49150)
[2023-10-05 03:49:06 +0000] [684] [ERROR] Worker (pid:49149) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:49:06 +0000] [49240] [INFO] Booting worker with pid: 49240
[2023-10-05 03:49:06 +0000] [684] [ERROR] Worker (pid:49150) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:49:06 +0000] [49242] [INFO] Booting worker with pid: 49242
2023-10-05 03:49:30.222856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:49:30.222856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:49:32.098496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:49:32.098501: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:49:35.243625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:35.243624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:35.258955: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:35.259005: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:35.263226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:35.263478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:49:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49240)
[2023-10-05 03:49:36 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49242)
[2023-10-05 03:49:36 +0000] [684] [ERROR] Worker (pid:49240) was sent code 134!
[2023-10-05 03:49:36 +0000] [49384] [INFO] Booting worker with pid: 49384
[2023-10-05 03:49:36 +0000] [684] [ERROR] Worker (pid:49242) was sent code 134!
[2023-10-05 03:49:36 +0000] [49385] [INFO] Booting worker with pid: 49385
2023-10-05 03:49:42.717584: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:49:42.752749: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:49:43.499011: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:49:43.532082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:49:44.303028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:44.306176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:44.308274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:44.334758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:44.337617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:44.339646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:46.705720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:46.705720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:46.710279: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:46.710540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:46.714796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:46.715057: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.420886: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.423681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.425863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.428365: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19036 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:49:49.637696: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.639733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.641597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:49:49.643414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:49:50.520250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.521376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.522754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.524141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.525450: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.527272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.528593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.530010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.531926: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.532984: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.534504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.535871: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.537287: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.538496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.539526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.540531: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.541522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.542699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.543708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.544695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.545688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.546724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.547719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.548707: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:49:50.549694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:50:18 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49385)
[2023-10-05 03:51:53 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49384)
[2023-10-05 03:53:39 +0000] [684] [ERROR] Worker (pid:49385) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:53:39 +0000] [49445] [INFO] Booting worker with pid: 49445
[2023-10-05 03:53:40 +0000] [684] [ERROR] Worker (pid:49384) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:53:40 +0000] [49446] [INFO] Booting worker with pid: 49446
2023-10-05 03:54:03.313802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:54:03.313803: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:54:05.057842: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:54:05.057846: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:54:07.904475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:07.904475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:07.918023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:07.918070: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:07.922328: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:07.922606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:10.759547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:10.759626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:10.764629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:10.764902: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:10.769271: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:10.769626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:54:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49445)
[2023-10-05 03:54:10 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49446)
[2023-10-05 03:54:11 +0000] [684] [ERROR] Worker (pid:49445) was sent code 134!
[2023-10-05 03:54:11 +0000] [49548] [INFO] Booting worker with pid: 49548
[2023-10-05 03:54:11 +0000] [684] [ERROR] Worker (pid:49446) was sent code 134!
[2023-10-05 03:54:11 +0000] [49549] [INFO] Booting worker with pid: 49549
2023-10-05 03:54:17.061119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:54:17.081952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:54:17.836834: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:54:17.860816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:54:18.644960: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:18.648233: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:18.650259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:18.661547: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:18.664708: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:18.666421: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:20.161605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:20.164144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:20.166342: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:20.175158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:20.190083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:20.192142: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.123795: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.126418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.128912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.130568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 19030 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:54:23.247584: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.249438: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.251217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:54:23.252667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:54:24.120832: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.122798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.124155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.125660: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.127053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.128408: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.129747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.131112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.132458: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.133789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.135190: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.136605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.138351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.139756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.141545: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.142945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.144294: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.145303: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.147077: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.148739: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.150058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.151813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.152826: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.153828: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:54:24.154959: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.48GiB (1587712768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:54:52 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49548)
[2023-10-05 03:56:28 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49549)
[2023-10-05 03:58:15 +0000] [684] [ERROR] Worker (pid:49548) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:58:15 +0000] [49610] [INFO] Booting worker with pid: 49610
[2023-10-05 03:58:16 +0000] [684] [ERROR] Worker (pid:49549) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 03:58:16 +0000] [49611] [INFO] Booting worker with pid: 49611
2023-10-05 03:58:39.466855: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:58:39.466855: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:58:41.220483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:58:41.220483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:58:44.080820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:44.080820: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:44.098130: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:44.098163: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:44.102419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:44.102684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 03:58:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49610)
[2023-10-05 03:58:46 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49611)
[2023-10-05 03:58:46 +0000] [684] [ERROR] Worker (pid:49610) was sent code 134!
[2023-10-05 03:58:46 +0000] [49682] [INFO] Booting worker with pid: 49682
[2023-10-05 03:58:46 +0000] [684] [ERROR] Worker (pid:49611) was sent code 134!
[2023-10-05 03:58:46 +0000] [49683] [INFO] Booting worker with pid: 49683
2023-10-05 03:58:52.273547: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:58:52.335243: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 03:58:53.062817: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:58:53.122441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 03:58:53.875063: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:53.878322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:53.880208: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:53.934348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:53.937403: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:53.939521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:55.755811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:55.755811: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:55.760216: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:55.760464: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:55.764640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:55.764888: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.549637: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.552218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.554250: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.556000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:58:58.683918: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.685751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.687485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 03:58:58.689214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 03:58:59.565606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.567514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.568894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.570264: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.571978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.573001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.574350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.575877: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.577251: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.578717: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.580237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.581581: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.583010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.584371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.585723: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.587117: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.588599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.590244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.591914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.593210: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.594901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.596549: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.598169: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 03:58:59.599427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 03:59:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49682)
[2023-10-05 04:01:27 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49683)
[2023-10-05 04:03:33 +0000] [684] [ERROR] Worker (pid:49682) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:03:33 +0000] [49740] [INFO] Booting worker with pid: 49740
[2023-10-05 04:03:34 +0000] [684] [ERROR] Worker (pid:49683) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:03:34 +0000] [49746] [INFO] Booting worker with pid: 49746
2023-10-05 04:03:57.798661: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:03:57.798658: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:03:59.471170: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:03:59.471169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:04:02.394448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:02.394448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:02.409257: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:02.409297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:02.413580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:02.413843: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 04:04:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49740)
[2023-10-05 04:04:04 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49746)
[2023-10-05 04:04:04 +0000] [684] [ERROR] Worker (pid:49740) was sent code 134!
[2023-10-05 04:04:04 +0000] [49914] [INFO] Booting worker with pid: 49914
[2023-10-05 04:04:04 +0000] [684] [ERROR] Worker (pid:49746) was sent code 134!
[2023-10-05 04:04:04 +0000] [49915] [INFO] Booting worker with pid: 49915
2023-10-05 04:04:10.318013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:04:10.424753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:04:11.098467: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:04:11.219199: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:04:11.902575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:11.905831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:11.907928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:12.037606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:12.040687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:12.042574: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:13.931492: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:13.931490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:13.935900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:13.936157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:13.940357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:13.940609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.709727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.712316: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.713964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.716243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:04:16.755105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.757477: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.759371: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:04:16.761238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:04:17.636131: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.637410: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.638418: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.639451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.640473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.641467: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.642478: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.643495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.644487: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.645519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.646556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.647629: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.648715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.649764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.650853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.651897: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.652982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.654089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.655212: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.656268: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.657313: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.658370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.659448: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:04:17.660497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:04:45 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49914)
[2023-10-05 04:06:29 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:49915)
[2023-10-05 04:08:24 +0000] [684] [ERROR] Worker (pid:49914) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:08:24 +0000] [50002] [INFO] Booting worker with pid: 50002
[2023-10-05 04:08:25 +0000] [684] [ERROR] Worker (pid:49915) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:08:25 +0000] [50004] [INFO] Booting worker with pid: 50004
2023-10-05 04:08:48.441295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:08:48.441295: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:08:50.131210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:08:50.131209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:08:53.013974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:08:53.013974: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:08:53.031288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:08:53.031288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:08:53.035751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:08:53.036001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 04:08:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50002)
[2023-10-05 04:08:55 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50004)
[2023-10-05 04:08:55 +0000] [684] [ERROR] Worker (pid:50004) was sent code 134!
[2023-10-05 04:08:55 +0000] [50074] [INFO] Booting worker with pid: 50074
[2023-10-05 04:08:55 +0000] [684] [ERROR] Worker (pid:50002) was sent code 134!
[2023-10-05 04:08:55 +0000] [50075] [INFO] Booting worker with pid: 50075
2023-10-05 04:09:01.525023: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:09:01.566856: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:09:02.313458: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:09:02.343794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:09:03.125854: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:03.129116: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:03.131173: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:03.146788: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:03.149833: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:03.151798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:04.782951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:04.783045: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:04.787353: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:04.787601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:04.791777: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:04.792027: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.421602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.423990: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.425771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.427667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:09:07.491202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.493284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.495147: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:09:07.496759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:09:08.350451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.351536: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.352527: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.353526: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.354570: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.355633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.356638: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.357651: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.358694: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.359705: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.360725: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.361734: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.362793: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.363808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.364812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.365814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.366962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.367986: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.369000: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.370012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.371096: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.372288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.373307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:09:08.374326: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:09:31 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50075)
[2023-10-05 04:09:47 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50074)
[2023-10-05 04:11:23 +0000] [684] [ERROR] Worker (pid:50075) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:11:23 +0000] [50129] [INFO] Booting worker with pid: 50129
[2023-10-05 04:11:24 +0000] [684] [ERROR] Worker (pid:50074) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:11:24 +0000] [50154] [INFO] Booting worker with pid: 50154
2023-10-05 04:11:47.635440: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:11:47.635441: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:11:49.419898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:11:49.419898: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:11:52.439709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:11:52.439709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:11:52.455930: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:11:52.455965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:11:52.460194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:11:52.460445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 04:11:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50129)
[2023-10-05 04:11:54 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50154)
[2023-10-05 04:11:54 +0000] [684] [ERROR] Worker (pid:50154) was sent code 134!
[2023-10-05 04:11:54 +0000] [50237] [INFO] Booting worker with pid: 50237
[2023-10-05 04:11:54 +0000] [684] [ERROR] Worker (pid:50129) was sent code 134!
[2023-10-05 04:11:54 +0000] [50238] [INFO] Booting worker with pid: 50238
2023-10-05 04:12:01.320138: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:12:01.368478: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:12:02.120380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:12:02.178581: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:12:02.985987: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:02.989455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:02.991613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:03.052305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:03.055452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:03.057441: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:04.971215: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:04.971253: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:04.975580: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:04.975831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:04.980009: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:04.980255: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.798675: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.801598: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.803813: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.805566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:12:07.833481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.835497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.837325: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:12:07.839089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 18982 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:12:08.730217: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 18.54GiB (19904528384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.731586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.68GiB (17914075136 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.732621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 15.01GiB (16122667008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.733644: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 13.51GiB (14510399488 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.734824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.16GiB (13059358720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.736195: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.95GiB (11753422848 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.737496: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.85GiB (10578080768 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.739172: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.87GiB (9520272384 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.740606: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.98GiB (8568244736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.742126: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.18GiB (7711419904 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.743291: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.46GiB (6940277760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.744601: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.82GiB (6246249984 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.746072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.24GiB (5621624832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.747672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.71GiB (5059462144 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.749312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.24GiB (4553516032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.750973: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.82GiB (4098164224 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.752412: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.43GiB (3688347648 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.753493: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.09GiB (3319512832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.754927: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.78GiB (2987561472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.756595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.50GiB (2688805376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.758156: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.25GiB (2419924736 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.759529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.03GiB (2177932288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.761193: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.83GiB (1960139008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:12:08.762901: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.64GiB (1764125184 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:12:42 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50238)
[2023-10-05 04:14:38 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50237)
[2023-10-05 04:16:52 +0000] [684] [ERROR] Worker (pid:50238) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:16:52 +0000] [50317] [INFO] Booting worker with pid: 50317
[2023-10-05 04:16:53 +0000] [684] [ERROR] Worker (pid:50237) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:16:53 +0000] [50325] [INFO] Booting worker with pid: 50325
2023-10-05 04:17:16.532757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:17:16.532757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:17:18.304999: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:17:18.304996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:17:21.242521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:21.242521: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:21.258322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:21.258374: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:21.262657: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:21.262928: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
[2023-10-05 04:17:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50317)
[2023-10-05 04:17:23 +0000] [684] [CRITICAL] WORKER TIMEOUT (pid:50325)
[2023-10-05 04:17:23 +0000] [684] [ERROR] Worker (pid:50317) was sent code 134!
[2023-10-05 04:17:23 +0000] [50501] [INFO] Booting worker with pid: 50501
[2023-10-05 04:17:23 +0000] [684] [ERROR] Worker (pid:50325) was sent code 134!
[2023-10-05 04:17:23 +0000] [50502] [INFO] Booting worker with pid: 50502
2023-10-05 04:17:29.236292: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:17:29.271170: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:17:30.228397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:17:30.274860: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:17:31.123654: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:31.126895: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:31.128948: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:31.157237: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:31.160336: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:31.162222: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:33.156552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:33.156552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:33.163430: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:33.163824: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:33.169965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:33.170357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.758747: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.760679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.762300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.763918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:17:35.781014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.783127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.784727: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:17:35.786320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:17:36.644144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 19.79GiB (21254373376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.645463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 17.81GiB (19128936448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.646466: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 16.03GiB (17216043008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.647480: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 14.43GiB (15494437888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.648459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 12.99GiB (13944993792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.649437: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 11.69GiB (12550494208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.650455: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 10.52GiB (11295444992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.651476: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 9.47GiB (10165900288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.652463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 8.52GiB (9149309952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.653445: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 7.67GiB (8234378752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.654433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.90GiB (7410940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.655449: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 6.21GiB (6669846528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.656427: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.59GiB (6002861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.657409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 5.03GiB (5402575360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.658397: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.53GiB (4862317568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.659420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 4.08GiB (4376085504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.660407: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.67GiB (3938476800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.661384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 3.30GiB (3544628992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.662417: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.97GiB (3190166016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.663462: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.67GiB (2871149312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.664452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.41GiB (2584034304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.665443: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 2.17GiB (2325630720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.666452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.95GiB (2093067776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.667485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.75GiB (1883760896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:17:36.668457: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 1.58GiB (1695384832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
[2023-10-05 04:17:51 +0000] [50501] [INFO] Started server process [50501]
[2023-10-05 04:17:51 +0000] [50502] [INFO] Started server process [50502]
[2023-10-05 04:17:51 +0000] [50501] [INFO] Waiting for application startup.
[2023-10-05 04:17:51 +0000] [50502] [INFO] Waiting for application startup.
[2023-10-05 04:17:51 +0000] [50501] [INFO] Application startup complete.
[2023-10-05 04:17:51 +0000] [50502] [INFO] Application startup complete.
[2023-10-05 04:56:16 +0000] [684] [ERROR] Worker (pid:50501) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:56:16 +0000] [52204] [INFO] Booting worker with pid: 52204
[2023-10-05 04:56:24 +0000] [684] [INFO] Handling signal: term
[2023-10-05 04:56:24 +0000] [684] [ERROR] Worker (pid:52204) was sent SIGTERM!
[2023-10-05 04:56:24 +0000] [50502] [INFO] Shutting down
[2023-10-05 04:56:24 +0000] [50502] [INFO] Waiting for application shutdown.
[2023-10-05 04:56:24 +0000] [50502] [INFO] Application shutdown complete.
[2023-10-05 04:56:24 +0000] [50502] [INFO] Finished server process [50502]
[2023-10-05 04:56:24 +0000] [50502] [INFO] Worker exiting (pid: 50502)
[2023-10-05 04:56:30 +0000] [684] [INFO] Shutting down: Master
[2023-10-05 04:57:25 +0000] [52391] [INFO] Starting gunicorn 21.2.0
[2023-10-05 04:57:25 +0000] [52391] [INFO] Listening at: http://0.0.0.0:9000 (52391)
[2023-10-05 04:57:25 +0000] [52391] [INFO] Using worker: uvicorn.workers.UvicornWorker
[2023-10-05 04:57:25 +0000] [52402] [INFO] Booting worker with pid: 52402
[2023-10-05 04:57:25 +0000] [52405] [INFO] Booting worker with pid: 52405
2023-10-05 04:57:40.714847: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:57:40.714899: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:57:40.714953: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:57:40.723757: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:57:40.736599: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:57:40.736644: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:57:40.736694: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:57:40.746912: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:57:41.692686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:57:41.932839: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:57:42.784529: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:42.805043: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:42.810894: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:43.022483: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:43.030048: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:43.035716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:45.656019: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:45.662123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:45.665689: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:45.672133: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:45.691967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:45.695677: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:47.935956: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:47.939288: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:47.942093: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:47.946653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:57:48.008259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:48.012153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:48.015333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:57:48.018026: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:57:49.664444: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 19.79GiB (21254373376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.666512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 17.81GiB (19128936448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.668263: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 16.03GiB (17216043008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.670576: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 14.43GiB (15494437888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.672718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 12.99GiB (13944993792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.677058: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 11.69GiB (12550494208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.679071: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 10.52GiB (11295444992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.681518: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 9.47GiB (10165900288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.684721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 8.52GiB (9149309952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.687782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 7.67GiB (8234378752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.689967: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.90GiB (7410940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.691724: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.21GiB (6669846528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.693831: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.59GiB (6002861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.696028: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.03GiB (5402575360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.697776: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.53GiB (4862317568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.699785: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.08GiB (4376085504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.701756: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.67GiB (3938476800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.703481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.30GiB (3544628992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.705597: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.97GiB (3190166016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.707719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.67GiB (2871149312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.709426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.41GiB (2584034304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.711539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.17GiB (2325630720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.713285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.95GiB (2093067776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.715452: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.75GiB (1883760896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:57:49.717512: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.58GiB (1695384832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:57:55 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52402)
[2023-10-05 04:57:55 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52405)
[2023-10-05 04:57:56 +0000] [52391] [ERROR] Worker (pid:52402) was sent code 134!
[2023-10-05 04:57:56 +0000] [52546] [INFO] Booting worker with pid: 52546
[2023-10-05 04:57:56 +0000] [52391] [ERROR] Worker (pid:52405) was sent code 134!
[2023-10-05 04:57:56 +0000] [52547] [INFO] Booting worker with pid: 52547
2023-10-05 04:58:10.879060: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:58:10.879060: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:58:10.879112: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:58:10.879112: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:58:10.879150: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:58:10.879152: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:58:10.887204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:58:10.887204: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:58:12.675088: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:58:12.675096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:58:14.774908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:14.774908: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:14.785600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:14.785600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:14.793746: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:14.794145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:16.813360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:16.813394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:16.817688: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:16.821868: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:16.850141: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:16.853083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.064231: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.066137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.067998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.069596: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:58:18.072904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.074782: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.076419: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:18.078228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:58:19.165113: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 19.79GiB (21254373376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.166428: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 17.81GiB (19128936448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.167485: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 16.03GiB (17216043008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.168506: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 14.43GiB (15494437888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.169575: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 12.99GiB (13944993792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.170636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 11.69GiB (12550494208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.171685: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 10.52GiB (11295444992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.172695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 9.47GiB (10165900288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.173709: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 8.52GiB (9149309952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.174770: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 7.67GiB (8234378752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.175786: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.90GiB (7410940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.176799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.21GiB (6669846528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.177812: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.59GiB (6002861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.178884: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.03GiB (5402575360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.179923: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.53GiB (4862317568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.180945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.08GiB (4376085504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.181964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.67GiB (3938476800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.183023: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.30GiB (3544628992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.184084: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.97GiB (3190166016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.185144: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.67GiB (2871149312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.186203: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.41GiB (2584034304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.187285: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.17GiB (2325630720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.188330: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.95GiB (2093067776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.189391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.75GiB (1883760896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:19.190440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.58GiB (1695384832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:58:26 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52546)
[2023-10-05 04:58:26 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52547)
[2023-10-05 04:58:27 +0000] [52391] [ERROR] Worker (pid:52547) was sent code 134!
[2023-10-05 04:58:27 +0000] [52617] [INFO] Booting worker with pid: 52617
[2023-10-05 04:58:27 +0000] [52391] [ERROR] Worker (pid:52546) was sent code 134!
[2023-10-05 04:58:28 +0000] [52618] [INFO] Booting worker with pid: 52618
2023-10-05 04:58:39.529525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:58:39.529525: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:58:39.529579: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:58:39.529579: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:58:39.529617: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:58:39.529620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:58:39.537739: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:58:39.537748: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:58:40.402447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:58:40.406126: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:58:41.175290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:41.177052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:41.183911: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:41.183919: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:41.190921: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:41.191322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:42.804550: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:42.804607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:42.812339: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:42.812743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:42.819191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:42.819582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.837265: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.839394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.841059: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.842982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20323 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:58:44.937763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.939789: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.941653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:58:44.943457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:58:46.020226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 19.79GiB (21254373376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.021277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 17.81GiB (19128936448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.022289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 16.03GiB (17216043008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.023320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 14.43GiB (15494437888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.024310: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 12.99GiB (13944993792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.025305: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 11.69GiB (12550494208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.026334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 10.52GiB (11295444992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.027370: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 9.47GiB (10165900288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.028358: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 8.52GiB (9149309952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.029346: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 7.67GiB (8234378752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.030333: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.90GiB (7410940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.031356: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.21GiB (6669846528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.032340: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.59GiB (6002861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.033317: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.03GiB (5402575360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.034307: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.53GiB (4862317568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.035331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.08GiB (4376085504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.036320: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.67GiB (3938476800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.037301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.30GiB (3544628992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.038327: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.97GiB (3190166016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.039361: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.67GiB (2871149312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.040360: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.41GiB (2584034304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.041357: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.17GiB (2325630720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.042381: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.95GiB (2093067776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.043426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.75GiB (1883760896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:58:46.044406: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.58GiB (1695384832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:58:57 +0000] [52391] [ERROR] Worker (pid:52617) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:58:57 +0000] [52712] [INFO] Booting worker with pid: 52712
[2023-10-05 04:58:58 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52618)
[2023-10-05 04:58:59 +0000] [52391] [ERROR] Worker (pid:52618) was sent code 134!
[2023-10-05 04:58:59 +0000] [52717] [INFO] Booting worker with pid: 52717
2023-10-05 04:59:13.539977: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:59:13.540000: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:59:13.540040: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:59:13.540180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:59:13.543444: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:59:13.543446: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:59:13.851248: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:59:13.851254: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:59:15.766419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:59:15.766424: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:59:18.668020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:18.668020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:18.682801: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:18.682837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:18.689471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:18.689867: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:21.426295: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:21.426662: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:21.433080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:21.433463: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:21.439497: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:21.439887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.872715: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.874799: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.876671: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.878404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:59:23.931898: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.933997: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.936024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:23.937848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:59:25.049945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 19.79GiB (21254373376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.051075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 17.81GiB (19128936448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.052110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 16.03GiB (17216043008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.053137: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 14.43GiB (15494437888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.054160: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 12.99GiB (13944993792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.055244: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 11.69GiB (12550494208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.056301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 10.52GiB (11295444992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.057319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 9.47GiB (10165900288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.058352: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 8.52GiB (9149309952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.059422: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 7.67GiB (8234378752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.060442: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.90GiB (7410940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.061465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.21GiB (6669846528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.062499: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.59GiB (6002861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.063548: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.03GiB (5402575360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.064571: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.53GiB (4862317568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.065599: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.08GiB (4376085504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.066656: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.67GiB (3938476800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.067679: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.30GiB (3544628992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.068742: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.97GiB (3190166016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.069783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.67GiB (2871149312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.070859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.41GiB (2584034304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.071892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.17GiB (2325630720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.072929: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.95GiB (2093067776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.073979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.75GiB (1883760896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:25.075038: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.58GiB (1695384832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:59:27 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52712)
[2023-10-05 04:59:28 +0000] [52391] [ERROR] Worker (pid:52712) was sent code 134!
[2023-10-05 04:59:28 +0000] [52777] [INFO] Booting worker with pid: 52777
[2023-10-05 04:59:30 +0000] [52391] [CRITICAL] WORKER TIMEOUT (pid:52717)
[2023-10-05 04:59:31 +0000] [52391] [ERROR] Worker (pid:52717) was sent code 134!
[2023-10-05 04:59:31 +0000] [52799] [INFO] Booting worker with pid: 52799
2023-10-05 04:59:36.726308: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:59:36.726361: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:59:36.726403: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:59:36.735329: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:59:37.560695: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2023-10-05 04:59:37.560742: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2023-10-05 04:59:37.560787: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-10-05 04:59:37.569076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-10-05 04:59:37.654232: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:59:38.431284: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:38.434395: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:38.436297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:38.440543: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2023-10-05 04:59:39.221241: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:39.224341: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:39.226368: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:39.798855: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:39.800969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:39.803127: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.528274: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.530822: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.533109: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.927276: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.929978: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.932759: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:40.934585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20271 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:59:41.647896: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:41.650040: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:41.651771: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:894] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355
2023-10-05 04:59:41.653553: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20269 MB memory:  -> device: 0, name: NVIDIA A10G, pci bus id: 0000:00:1e.0, compute capability: 8.6
2023-10-05 04:59:42.740345: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 19.79GiB (21254373376 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.741396: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 17.81GiB (19128936448 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.742435: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 16.03GiB (17216043008 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.743498: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 14.43GiB (15494437888 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.744501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 12.99GiB (13944993792 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.745502: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 11.69GiB (12550494208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.746582: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 10.52GiB (11295444992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.747668: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 9.47GiB (10165900288 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.748664: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 8.52GiB (9149309952 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.749653: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 7.67GiB (8234378752 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.750741: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.90GiB (7410940928 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.751735: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 6.21GiB (6669846528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.752728: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.59GiB (6002861568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.753719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 5.03GiB (5402575360 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.754761: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.53GiB (4862317568 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.755754: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 4.08GiB (4376085504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.756755: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.67GiB (3938476800 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.757751: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 3.30GiB (3544628992 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.758900: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.97GiB (3190166016 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.759922: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.67GiB (2871149312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.760916: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.41GiB (2584034304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.761914: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 2.17GiB (2325630720 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.762968: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.95GiB (2093067776 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.763989: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.75GiB (1883760896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2023-10-05 04:59:42.764979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:746] failed to allocate 1.58GiB (1695384832 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
[2023-10-05 04:59:56 +0000] [52391] [ERROR] Worker (pid:52777) was sent SIGKILL! Perhaps out of memory?
[2023-10-05 04:59:56 +0000] [52903] [INFO] Booting worker with pid: 52903
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.
[2023-10-05 04:59:59 +0000] [52799] [INFO] Started server process [52799]
[2023-10-05 04:59:59 +0000] [52799] [INFO] Waiting for application startup.
[2023-10-05 04:59:59 +0000] [52799] [INFO] Application startup complete.
[2023-10-05 05:00:02 +0000] [52391] [INFO] Handling signal: term
[2023-10-05 05:00:02 +0000] [52391] [ERROR] Worker (pid:52903) was sent SIGTERM!
[2023-10-05 05:00:02 +0000] [52799] [INFO] Shutting down
[2023-10-05 05:00:02 +0000] [52799] [INFO] Waiting for application shutdown.
[2023-10-05 05:00:02 +0000] [52799] [INFO] Application shutdown complete.
[2023-10-05 05:00:02 +0000] [52799] [INFO] Finished server process [52799]
[2023-10-05 05:00:02 +0000] [52799] [INFO] Worker exiting (pid: 52799)
